{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Equivariant Quantum Circuits for Combinatorial Optimization Problems"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:31:27.934401900Z",
     "start_time": "2023-12-10T12:31:27.917442500Z"
    }
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from itertools import combinations\n",
    "from utils.utils import graph_to_list, compute_tour_length, compute_reward, get_masks_for_actions\n",
    "\n",
    "# Qiskit algorithms imports\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "\n",
    "# Qiskit imports\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit.circuit import QuantumCircuit, Parameter\n",
    "\n",
    "# Qiskit ML imports\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch.nn import MSELoss, Module\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "BASE_PATH = '../data/'\n",
    "\n",
    "# Fix seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "algorithm_globals.random_seed = random\n",
    "\n",
    "# For smooth plotting\n",
    "# REFERENCE: handson-ml3\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval\n",
    "    )\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'n_vars': 10,\n",
    "    'episodes': 5000,\n",
    "    'batch_size': 10,\n",
    "    'epsilon': 1,\n",
    "    'epsilon_decay': 0.99,\n",
    "    'epsilon_min': 0.01,\n",
    "    'gamma': 0.9,\n",
    "    'update_after': 10,\n",
    "    'update_target_after': 30,\n",
    "    'learning_rate_in': 0.00001,\n",
    "    'n_layers': 1,\n",
    "    'epsilon_schedule': 'fast',\n",
    "    'memory_length': 10000,\n",
    "    'num_instances': 100,\n",
    "    'data_path': BASE_PATH + 'tsp/tsp_10_train/tsp_10_reduced_train.pickle',\n",
    "    'repetitions': 1,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:31:27.952106400Z",
     "start_time": "2023-12-10T12:31:27.924790900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Equivariant Quantum Circuit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def graph_encoding_circuit(edges, num_qubits, reps, params, insert_barriers = True) -> QuantumCircuit:\n",
    "    # Create a quantum circuit\n",
    "    circuit = QuantumCircuit(num_qubits)\n",
    "\n",
    "    # Apply Hadamard gates to all qubits\n",
    "    circuit.h(range(num_qubits))\n",
    "\n",
    "    for rep in range(reps):\n",
    "        edge_w = params[rep][-1]\n",
    "\n",
    "        # Edge encoding\n",
    "        for edge_i, edge in enumerate(edges):\n",
    "            circuit.cx(edge[0], edge[1])\n",
    "\n",
    "            circuit.rz(edge_w[edge_i], edge[1])\n",
    "\n",
    "            circuit.cx(edge[0], edge[1])\n",
    "\n",
    "        # This barrier is just to improve visualization, it can be removed\n",
    "        if insert_barriers: circuit.barrier()\n",
    "\n",
    "        # Vertex encoding\n",
    "        for q in range(num_qubits):\n",
    "            circuit.rx(params[rep][q], q)\n",
    "\n",
    "    return circuit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:31:27.952106400Z",
     "start_time": "2023-12-10T12:31:27.927821900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1290.83x367.889 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/UAAAEvCAYAAAAEgeGZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABxlUlEQVR4nO3deVhUZf8G8HuGAYZVdtlEFBBwN7fUzN0kldzTSCtL803T10zfsnxLy8r0zTXL0lLLyF3JXVOTzAV3RQREBJF1kJ1hGWZ+f/BrigBhWOZwZu7PdXEVZ55znltlDvM9z3OeI9FoNBoQERERERERkehIhQ5ARERERERERHXDop6IiIiIiIhIpFjUExEREREREYkUi3oiIiIiIiIikWJRT0RERERERCRSLOqJiIiIiIiIRIpFPREREREREZFIsagnIiIiIiIiEikW9UREREREREQixaKeiIiIiIiISKRY1BMRERERERGJFIt6IiIiIiIiIpFiUU9EREREREQkUizqiYiIiIiIiESKRT0RERERERGRSLGoJyIiIiIiIhIpFvVEREREREREIsWinoiIiIiIiEikWNQTERERERERiRSLeiIiIiIiIiKRYlFPREREREREJFIs6omIiIiIiIhEikU9ERERERERkUixqCciIiIiIiISKRb1RERERERERCLFop6IiIiIiIhIpFjUExEREREREYkUi3oiIiIiIiIikZIJHYCIiIiIiKiuIiIidGqvUCiwZ88ejBkzBk5OTrXap3v37nWJRqQXHKknIiIiIiKjoVAosHHjRigUCqGjEDUIFvVEREREREREIsWinoiIiIiIiEikWNQTERERERERiRSLeiIiIiIiMho2NjYYNmwYbGxshI5C1CAkGo1GI3QIIiIiIiKiutB19fu64Or31JRxpJ6IiIiIiIxGcXExHjx4gOLiYqGjEDUIFvVERERERGQ04uPjMXbsWMTHxwsdhahByIQOQFXTaDSAmK4emptDIpEInYKIiIiIyOhpNBqolCKqJZoYmYW4ahsW9U1VcTFUE14SOkWtyXZsAeRyoWMQERERERk9lbIY23xeFDqGaIXE/QhTS/HUNpx+T0RERERERCRSLOqJiIiIiIiIRIrT74mIiIiIyGgEBATg4sWLQscgajAcqSciIiIiIiISKRb1RERERERkNBISEjB16lQkJCQIHYWoQbCoJyIiIiIio6FUKnHr1i0olUqhoxA1CBb1RERERERERCLFhfKIiIiIiIioQbj2aodhexZX2FZaoETuvRTE7TqDqE2HoClTC5TOMLGoJyIiIiIiogZ1b084kk5eASQSWDjbwXd8P/RY/DKa+Xng3PwNQsczKCzqiYiIiIjIaLi5uWHx4sVwc3MTOopBy7wZj3u7w7XfR28+itHhq9HmhUG48lkoijNzBUxnWHhPPRERERERGY1mzZohKCgIzZo1EzqKUVEpi5FxJRYSqRS2LZsLHcegsKgnIiIiIiKjkZWVhZ07dyIrK0voKEbHxru8mC/Ozhc4iWHh9HsiIiIiIjIaaWlpWL58OTp06AB7e3uh4xgsmYUZzB1stPfU+08ZCscOrZFxJRa591KEjmdQjGKkXqFQYMGCBfD19YVcLkeLFi0wZ84cFBQU4NVXX4VEIsG6deuEjkmNrKxMjazcYuTml0Cj0Qgdh0jv1GoNcvJKkJ1bDLXaON8DyiIVMrOLUFrKVXeJjJFareFnASI96bJgIiZFfo9Jt77DqFNfIPCVYbh/8DxOvrxM6GgGx+BH6q9du4agoCCkpqbCysoKbdu2RXJyMtasWYO4uDg8evQIANC5c2dhgzaS3xTpGHLuND5r2xFv+QRU2cbslx141sUN+3r21XM6/Yi4lYH126Pw85F7KCouAwA428vx2hh/zJgQAC83a4ETEjWu6PhsfLXjDjaHxSInrwQAYGNliskjfPHG84Fo52vYoxT5haX48cBdrN8ehZuxf021HNjDDW88H4jg/i1hamoU17iJjNa1O5lYvz0K2w7GobBIBQBwtDPH1FFtMGNCAFp72gqckMjwRP9wDPd/OQepqQz2AV5oP3MUrNwcUVZcom0jNZNh5LHliN8bjhur92i3P7VqJuTOdjgRslSI6KJj0J9iFAoFRo4cidTUVMybNw8pKSm4cuUKUlNTsWzZMhw8eBARERGQSCTo2LGj0HGpgRWXlGHywtPo8UIYNu+P1Rb0AJCRVYRPN11Hq6AdWP3jLQFTEjUejUaD99ZcQsBzu7F6W6S2oAeAvIJSrN8ehfZj9uCt5ecNduT+3PU0tAragX99/EeFgh4ATl5Mwbh5J9Flwl4kpvDePiJDpFKp8fqS39Flwj58uztaW9ADQGZ2MZZvvgnf4Tvx6cbrHLknamC591KREn4TD09exa31+/HrS5/BqbMPei17XdtGXaLC77PXosPsMbBv2xIA4DWsOzyHdMPZt9YLFV10DLqonz17NpKSkjBr1iysWLECNjY22tcWLFiATp06QaVSwdvbG7a2vEJrSMrK1Jj0n1P48UDcY9up1Rr8+/ML+N+Wm3pKRqQ/8/93EZ9svF5ju5U/RGLm0j8M7gPtxZsZGPTaYSiyih7bLjIuG0+/chApGYV6SkZE+qDRaPDyojP4Zld0De2AhWsu4aMN1/QTjARnaWmJnj17wtLSUugoRiXjUjTidp1Bq1F94NzNX7s988Y9RH4Vhr5r3oSlmwN6LZ+BCws3QpnGhQxry2CL+qioKGzfvh1OTk749NNPq2zTtWtXAECnTp0qbI+Pj0dwcDBsbGxgb2+PKVOmIDMzs9EzU8P5dnc09v6aUOv2b//vIm7FPmrERET6deL8Q/xva+1noXy98w7CTic2YiL9KitTY8L8k1D+bYbO4yQk52PGR2cbORUR6dO2g3HYdvDxF/f/7oP1V3DhRnojJqKmwsvLC2vXroWXl5fQUYzO9ZW7oFaVocv85ytuX7Ub6rIyBB9fjtSztxC/n7+TdWGwRX1oaCjUajVCQkJgbV31PdMWFhYAKhb1eXl5GDBgAJKSkhAaGopvvvkG4eHhGDFiBNRq8S6sVFhWBkVxcZVfhkaj0eDLn6N03u+rHXcaIQ2RMOryHvjy59uNkEQYB888QEKyblPqD5x5gITkvEZKRET6ti5U93Pa+u26nztJfMrKypCfn4+ystpd+KWGk3c/FfH7z8L96Y5w6Rmo3a5RlSEjIhpyx2a4u/2UgAnFyWCL+pMnTwIABgwYUG2bpKQkABWL+m+++QYPHz7Evn37MGLECIwfPx4//fQTzp8/j7CwsMYN3YiWREfC/dj+Kr8Mzbnr6bh1V/fpOlt/iUVBYWkjJCLSr+T0gjqNuh8/l4y4B7mNkEj/NuzS/SKdWq3Bxj0xjZCGiPTt2p1MXLiZofN+24/GIyvX8AY8qKLY2FgMHDgQsbGxQkcxSjdWl4/K/3203qVnIHyfH4CoTYfQY8krMJGbCZhQfCQaQ7uJ8v+1aNECSUlJuHr1apUr26tUKri5uUGhUCAuLg6tW7cG8NdFgFOnKl4h8vHxQf/+/bFp0yads3Tr1g2pqak67WMhleJ251469/VPf65+/5pXa4x1b1Flm6Dzv9V79fu2185B2URmMhSYd0W2VXCd9nXJXgNTNW+1IHErlnlDYftKnfZ1yNsGi1LxF7apzf6NMhPdV/WXl9yGY/72RkhERPpUaNYBWdbj6rSvc87XMCvjM7TFZNw43f6t09PTERoaikmTJsHFxaVW++zatasu0QRjqpHiA3UPoWPUisxSjuBfV+D2hgO4s+UogvYugeJ6HCI+2CxYpsXSiyiV6L+2cXV1xaVLl3Tez2AfaVdQUAAAUCqVVb6+fft2KBQK2NjYoFWrVtrtt2/fxvjx4yu1b9euHW7frtvU1NTUVDx8+FCnfSxNTIDOdequSr7W1hjk3LzhDvgPycnJKGwqU5gc2wBWdds1PeMRUKTbvxVRk2NtB9Rx7c9Hj3KAXAN4D9hIABPddysqKtX5fE1ETZC9N1DHJ9ZmKLKAQp4HxOTPz/219Wd9oFQqa72v2H43mElMgMb76N+gun84BfmJ6biz+QgA4Pc56xB8YgUSD19A2nlhbolJTklGiaaJ1Da1YLBFvaurK7KysnDlyhX06lVxxDslJQXz588HAHTs2BESiUT7WlZWFuzs7Codz8HBAdHRj1899XFZdGUhFdedEe7u7k1mpL7QzAw6T77XaACJBK5O1jDReDRGLCK9KTGxhO6TTss52cthbiP+90CapBiqmptVYmmuhr2H+P/8RMZOaWoGnZe//f/PAi6OVjC153lATKysdBvN+bOQt7CwqPW+HiL73WCqkQJN46P5Y3kM7IJWwX2wf9A87ba8hDRcXroNfVbORNjAeVAp9X9LjLubu2Aj9XVhsEX94MGDERUVhWXLlmHIkCFo06YNACAiIgKTJ0+GQqEAgCqn5je0ukyh0BQVQTXhpUZI0zhiYmIgkcuFjgEAyM4thsfgnys8i7ZGEgl6dXLBHz9wgRwSv7IyNfxG7EL8Q90WfXN1skDi5XCYmorromJVFn91BR9+dVXn/bZ/swAj+q1rhEREpE+FShU8BociO6+k9jtJJOjgZ4/ru65VGPChpi8iIkKn9nfu3EFoaCiCgoIQEBBQq31WrVpVh2TCKS0swjafF4WOUaOHJ6/ip4DKNc+dzUe0I/dCiImNgall06htakP8n9yqsWDBAjg6OuLBgwdo164dOnToAD8/P/To0QOtW7fGwIEDAVR+nJ29vT2ys7MrHe/Ro0dwcHDQR3SqJztbc4QM99F5vzeeD6y5EZEImJhIMWN87T6k/N30sQEGUdADwLSx/pCZ6PahvKW7NYKe8mykRESkT5YWMrwyyk/n/d54PpAFvRHw9fXF0aNH4evrK3QUogZhGJ/equDp6Ynw8HAMHz4ccrkc9+/fh4ODAzZs2ICDBw8iJqZ8Iah/FvWBgYFV3jt/+/ZtBAay6BOLBa90hJ1N7VfN7BLgiHFDvBsvEJGeTRvnj9aeNrVu79ncCjMnGs45zt3FCnNC2um0z9I3u8LExGB/LRIZnbmT28PZvvYjbW197DB5BIs8YyCTyWBvbw+ZzGAnLZORMehPL4GBgThw4ADy8vKQl5eHCxcuYPr06SgoKMD9+/chlUrRvn37CvuMGDECv//+u/ZxdwBw4cIFxMXFYeTIkfr+I1Ad+XrZ4pe1Q9DMuubCvq2PHQ5+ORRyc57YyXDY25rj8Ppn0MK15nsFXZ0scHj9ULg4Wughmf4sm9sdL46o3ayd5W/1QMhwfpgnMiQtXK1xaP1QONqZ19jW18sWh9c/AytLUz0kI6ElJSVh3rx5FT7vE4mZQRf11YmMjIRGo4Gfnx8sLS0rvDZ9+nS4ubnhueeew4EDB7Br1y5MmjQJPXr0wHPPPSdQ4rrr5+SCkpET8JZP9VNxS0ZOqNfj7Jqqp55wxbkfR2L80FZVTsNtZmOG2S+0xdktI+DmbFnFEYjErY13M5z/cSReG9MGFvLKS8Gbm5ng5ef8cGFbMNr7Gd7tRSYmUmz5uB/WvtsLPi2qnrXQs4Mz9q0ejLdf7qDndESkD93aOeP8j8F44VkfmMoqf+y1tpThXxMCcO6HkfByq+Ny+SQ6+fn5CA8PR35+vtBRiBqEUQ5N3rx5E0DlqfcAYGtri5MnT2LOnDmYOHEiZDIZRowYgZUrV0IqshXpCQhsbYcdKwYiOb0AO4/F4/11l5FfqIKdjRmSjk/kFXkyeO4uVvj2w75Y/lYP7DgWj7dXXEBeoQrNrE0Rd2gCHO3EswhMXUilEsya1BZvPB+I4+ceYvzbJ5FXUApbK1Oc3PQsurZ1EjoiETUyXy9bbPusP76Y3xM7j8Xj3dUR2s8Ciceeh41V7W/XIyJqioyySn1cUQ8APj4+OHDgAPLz85GdnY0ff/wRzs7O+oxIDczdxQpzXmyvnY5vZSFjQU9Gxc7WHNPHBcD2/98D1pamBl/Q/51UKsEzfTxha1X+vrexMmVBT2RkmjtaYNakthU+C7CgJyJDwKKeiIiIiIjIwL2csgtmtvq75XTcxfUYHb4afi8M0m7zmzQQY86uxZhz69B7xQxIZOW3B7r0DETw8eX1yjhs92KMvfAlOs0dp93mObgrRoevxpizazFg03yYWpevH2TTsjmCjy/H5IRQOLTzrvsfsokwyqL+5MmT0Gg0GD58uNBRiIiIiIhIj5ydnTFnzhzOxG0sEkn5F4DfZqxE7E+/AgCsW7igy4KJODxqEfb0mgW5UzP4vzgEAJB+IQphQ+bXu+uIDzbj+spdAACZpRx9vvgXTr6yDHv6vInC1Efagj8vIQ1hQ+ZDmZZV7z6bAqMs6omIiIiIyDg5OjoiJCQEjo6OQkcRTLf/TsGIw58h+PhyDNu7BLY+7gCAdjOC0Wv569p2ZraWmBj5HczsrLWvDz/0KUYe+xxDfnoPVp7lt7J1njcB/Te+jSGh72PU6S9g2dy+Up8tRzyJB8cuQZmRDQCI3noMrUb30Sn36N/XwLHTX0+28Z3QHwO+q/pigMfALnh0Kx45d5MBAHe2HEWrUU/p1J9YsKgnIiIiIiKjkZubixMnTiA3N1foKIK5+eU+HAh6B2FD5iN681H0/OgVAEDsTyfgNayHdgq878SBSDwSgZLsfLQa/RSa+brj0Ij38MvQBYjbE44nP52mPaZL1zYIf3Mt9vWbi8LUR5X6tPZwQn5Shvb7/KQMWHnotr7N3R2n4Tuhv/Z734kDEBt6ssq2lfp7kA6L5naQmBheCWyUq98TEREREZFxSk5OxsKFC7F161bY2toKHUcQ7k93RODUIJhaW0AilcD8/0fiS3ILkXDgHHwnDcTtDQfgP2UofpuxEgDgNawHnDr7YOTRZQBQqThO+vUKihQ5jZo7budpBB9fjojFW2Dp6gDb1u54ePJqo/YpBizqiYiIiIiIjISVhxOeXPoqDgS9g7yENNgHtkTQ3iXa16M2HcLAze8gJ/YhijJz8ehWPABAIpHg5tq9iPnxRJXHVRUWPbbf/IcK2Hq7ar+39nRGwUOFTtkLUx5Bcf0evJ7pDjv/Fri3+ww0Zepq+3Pv1/Gv/lq4QJmWXW17MTO8uQdERERERERUJVMbS6hVZShML18kLmDqsAqv59xNRn5iGnp//jrufH9Yuz3xyEX4Txmqvb9eIjOBQ/tWte434eB5tBjaDRbOdgAA/ylDEb/vrM757/58En6TBsJnfD/E/lz11HsAeHjqKhw6tEYz3/L1AgJeegbx+3XvTww4Uk9ERERERGQksu8kIn7/WYw6vRLFWflIPHKxUpuYbSfQc+lruH/gvHbbvT3hMLe3wbBdHwIoL+rvhp7UjuTXJD8xHVdX7EBQ2McAgNQ/IhH9w3Gd8ycejcCTn01HXnwKcmIfVttOVVCEP+Z9hYHf/wcSEymyox8gfPY6nfsTAxb1RERERERkNMzNzeHv7w9zc3Oho+jVZre/nt9+cdH3uLjoe+33N1btrtDWtXd73NlyFBpVWYXtUZsOIWrToUrHvva/HbXKELvtBGK3VT19v7bUJSr83O6VWrV9cOwSHhy7VK/+xIDT74mIiIiIyGi0atUKP/zwA1q1qv3UcWNh0dweo8NXw7FDa9z+5kC9jlWUmYu+62bD74VBNbZ16RmI4OPLoUzPgkatqVN/xdn5eGJhiPZZ9I9j07I5go8vh0RmAnWpqk79NSUcqSciIiIiIiIo07Kwt++cBjnWgaB3at02/UIUwoaUP29e7mhbYeG+PyWfuYFLH/1Q7TFOvbq81v3lJaRp+zMELOqJiIiIiMhoREdHY+rUqfjuu+/g7+8vdBz6h6LMXIMquPWB0++JiIiIiMhoaDQalJaWQqOp2zRvoqaGI/VNlbk5ZDu2CJ2i9oxsoREiIiIioqZKZmGOkLgfhY4hWjILcdU2LOqbKIlEAsjlQscgIiIiIiKRkUgkMLVkLWEsOP2eiIiIiIiISKQ4Uk9EREREREbD29sboaGh8PDwEDoKUYNgUU9EREREREZDLpfDx8dH6BhEDYbT74mIiIiIyGikpKTg448/RkpKitBRiBoEi3oiIiIiIjIaOTk5CAsLQ05OjtBRiBoEi3oiIiIiIiIikWJRT0RERERERCRSLOqJiIiIiIiIRIpFPRERERERGQ2pVIouXbpAKmUpRIaBP8lERERERGQ01Go1rl69CrVaLXQUogbBop6IiIiIiIhIpFjUExEREREREYkUi3oiIiIiIiIikWJRT0RERERERsPGxgbDhg2DjY2N0FGIGoRM6ABERERERET64uHhgSVLlggdg6jBcKSeiIiIiIiMRnFxMR48eIDi4mKhoxA1CBb1RERERERkNOLj4zF27FjEx8cLHYWoQbCoJyIiIiIiIhIp3lPfRGk0GkBMU4LMzSGRSIROQQA0GqCoTOgU4iU3AfijTERERGKm0WigUoqolmhiZBbiqm1Y1DdVxcVQTXhJ6BS1JtuxBZDLhY5BKC/o+x4SOoV4hT8LWPDMSERERCKmUhZjm8+LQscQrZC4H2FqKZ7ahtPviYiIiIiIiESK41FERERERGQ0AgICcPHiRaFjEDUYjtQTERERERERiRSLeiIiIiIiMhoJCQmYOnUqEhIShI5C1CA4/Z4MXqFShesxmYi+n4P8wtLybUUq3Ix5hMDWdpDJeG2LDFtJaRluxWYhMi7rr/eAUoVLkRno4OcAczMTgRMSERHpj1KpxK1bt6BUKoWOQtQgWNSTQUrPVGLT3hiEHo7D7XvZKCvTVHg9K7cEHcfthYXcBN3bOWPqqDaY8EwrWMj5liDDkFdQgh8PxGFLWCyu3slESam6wutZeSXoPikMpjIpOvk7YPIIX0wZ6Qs7W3OBEhMRERFRXbCCIYOSklGId1ZF4Ocj9yoVMVVRFpXhzOVUnLmcirdWXMCsiW3x7msdITfnW4PEKTe/BB+sv4KNe2K0o/KPU6pS41KkApciFXh39SW8MsoPH8/qyuKeiIiISCRYuZBB0Gg0+OlQHN789ByyckvqdIxHOcVYsuEqdp2Ix/dLnkaPDs4NnNLw5d08jZj3B1TYJpVbwdy9DRz7T4bLiDchMeFpp7GcOP8Qr34QjsSUgjrtX1ikwpc/R2HvyQR8+8FTeLZviwZOSERERIbOtVc7DNuzuMK20gIlcu+lIG7XGURtOgRNWc2Db1R7/HRNoqdSqTFt8e/YvD+2QY53Oy4bvSb/gi8X9sKMCYENckxjY//0JDTr+iyg0aA0KxWZp7ci6bu3UJQUhZYzvxE6nsHRaDRY8vVVfPjV1QY5XnJ6IYbPPIYFr3TAZ//uDolE0iDHJSIiagrc3NywePFiuLm5CR3FoN3bE46kk1cAiQQWznbwHd8PPRa/jGZ+Hjg3f4PQ8QwKi3oStbIyNULePY0dR+Mb9LhqtQb/+vgPFJeUYc6L7Rv02MbAsvUTcOz/ovZ752ffQOQbAVAc3wj3F5fCtBlnQTSkd1dfwrLvbjT4cT///iYKi8qw5p0nWdgTEZHBaNasGYKCgoSOYfAyb8bj3u5w7ffRm49idPhqtHlhEK58ForizFwB0xkWLvtNojZ3+YUGL+j/7t+fX8COo/ca7fjGwkRuBSv/JwGNBsWpcULHMShrf4pslIL+T+tCb+OzTY13fCIiIn3LysrCzp07kZWVJXQUo6JSFiPjSiwkUilsWzYXOo5BYVFPonX83EOs/em2TvtEhAbjwfGJiAgNrvU+Mz46i5SMQl3j0T/8WczLrB0ETmI47sRnY/4XETrtU5f3wH/XX8a1O5m6xiMiImqS0tLSsHz5cqSlpQkdxejYeJcX88XZ+QInMSxGUdQrFAosWLAAvr6+kMvlaNGiBebMmYOCggK8+uqrkEgkWLdundAxSQe5+SV47cPwmhv+g6uTJTybW8HVybLW+2TllmDGR2eh0WhqbkwAAHVxIVS5CpTmZEB5/yYSv54J5b2rsPTrAblHG6HjGYSyMjVeWXQGxSVlOu1Xl/eASqXBy4vOoKRUt76IiIjIeMkszGDuYANzR1vYBXih5yevwbFDa2RciUXuvRSh4xkUg7+n/tq1awgKCkJqaiqsrKzQtm1bJCcnY82aNYiLi8OjR48AAJ07dxY2aCP5TZGOIedO47O2HfGWT0CVbcx+2YFnXdywr2dfPaeru7U/3a7zCt91EXY6EeGXU/F0Ny6oUhspoR8gJfSDCtvseo2B1+tfCpTI8Ow6fh/nb2Torb/r0Y/w44E4TB3NizJipFZrcOyPh9h5PB6KrCKYmUoR2NoOr43xh5ebtdDxiEgP0jKV2LQnGtdjHkFZVAZ7WzM8N6Algvt7QSYzinE+0rMuCyaiy4KJFbbdP3geF97dKFAiw2XQRb1CocDIkSORmpqKefPm4YMPPoCNjQ0A4PPPP8d//vMfyGQySCQSdOzYUeC0VFsqlRobdt3Re79f7bjDor6WnJ6ZDvve46EpK4Uy4SZS9yxDiSIJElO5tk1eZDjuLqm8SI1GVQKNugxd93JU+HHWb4/Se59f/nwbr4zy46J5IrP9yD0sXHMJ95LyKr229NvrCO7vhfXv9Yabc+1nbxCReGTlFmP2Z+ew/Ug8SlUVHyO29Ze78HCxxH9ndMH0cVUP/hDVVfQPx3D/l3OQmspgH+CF9jNHwcrNEWXFfz1+Wmomw8hjyxG/Nxw3Vu/Rbn9q1UzIne1wImSpENFFx6Avy82ePRtJSUmYNWsWVqxYoS3oAWDBggXo1KkTVCoVvL29YWtrK2BS0sXB8Ad4kKq/Ufo/7T5xH6kK3ltfG+ZufrDtPBjNugbBdcwC+L73CwrvRiDxqxnaNjbt+qLL9vwKX+3Wx0Bm4wT3Fz4SMH3Tdyv2Ec5cTtV7v1eiMnHxpv5mB1D9rf7xFiYuOFVlQQ+Uj+DvO5mAXpN/QUJy1W2ISLwUWUXo+9IB/HggrlJB/6eH6YV4fclZvLfmkp7TCcfS0hI9e/aEpSUvZjam3HupSAm/iYcnr+LW+v349aXP4NTZB72Wva5toy5R4ffZa9Fh9hjYt20JAPAa1h2eQ7rh7FvrhYouOgZb1EdFRWH79u1wcnLCp59+WmWbrl27AgA6deqk3fbnRYAePXrA3NycI1JN0IHfEgXpt1SlxvFzDwXpW+ysA3vDof9kZP2+HflRf1TZRl1ajHufjYF126fgNn6hnhOKy8HwB0bZN+nmUPgD/PvzC7Vqm5Ccj+Ezj6G0tOoP/UQkPhqNBmPmnkBkXHat2n+y8Tq27I9t3FBNhJeXF9auXQsvLy+hoxiVjEvRiNt1Bq1G9YFzN3/t9swb9xD5VRj6rnkTlm4O6LV8Bi4s3AhlGp9OUFsGW9SHhoZCrVYjJCQE1tZV3y9oYWEBoGJRf/fuXezevRuurq7o3r27XrLqQ2FZGRTFxVV+ic3l28Ktwi1k32Ln9vwiQGqC5J/+W+XrietnQF1aBO85m/UbTISEfQ8oBOubdPPJxus6tY+My8a+UwmNlIaI9O3M5VSEX9FtdfelG69BrTb8hYHLysqQn5+PsjLe6qdv11fuglpVhi7zn6+4fdVuqMvKEHx8OVLP3kL8/rMCJRQngy3qT548CQAYMGBAtW2SkpIAVCzqn376aaSkpCAsLAyDBw9u3JB6tCQ6Eu7H9lf5JSbFJWW4dVe4q3YsaOpO7uYLh74TkXfjV+RFVnxyQfova5Bz6QB83t0HqTmnwtVEyJ/Dy7cz+SQIEbgenYmzV3V/VJMQazUQUeOoy/s5NiEXJy8mN0KapiU2NhYDBw5EbKxxzExoSvLupyJ+/1m4P90RLj0Dtds1qjJkRERD7tgMd7efEjChOBlsUZ+QUD7a0LJlyypfV6lUOHu2/ArQ34t6qdQw/0pe82qNw0/2q/JLTFIVhdXeE6YPial8pmZ9uI5/D5BKK4zW5904haSt/0HrBTth3txbuHAiIuTPYVqmEioVi/qm7sjZpDrtdzoiBUXFqgZOQ0RCqOt54PDvdduPqLZurC4flf/7aL1Lz0D4Pj8AUZsOoceSV2AiNxMwofgY7Or3BQXlC6kplcoqX9++fTsUCgVsbGzQqlWrRs3SrVs3pKbqtqiVhVSK2517NVgGX2trDHJu3mDH+6c2bdpAqW78YrtU6gjYza729YjQ4Mc+f9vVyUL73wfHJ1bbLlVRiO6TwiptT3qYBk9PTx0S65/EzALNVwlz5dmmQ3903V99wWfRIrDCqvbFafdxb/kEeL68HDYd+ushYc3atPGDpqTq80ZToIEEKocPq329sd8DANCylS+kKKnytaYuxe4tQNoMKakpTf69XB85FoMAi6frtG8r33Yw0eh/MVIifTGG84AGEuQ6fFinfb/+diu2rxrboHka27hx43Rqn56eDgA4fPgwLl++XKt9Ro8erXMuIZlqpPgAPQTpO/VcJDa7Vf9vkhP7EFs9/yroZZZyPLVqJi4v3YY7W44iaO8SPPHuC4j4YLMe0latjV8blEr0P5Do6uqKS5d0X7TSYIt6V1dXZGVl4cqVK+jVq2JxnJKSgvnz5wMAOnbs2OiL4aWmpuLhQ90WWLM0MQE6N06expCcnIxCfdyXZFoM2FX/squTJTybW9V4GJmJtFbt/kmtKtH531LfpOaWaLzLNw1HXVyIuE9HoVmPYLgMnyV0HK3k5GSoi5v4Uw7sywCJSZUvNfZ7AABSkh8AGpGO5tqUAVJAXVbW5N/L9eKcCVjUbdfU5ARALb71VohqzVjOA3bFgNRc590K8x6hMEVcfy9/DubV1p+Dfkqlstb7iu1nxUxiAlF8IATQ/cMpyE9Mx53NRwAAv89Zh+ATK5B4+ALSzgtzW1hySjJKNOJZc8Fgi/rBgwcjKioKy5Ytw5AhQ9CmTRsAQEREBCZPngyFovye1M6dOzd6FldXV533sRDZbQDu7u56GalXwwwpj3m9pkfOuTpZQGYihapMjVRF9aOx1R3HVFoEFw+P2kQVjMSsjp/k9Szrj91Qxl9H0cMYZP2+vdLr7dbdhpmz/leldXd3b9Ij9QCQoimEWmJT5WuN/R6QaIrg5t4cYn0uSIqJCdQApCYmcGvi7+X6KJLlIhMANBpAhwvXsrJ0uLg5ifbfl6g2jOU8kFGWjBKp7rNR7eXZsBTZ34uVlW4Xqf8s5C0sLGq9r4fI/k5MNVJABA808RjYBa2C+2D/oHnabXkJabi8dBv6rJyJsIHzoFLq/0Kzu5u7YCP1dSHRGOiKR0lJSejcuTMyMzMhk8kQEBCAoqIi3L17F0FBQVCr1Th69Ci++eYbTJs2rcpjfPjhh1i8eLEgi0JpioqgmvBSvY/zmyIdQ86dxmdtO+Itn4Aq25j9sgPPurhhX8++de5HtmMLJHJ5nffXhf/IXYhJyKnTvg+OT4RncyskpRWgxZCfdd7/tTFt8O2Hdf970gelCuh7SOgU4hX+LGDRxC93Dp95FIfC63bPY33fA/26ueL0d8Pr1HdT4Dk4FA/TC+HhYomkE5OEjtNo1GoN/IN34W5irk77rX23F2ZNattIqYiaBmM5D+w4eg/Pz9dtwTEnezmSjk+EuVnVs8GaqoiICJ3aq1Qq5OXlwcbGBjJZ7X7pi+2pWKWFRdjm86LQMUQrJO5HmFrqp7ZpCOIaDtaBp6cnwsPDMXz4cMjlcty/fx8ODg7YsGEDDh48iJiYGAAVF8kjceja1lHAvp0E65voT0L+HPI9IA5SqQRzQtrptI+9rRkmj/BtpEREpG+jBraEl5tuI9j/mhAguoK+LmQyGezt7Wtd0BM1dQZb1ANAYGAgDhw4gLy8POTl5eHChQuYPn06CgoKcP/+fUilUrRv317omKSjAd3dBOu7v4B9E/1JyPeAkH2Tbt54PhAhw31q1dbcTIp9qwajmQ1XGyYyFGamJghbMwS21qa1aj+sjycWTe/SyKmahqSkJMybN0/7eGsisTPoor46kZGR0Gg08PPzg6Vl5VWid+3ahV27duH27dsVvq/LSoRC6+fkgpKRE6qdeg8AJSMn1Gvqvb5NetYHNla1+wXVkPp3d0NAKzu990v0T/27u8Hfu5ne+/Vys0LQU4a5UrQhkkol2PLx05g7uR1MTKq/S96zuRVObnwWT3fjBRsiQ9PJ3xG/bx4Bv5a21baRSICXn/PDvtWDYWpqHKVBfn4+wsPDkZ/PRxWTYTCOd+4/3Lx5E0D1U+/Hjx+P8ePHY+fOnRW+X7dund4yUvWsLU3xUrCf3vt94/lAvfdJVBWJRCLIz+OM8YEwMTHKXxuiZWIixRfzn0TCkefxwYwuaOtjB+n/1/fmZlLs/mIQ4g9PQO/OIlkimYh01qGNA+7sH4fD659BcH8vmPz/SUBmIsGCVzrg7sHx+P6jp41i2j2RoTLKT2c1FfUajabKr82bN+sxJT3OvCntYaXH1cw6tnHAqAEt9dYfUU1eGeWHFq51eyRdXTR3tMDr46uf8UNNm0dzK3z4xhOI3DsWbs7lM9Sc7OQYM9gbMplRfhQgMipSqQTDnvLE/jVD4OpU/pSc5o4WWDa3B1p7Vj+KT0TiYJS/yWsq6qnp8/awwedz9bMKqUwmweaP+op6Strl5yRQ5Wfrrb+b07xx61/+UBzbqN2mOL4Jt2b44ebrPkhYNw0aVSkAIC8yHLf/3bleGZNDP8T1yc64+3GwdltRcizuLOiNW/9qg6h53aFMjNS+Fv3eAFwLcUBa2Ko69dcU2FiZYeOHT+mtv68X9YZDM92fd0xERERNw8spu2BmW/nW48Yy7uJ6jA5fDb8XBmm3+U0aiDFn12LMuXXovWIGJLLyGSIuPQMRfHx5vTIO270YYy98iU5zxwEAZJZyDAl9HxMjv8MLd7ZUaGvTsjmCjy/H5IRQOLTzrtsfsAkRb5VSDydPnoRGo8Hw4eJ9LBMBMyYEYvCT7jrtk6ooRFJaQY3P8v67917rjC6BXPG7NjRqNTTq8md6tp6/HU5DXwMAFKfFI3nbIvh/Go72X99FaXYaMo5+AwCwadcXbVddq3ffDv1C4Pt+mPb7xPWvw+mZ6Wj/VQxcx/wH91e/rH3Nf+kp2PUIruIo4jK0t6fOo+d1eQ+EDPfBqIHeOqYjIiJqmpydnTFnzhw4OzsLHcUwSSTlXwB+m7ESsT/9CgCwbuGCLgsm4vCoRdjTaxbkTs3g/+IQAED6hSiEDZlf764jPtiM6yt3AQDUKhVurtuHYxOWVGqXl5CGsCHzoUzLqnefTQGf40CiJZVKsGPFQPSfegg3Yh7Vap/uk8JqbvQ3k0f44r8zDGsl2KTv30berd+gKSuFiYUtWs78FnJPf6TuXYHi5Bi0nFlebKvys3Frhi/afxUDmY0DUveuQNbZHdCUqWDazAVeb2yAuUtLJId+CGXCTaiV+ShRPIDfkuOV+sw6uwvNegTD1N4VAOA8bAZSdn0Cl+EzdcpeXYZ/Ks1OR8HdS/BbfAwAYNd7LBK/mYWilLuQuxnWI7vWvPMkEpLzceRs7Vbw1fU90K+bK779QH8zAoiIiBqbo6MjQkJChI4hqG7/nQLXXm0hlZmgJF+JP97+GrlxyWg3Ixi2Pm44N38DAMDM1hJjzq3Dnj6zUZKdj3YzguEd3AtSmQmKFDn4Y8EGFCQp0HneBNgFesHUSg4rd0cce/6jSn22HPEkHhy7BGVGNgAgeusxdJwzBnc2H6l17tG/r8GZmauReT0OAOA7oT9aDOuOU1OXV2qrLlEh9ewtWHsa/sUboxypJ8Nhb2uOX78NapRnZ78yyg/fLekLqbT6VaPFqPmY/yDwfxFou+oanIPewIONcwAATkNeQ/aFfdop8Jm/fg+7ns9BZuOAR7/9hOKH0QhYdg5tV16BQ78QJH79hvaYBXfOwXvuVrT78jbMHD0q9VmiSITZ34pvMxdvlGQk6pS7pgwV+3sAU3s3SEzKr1tKJBKYOXvp3KcYmJmaYM/KQRjxdIsGP/aQXu44sG4oLOS8/ktERIYjNzcXJ06cQG5urtBRBHPzy304EPQOwobMR/Tmo+j50SsAgNifTsBrWA/tFHjfiQOReCQCJdn5aDX6KTTzdcehEe/hl6ELELcnHE9+Ok17TJeubRD+5lrs6zcXhamVB9ysPZyQn5Sh/T4/KQNWHrp9hr+74zR8J/TXfu87cQBiQ0/qdAxDxE9qJHpO9nL89t2zeGf1JawLvV3v41lZyPD53O6YMSHQ4Ap6AMi7dhzpB9eiTJkHqNVQ5ZefdGXWdrDvPQ6Zv34Hl+C5yDjyFVrP3w4AyL6wDwWxEYia1xUAoFGXVTimbddnYWrXuKtn15TBmFnIZdi7ajCWfXcDi7++ilKVul7HMzGRYOFrnfD+9M4wM+VqyEREZFiSk5OxcOFCbN26Fba2xrlQoPvTHRE4NQim1haQSCUwt7MGAJTkFiLhwDn4ThqI2xsOwH/KUPw2YyUAwGtYDzh19sHIo8sAAJJ/PBEn6dcrKFLkNGruuJ2nEXx8OSIWb4GlqwNsW7vj4cmrjdqnGLCoJ4NgZWmKte/2wtjB3pjx0VlE36/bCWVobw98/X4ftPK0aeCETUNJRiISv5mFwBURMHfzQeH9G4hZ+LT2dZcRs3F3aTDknoGQ2TrDsnX5rQcajQau496F8zPTqzyuiYX1Y/s1c/JCcWrcXznS78PM2Uun7DVlqNhfC5RmpUBTpoLERAaNRoOSjESd+xQTmUyK96Z3RnB/L0xf8jvO38ioeacqPBHoiG8/eApPNMLsFyIiIhKelYcTnlz6Kg4EvYO8hDTYB7ZE0N6/7juP2nQIAze/g5zYhyjKzMWjW/EAymc+3ly7FzE/nqjyuKrCosf2m/9QAVtvV+331p7OKHio0Cl7YcojKK7fg9cz3WHn3wL3dp+Bpqx+gxmGgNPvyaD07+6G2/vG4ujXz+C5AV61Gmm3tjTFvyYE4Mau0Tj69TCDLegBoKwgBxITU5g6uEGj0SDj4LoKr8s9A2DevDUSvpwOl+GztNvteo5CxpGvocorH9XXqEpReK/2V0Xte49FzsUwlGallvd75Gs49J2oU3ZdMpjaucDS5wlknv4RAJD9x26YOXoa3P30VenQxgF//DAS538ciSkjfWv13GFTmRQvPOuD37eMwKWfn2NBT0REZMBMbSyhVpWhML18kbiAqcMqvJ5zNxn5iWno/fnruPP9Ye32xCMX4T9lKMz+f1RfIjOBQ/tWte434eB5tBjaDRbOdgAA/ylDEb/vrM757/58En6TBsJnfD/E/syp9wBH6skASaUSDO3tiaG9PZFXUIJrdx7h0m0F7sRno7BIBROpFDZWpujUxgFd2zqina+90UwxtvDuAIe+ExE5qx1kNo6we3JUpTZOQ6ch8ZtZsO89TrvNsX8IyvIyEfP+AACApkwFp8FTtSP5NTF3bQ23Fxbjzjt9AAA27fvD+ZnXdcqua4aW/9qA+2teRuquT2BiYQvv2d/r1J+YSSQS9Ozogp4dXbDhv31wIyYLl28rcOtuFvILS6HRlN9m0t7PHl0DndDJ34H3zRMRERmJ7DuJiN9/FqNOr0RxVj4Sj1ys1CZm2wn0XPoa7h84r912b084zO1tMGzXhwDKi/q7oSe1I/k1yU9Mx9UVOxAU9jEAIPWPSET/UHmB5ZokHo3Ak59NR158CnJiHz62bfCv/4Pc0RamNhYYf3kDUv+4hfA31+rcZ1PHT3Fk0GyszNC3qyv6dnWtubEB67pfo/3/FtNWo8W01drv3Sa8X6Ft3s1TcAl6AxKZaYXtLiNnw2Xk7ErHdp/0Ya0yOA+dBueh02pu+BjVZaiK3NMfAZ+fq1d/hkBuLkOPDs7o0cHwV34lIiKqDXNzc/j7+8Pc3FzoKHq12e2vAZuLi77HxUV/DXjcWLW7QlvX3u1xZ8tRaFQV1zCK2nQIUZsOVTr2tf/tqFWG2G0nELut6un7taUuUeHndq/Uqm3YoHn16kssOP2eiAAAJZnJuPVGAArjrsBl5L/rdSyZrTPiV74IxbGNNbbNiwzH7X93hsyuOSTSup2SpHJrZEf8grsf1+7Z89HvDUDerd8glVvVqT8iIiISr1atWuGHH35Aq1a1nzpuLCya22N0+Go4dmiN298cqNexijJz0XfdbPi9MKjGti49AxF8fDmU6VnQqDU1tq9KcXY+nlgYgk5zx9XY1qZlcwQfXw6JzATqUlWd+mtKOFJPRAAAM0d3tF9/p0GOFfi/iFq3tWnXF21XXQNQ/nz56L8t3Pcn205D4PlK5eeP/sl19NtwHf12rfv0X3qq1m2JiIiIjIUyLQt7+85pkGMdCHqn1m3TL0QhbMh8AIDc0bbCwn1/Sj5zA5c++qHaY5x6tfrPiv+Ul5Cm7c8QsKgnoibD1M5FW+ATERERNYbo6GhMnToV3333Hfz9/YWOQ/9QlJlrUAW3PnD6PRERERERGQ2NRoPS0lJoNHWb5k3U1LCoJyIiIiIiIhIpTr9vqszNIduxRegUtWdkq4c2ZXITIPxZoVOIl9w4nm5IREREBkxmYY6QuB+FjiFaMgtx1TYs6psoiUQCyOVCxyARkkgAC76ziYiIiIyWRCKBqSVrCWPBj/5ERERERGQ0vL29ERoaCg8PD6GjEDUIFvVERERERGQ05HI5fHx8hI5B1GC4UB4RERERERmNlJQUfPzxx0hJSRE6ClGDYFFPRERERERGIycnB2FhYcjJyRE6ClGDYFFPREREREREJFIs6omIiIiIiIhEikU9ERERERERkUixqCciIiIiIqPh4OCAl156CQ4ODkJHIWoQLOqJiIiIiMhoSKVSmJqaQiplKUSGgT/JRERERERkNBQKBTZu3AiFQiF0FKIGwaKeiIiIiIiISKRY1BMRERERERGJFIt6IiIiIiIiIpFiUU9EREREREbDxsYGw4YNg42NjdBRiBqETOgARERERERE+uLh4YElS5YIHYOowXCknoiIiIiIjEZxcTEePHiA4uJioaMQNQgW9UREREREZDTi4+MxduxYxMfHCx2FqEFw+n0TpdFoADFdPTQ3h0QiEToFERERNBqgqEzoFOIlNwH4K51IvDQaDVRKEdURTZDMQly1DYv6pqq4GKoJLwmdotZkO7YAcrnQMYiIiFBUBvQ9JHQK8Qp/FrDgJ0Qi0VIpi7HN50WhY4haSNyPMLUUT23D6fdEREREREREIsWinoiIiIiIiEikOLmKiIiIiIiMRkBAAC5evCh0DKIGw5F6IiIiIiIiIpFiUU9EREZDo9FU+C8RGReNRsPzACEhIQFTp05FQkKC0FGIGgSn3xMRkUFSqzU4fu4hTl5MxqVIBa5EZSI7rwQAkJyhhPugUHQNdES3dk4Y/nQLdGvnLHBiImpoqYpC7DwWj4hbClyOUuBOfA7U6vJiPjlDiScm7EO3dk54sqMLxg72RjMbM4ETkz4olUrcunULSqVS6ChEDYJFPRERGZTs3GJ8uzsaX+24g/iHedW2S8koxIGMQhw48wAffnUV3do54Y3nAxEy3AdmpiZ6TExEDe3s1TSsDY3E7hP3oVJVPyJ/9U4mrt7JxLe7ozH7s3N4cYQv3pzUFu187fWYloiofljUExGRwTh4JhHTl5xFcnqhzvteilRg6n/DsXpbJLZ8/DQ6+Ts2QkIydHk3TyPm/QEVtknlVjB3bwPH/pPhMuJNSEz48aux5OSV4K0VF/Dd3hid9y1QqrBh5x1s3BONd1/thEWvd+YFPqI6cO3VDsP2LK6wrbRAidx7KYjbdQZRmw5BU6YWKJ1h4m8VIiISvaJiFd5Y+ge+3xdb72Ndj36EbpP246OZXfGfqR0hkUgaICEZG/unJ6FZ12cBjQalWanIPL0VSd+9haKkKLSc+Y3Q8QzS71dSMek/p5GUVlCv45SVafDxN9ew/1QCdq4YCP9Wdg0TkMjI3NsTjqSTVwCJBBbOdvAd3w89Fr+MZn4eODd/g9DxDAoXyiMiIlErVKowYtbxBino/6RSafDu6kuYs+w8F9OiOrFs/QQc+78IxwGT4TpmPgI+Pw9TR08ojm9EaU6G0PEMzuHwBxjy+pF6F/R/dzM2C31fPojr0ZkNdkxqGtzc3LB48WK4ubkJHcWgZd6Mx73d4bi36wwivwrDweELUfBQgTYvDIK5o63Q8QwKi3oiIhKt0lI1xr71K369kNwox1/70228syqiUY5NxsVEbgUr/ycBjQbFqXFCxzEoZy6lYMxbv6KouKzBj52RVYQh048g5n5Ogx+bhNOsWTMEBQWhWbNmQkcxKiplMTKuxEIilcK2ZXOh4xgUFvVERCRan313HUfOJjVqH59/fxMHzyQ2ah9kHP4s5mXWDgInMRyPcorx/IJTjVLQ/ykjqwiT/nMKpaW8B9hQZGVlYefOncjKyhI6itGx8S4v5ouz8wVOYlhY1BMRkSjdiHmEjzZc02mfiNBgPDg+ERGhwTrtN23xWWTlFuu0Dxk3dXEhVLkKlOZkQHn/JhK/ngnlvauw9OsBuUcboeMZjH8vO49UhW6PJavLeeBKVCY+//6GrvGoiUpLS8Py5cuRlpYmdBSDJrMwg7mDDcwdbWEX4IWen7wGxw6tkXElFrn3UoSOZ1CMoqhXKBRYsGABfH19IZfL0aJFC8yZMwcFBQV49dVXIZFIsG7dOqFjNorfFOkw+2UHvoi7U20bs192YNSFcD2mIiKqH41Gg+mLf0epSreRM1cnS3g2t4Krk6VO+6VkFOK9NZd02oeMW0roB7g+2Rk3prjg9pyOyDi8Hna9xsD3vf1CRzMYJy8k44cDd3Xer67ngcVfX8W9pFyd+yMyVl0WTMSkyO8x6dZ3GHXqCwS+Mgz3D57HyZeXCR3N4Bj86vfXrl1DUFAQUlNTYWVlhbZt2yI5ORlr1qxBXFwcHj16BADo3LmzsEGJiKjWLt7MwIWb+l1sbHNYLD6Z3Q12tuZ67ZfEyemZ6bDvPR6aslIoE24idc8ylCiSIDGVa9vkRYbj7pKgSvtqVCXQqMvQdW/jTSk3BKu3Req1v1KVGl/vuIPP3+qh136JxCr6h2O4/8s5SE1lsA/wQvuZo2Dl5oiy4hJtG6mZDCOPLUf83nDcWL1Hu/2pVTMhd7bDiZClQkQXHYMeqVcoFBg5ciRSU1Mxb948pKSk4MqVK0hNTcWyZctw8OBBREREQCKRoGPHjkLHJSKiWvry5yi996ksKsOWsIZbYZ8Mm7mbH2w7D0azrkFwHbMAvu/9gsK7EUj8aoa2jU27vuiyPb/CV7v1MZDZOMH9hY8ETN/0JSTn4cCZB3rvd9PeGCiLVHrvl0iMcu+lIiX8Jh6evIpb6/fj15c+g1NnH/Ra9rq2jbpEhd9nr0WH2WNg37YlAMBrWHd4DumGs2+tFyq66Bh0UT979mwkJSVh1qxZWLFiBWxsbLSvLViwAJ06dYJKpYK3tzdsbflYBSIiMSgqVmHHsXhB+t76i+5TfYkAwDqwNxz6T0bW79uRH/VHlW3UpcW499kYWLd9Cm7jF+o5obj8dOge1Gr9P27yUU4xDoXr/2ICNSxLS0v07NkTlpa63YJB9ZNxKRpxu86g1ag+cO7mr92eeeMeIr8KQ981b8LSzQG9ls/AhYUboUzjQoa1ZbBFfVRUFLZv3w4nJyd8+umnVbbp2rUrAKBTp07abbt27cLYsWPRsmVLWFpaIiAgAO+99x7y88W9QmNhWRkUxcVVfhERicnN2CwUlwgzLflG7CMUFXOUjurG7flFgNQEyT/9t8rXE9fPgLq0CN5zNus3mAhdvKXf22/+LiJSIVjf1DC8vLywdu1aeHl5CR3F6FxfuQtqVRm6zH++4vZVu6EuK0Pw8eVIPXsL8fvPCpRQnAz2nvrQ0FCo1WqEhITA2tq6yjYWFhYAKhb1K1asgJeXFz755BN4enri2rVrWLx4MX777TecOXMGUqk4r4MsiY7Ekmj93ntGRNQYLt8W7gO1SqXBjZgs9OjgLFgGEi+5my8c+k7Eo9+2IS8yHDbt+mpfS/9lDXIuHUDAighIzTl6WJNLAhbWQp6DqGGUlZVBqVTCwsICJiYmQscxKnn3UxG//yx8xj4Nl56BSL9QfjudRlWGjIhoOHX0wd3tpwROKT4GW9SfPHkSADBgwIBq2yQllT/b+O9F/S+//AJn578+rPXr1w/Ozs4ICQnB77//jqeffrqREjeu17xaY6x7iypfCzr/m57TEBHVXWRctsD9s6inunMd/x4ehYci+af/wn9p+QfXvBunkLT1P/D772GYN/cWNqAI5OaXICmtQLD+hT4HUf3FxsZiypQp2Lp1KwICAoSOY3RurN6NVqP6oMv853F03IcAAJeegfB9fgCiNh1CjyWvIGzIfJQVlTz+QKRlsEV9QkICAKBly5ZVvq5SqXD2bPm0jr8X9X8v6P/UrVs3AMDDhw/rlKVbt25ITU3VaR8LqRS3O/eqU39V8bW2xiDn5g12vH9q06YNlGrdHi1FRFQXWVajAPMuVb4WERr82MdUuTpZaP/74PjEx/aTqihE90lhlbbPnfcuFs26UPvATUyK3VuAtBlSUlPg6ekpdJxGITGzQPNVwixqaNOhP7rur/5eb4sWgRVWtS9Ou497yyfA8+XlsOnQXw8Ja9amjR80Jbo9+12fyiTWgP38al9vqPNAdeeA1PRHon7vGOI5YNy4cTq1T09PBwAcPnwYly9frtU+o0eP1jmXUEw1UnwA4Z7SkHouEpvdqv83yYl9iK2ef02/l1nK8dSqmbi8dBvubDmKoL1L8MS7LyDig816SFu1Nn5tUCrRf23j6uqKS5d0f4SuwRb1BQXlV3CVyqp/KW3fvh0KhQI2NjZo1arVY4916lT5lfTAwMA6ZUlNTdX5goCliQnQuU7dCSI5ORmFZXz0DhHpgUcBUM1T5f58/nRNZCbSWrWrSk5OFnIy63aRt0mwKQOkgLqsrM4Xq5s6qbklGu8ydsNRFxci7tNRaNYjGC7DZwkdRys5ORnq4kKhY1RPZgvYV/9yY58HNGq1uN87BngO+PNzf239WR8olcpa7yumvysziQlEcRL8f90/nIL8xHTc2XwEAPD7nHUIPrECiYcvIO28/p92AwDJKcko0YintjHYot7V1RVZWVm4cuUKevWqOOKdkpKC+fPLr/B27NgREomk2uM8fPgQixYtwrBhw+r8LHtXV1ed97EQ2b377u7uHKknIr3ItpShuo9gqYrHFyKuThaQmUihKlMjVfH4kcjqjmVnawkruUdtojZJKSYmUAOQmpjAzUO8f47HkZhZCB2hVrL+2A1l/HUUPYxB1u/bK73ebt1tmDnrfyEvd3f3Jj1Sr4YZUh7zekOdB6o7jlSiEvV7xxDPAVZWul2c+bOQt7CwqPW+HiL6uzLVSAGRfCz3GNgFrYL7YP+gedpteQlpuLx0G/qsnImwgfOgUup/YW93N3fBRurrwmCL+sGDByMqKgrLli3DkCFD0KZNGwBAREQEJk+eDIWifJGTxxXq+fn5eO6552BmZobvvvuuzlnqMoVCU1QE1YSX6tynvsXExEAilwsdg4iMwIaddzDjo6pXxa1qquzfPTg+EZ7NrZCqUKLFkJ/r1P+xsO/Rvb1476n3HByKh+mFcHN1Q9KtJKHjNAqlCuh7SOgUNXMcMBmOAyYLHaOSmJhYWDTxT4heQ3/Gg9SqL+819nlgUJ8AHNsg3veOIZ4DIiIidGp/584dhIaGIigoqNb31K9ataoOyYRRWliEbT4vCh2jVh6evIqfAirXPHc2H9GO3AshJjYGppbiqW3ENRysgwULFsDR0REPHjxAu3bt0KFDB/j5+aFHjx5o3bo1Bg4cCKDi/fR/p1QqMXLkSMTHx+PYsWNwc3PTZ3wiIqpG17aOgvUtk0nQwe8x836JSC+6tnUyyr6pYfj6+uLo0aPw9fUVOgpRgzDYot7T0xPh4eEYPnw45HI57t+/DwcHB2zYsAEHDx5ETEwMgKqL+tLSUowbNw6XLl3C4cOH0bZtW33HJyKianTwc4DcXJhHEHVq4wi5eRMfwiQyAj0FfAJFDxHP1KFyMpkM9vb2kMl4PifDYNA/yYGBgThw4ECl7fn5+bh//z6kUinat29f4bU/n23/66+/4tChQ+jRQ7iVIxtCPycXlIyc8Ng2Nb1ORNSUmJuZ4PlnWmNLmP5XN38pmKM6RE3BC8/64L21l6FWV/+kgcbgaGeOoKcMY8V4Y5aUlISVK1di7ty5BvMEADJuBjtS/ziRkZHQaDTw8/ODpWXFR57MnDkTO3fuxNy5c2FpaYnz589rvzIyMgRKTEREf/fG83V7Gkl9WMplmDLST+/9ElFlXm7WGNmvhd77fXV0G87WMQD5+fkIDw9Hfn6+0FGIGoRRFvU3b94EUPXU+8OHDwMAPvvsM/Tq1avC18GDB/Wak4iIqtajgzN6d3bRa59TR7dBMxszvfZJRNWbO7l9zY0akJmpFP+aoP8LikRENWFR/w/379+HRqOp8uvll1/Wc1IiIqrOhkV9YGaqn19jns2t8PGsrnrpixrH5eckUOVn662/m9O8cetf/lAc26jdpji+Cbdm+OHm6z5IWDcNGlUpACAvMhy3/925XhmTQz/E9cnOuPtxsHZb4jezcXOaNy4/J0HhvWsV2ke/NwDXQhyQFraqTv01Bf26ueHl5/Q3e2bxG0/A28NGb/0RNbSXU3bBzNay5oYNZNzF9Rgdvhp+LwzSbvObNBBjzq7FmHPr0HvFDEhk5WvkuPQMRPDx5fXKOGz3Yoy98CU6zR0HALAL8MKwvUswOnw1njv1Bfp88QZM5OUX503kZgg+vhwhd3+A17Du9fyTCo9FPRERiVJ7Pwd8MKOLTvukKgqRlFZQ43Os/+nbD57iKD3VikathkZd/mzj1vO3w2noawCA4rR4JG9bBP9Pw9H+67sozU5DxtFvAAA27fqi7apr9e7boV8IfN//63Fu9n3Gwf/T32Hm0rJSW/+lp2DXI7jSdrFZOb8nPFx0KwDqch7o0d4Zb7/UQdd4RMZJIin/AvDbjJWI/elXAIB1Cxd0WTARh0ctwp5esyB3agb/F4cAANIvRCFsyPx6dx3xwWZcX7kLAFBWXIILCzdhb985CBv0NmSW5ugwc1T5a0UlCBsyH5nX79W7z6bAKG8KOnnypNARiIioASx4pSPOXU/HgTMPatW+pudXV+W9aZ0wjAtjGZSk799G3q3foCkrhYmFLVrO/BZyT3+k7l2B4uQYtJxZXmyr8rNxa4Yv2n8VA5mNA1L3rkDW2R3QlKlg2swFXm9sgLlLSySHfghlwk2olfkoUTyA35LjlfrMOrsLzXoEw9TeFQDgPGwGUnZ9ApfhM3XKXl2Gqti0e1rHvxnxsbM1x/blAzHk9cNQFpXVah9dzwPNHS3w07L+kMmMcizMIDk7O2POnDlwdjbeJxl0++8UuPZqC6nMBCX5Svzx9tfIjUtGuxnBsPVxw7n5GwAAZraWGHNuHfb0mY2S7Hy0mxEM7+BekMpMUKTIwR8LNqAgSYHO8ybALtALplZyWLk74tjzH1Xqs+WIJ/Hg2CUoM7IBANFbj6HjnDE6PY9+9O9rcGbmamRejwMA+E7ojxbDuuPU1OWV2ubFp2r/X6NWQ3EtDvYB+l+LQx94diIiItGSyaTYsWIgnunt0SjHnzu5HT7itHuD03zMfxD4vwi0XXUNzkFv4MHGOQAApyGvIfvCPu0U+Mxfv4ddz+cgs3HAo99+QvHDaAQsO4e2K6/AoV8IEr9+Q3vMgjvn4D13K9p9eRtmjpV/HksUiRVGzM1cvFGSkahT7poyGKs+XZpj/+ohsJA3/KMumzta4PiGYfBpYdvgxybhODo6IiQkBI6OjkJHEczNL/fhQNA7CBsyH9Gbj6LnR68AAGJ/OgGvYT20U+B9Jw5E4pEIlGTno9Xop9DM1x2HRryHX4YuQNyecDz56TTtMV26tkH4m2uxr99cFKY+qtSntYcT8pP+Wng8PykDVh5OOuW+u+M0fCf0137vO3EAYkNrHrCVWZijTcggJB6N0Kk/sTDKkXoiIjIcFnIZwtYOwZufnsM3u6Ib5JimMik+md0N815qD8n/TyEkw5F37TjSD65FmTIPUKuhyi//8CmztoN973HI/PU7uATPRcaRr9B6/nYAQPaFfSiIjUDUvPKLPBp1xVFh267PwtSueaPmrimDMRvSywO/fhuESf85jYTkhlnRvEuAI3asGAhfLxb0hiY3NxcXL15Ejx49YGtrnP++7k93RODUIJhaW0AilcDczhoAUJJbiIQD5+A7aSBubzgA/ylD8duMlQAAr2E94NTZByOPLgMASEwqjg8n/XoFRYqcRs0dt/M0go8vR8TiLbB0dYBta3c8PHn1sftITWXot+EtPDx9HYmHLzZqPqGwqCciItEzMzXBhv8+hTGDvPHah78jKa2gzsfq2tYJmz/qi/Z+Dg2YkJqKkoxEJH4zC4ErImDu5oPC+zcQs/CvaeouI2bj7tJgyD0DIbN1hmXr8nUbNBoNXMe9C+dnpld5XBML68f2a+bkheLUuL9ypN+HmbOXTtlrymDsenVqjpu7R2PBFxH4euedOh/HVCbFotc7452pnWCqp8U4Sb+Sk5OxcOFCbN261SiLeisPJzy59FUcCHoHeQlpsA9siaC9S7SvR206hIGb30FO7EMUZebi0a14AIBEIsHNtXsR8+OJKo+rKix6bL/5DxWw9XbVfm/t6YyChwqdshemPILi+j14PdMddv4tcG/3GWjK1NW2l8hM0G/DXCjTs3Bx0Xc69SUmPFMREZHBeKaPJyL3jsHK+T3h11K3D2q9O7vgh0/64fyPI1nQG7CyghxITExh6uAGjUaDjIPrKrwu9wyAefPWSPhyOlyGz9Jut+s5ChlHvoYqr3xUX6MqReG9x48O/Z1977HIuRiG0qzU8n6PfA2HvhN1yl7fDMbAxsoMXy3qgwvbgvHiCB+dnpBhY2WKWZPa4ubu0Vj0ehcW9GSwTG0soVaVoTA9CwAQMHVYhddz7iYjPzENvT9/HXe+P6zdnnjkIvynDIXZ/4/qS2QmcGjfqtb9Jhw8jxZDu8HC2Q4A4D9lKOL3ndU5/92fT8Jv0kD4jO+H2J+rn3ovMZGi39dzUZyVjz/e/lrnfsSEI/VERGRQbK3N8O/J7TE7pB1OXUzBqYhkXL6diStRCqQ/Kh9FkEgALzdrdA10Qte2jni2bwt0DjDeeyuNiYV3Bzj0nYjIWe0gs3GE3ZOjKrVxGjoNid/Mgn3vcdptjv1DUJaXiZj3BwAANGUqOA2eqh3Jr4m5a2u4vbAYd97pAwCwad8fzs+8rlN2XTMkrH8dOZcOojQrFbEfPgMTCxu033BXpz7FqkcHZ/zQoT++eLsndp+4j4hIBS7fViDqXjZKSstH9SzlMnTws0fXtk7o2cEZYwZ7w9rSVODkRI0v+04i4vefxajTK1GclY/EI5WnpMdsO4GeS1/D/QPntdvu7QmHub0Nhu36EEB5UX839KR2JL8m+YnpuLpiB4LCPgYApP4RiegfKi8sWpPEoxF48rPpyItPQU7sw2rbtXquD7yHP4lHkfcRfLx8Ib20iGhcWLix2n3EikU9EREZJKlUgkFPumPQk+7abRqNBiqVBjKZhPfKG5mu+zXa/28xbTVaTFut/d5twvsV2ubdPAWXoDcgkVUs8FxGzobLyNmVju0+6cNaZXAeOg3OQ6fV3PAxqstQlZZvbKhXX4bA2cECMyYEYsbftqlUakgkgIkJR+LJuGx2++tC5cVF3+Piou+1399YtbtCW9fe7XFny1FoVBXX7ojadAhRmw5VOva1/+2oVYbYbScQu63q6fu1pS5R4ed2r9TY7t6ecNzbE16vvsSCZzMiIjIaEokEpqZSFvRUpZLMZNx6IwCFcVfgMvLf9TqWzNYZ8StfhOJYzSNCeZHhuP3vzpDZNYdEWrePZlK5NbIjfsHdj2v37Pno9wYg79ZvkMqt6tSfmMlkUhb0Rs7c3Bz+/v4wNzcXOkqTY9HcHqPDV8OxQ2vc/uZAvY5VlJmLvutmw++FQTW2dekZiODjy6FMz4JGramxfVWKs/PxxMIQdJo7rsa2JnIzBB9fDuuWLigrLq1Tf02JRKPR1O1vjRqVpqgIqgkvCR2j1mQ7tkAilwsdg4iIauA5OBQP0wvh4WKJpBOThI7TKJQqoG/lgSRRKM1OR+yHQyttt+00BJ6vVH4Oc2MIfxaw4FxOg2WI54CIiMZ/TFn37t0bvY+GUlpYhG0+Lwodo87kjrYY+vOiStuTz9zApY9+0EuGkLgfYWopntqGp2wiIiKiJsLUzgVtV10TOgYRkWCKMnMRNmS+0DFEhXOPiIiIiIjIaERHR6NPnz6Ijo4WOgpRg2BRT0RERERERkOj0aC0tBS8C5kMBaffN1Xm5pDt2CJ0itrjQiNERNREyE3K7wunupGbCJ2AiOpDZmGOkLgfhY4hajILcdU2LOqbKIlEAnDhOSIiIp1JJFzojYiMl0QiEdUib1R/nH5PREREREREJFK8jk1EREREREbD29sboaGh8PDwEDoKUYNgUU9EREREREZDLpfDx8dH6BhEDYbT74mIiIiIyGikpKTg448/RkpKitBRiBoEi3oiIiIiIjIaOTk5CAsLQ05OjtBRiBoEi3oiIiIiIiIikWJRT0RERERERCRSLOqJiIiIiIiIRIpFPRERERERGQ2pVIouXbpAKmUpRIaBP8lERERERGQ01Go1rl69CrVaLXQUogbBop6IiIiIiIhIpFjUExEREREREYkUi3oiIiIiIiIikWJRT0RERERERsPGxgbDhg2DjY2N0FGIGoRM6ABERERERET64uHhgSVLlggdg6jBcKSeiIiIiIiMRnFxMR48eIDi4mKhoxA1CBb1RERERERkNOLj4zF27FjEx8cLHYWoQXD6fROl0WgAMV09NDeHRCIROgURERk5jQYoKhM6hbjJTQD+SicSN41GA5VSRLVEEyOzEFdtw6K+qSouhmrCS0KnqDXZji2AXC50DCIiMnJFZUDfQ0KnELfwZwELfkIkEjWVshjbfF4UOoZohcT9CFNL8dQ2nH5PREREREREJFIs6omIiIiIiIhEipOriIiIiIjIaAQEBODixYtCxyBqMBypJyIiIiIiIhIpFvVERERERGQ0EhISMHXqVCQkJAgdhahBcPo9ERGRAct4pMTl25m4fFuBuw9y8Sin/BFH2Xkl+HbXHXRt64T2fvYwMzUROCkRNQaVSo2oe9m4HKXA1ahM7TkgK7cYi7+6gq5tndC1rRPcnC0FTqo/SqUSt27dglKpFDoKUYNgUU9ERGRgSkrLsO9kAtZvj8Jvl1KrbFOgVGH6krMAAFtrU7wU7Id/TQhEYGs7PSYlosYSn5SHDbvuYNPeGCiyiiq9XlhUhg+/uqr9vkd7Z7zxfCAmPNMKFnKWCERiwncsERGRAdl9PB6zl51HcnphrffJzS/F2p9uY+1Pt/HcAC+sf6833F2sGjGl4cq7eRox7w+osE0qt4K5exs49p8MlxFvQmLCj1/UeDKzizD38wv48eBdaDS13+/irQxcvJWBef+7gM/n9sAro/wgkUgaLygZLNde7TBsz+IK20oLlMi9l4K4XWcQtekQNGVqgdIZJv5WISIiMgCKrCLM/OQP7DgaX6/j7D+ViN8upWL1f57E5JG+/FBfR/ZPT0Kzrs8CGg1Ks1KReXorkr57C0VJUWg58xuh45GB2n8qAa8vOYu0zLpPK8/MLsarH4Rj57F4fPvBU/B05QU+qpt7e8KRdPIKIJHAwtkOvuP7ocfil9HMzwPn5m8QOp5B4UJ5REREIheflIeeIWH1Luj/lJ1XgpfeP4N5Ky5Ao8tQH2lZtn4Cjv1fhOOAyXAdMx8Bn5+HqaMnFMc3ojQnQ+h4ZGA0Gg0+3Xgdo+acqFdB/3dHziah26T9uBnzqEGO15S4ublh8eLFcHNzEzqKQcu8GY97u8Nxb9cZRH4VhoPDF6LgoQJtXhgEc0dboeMZFBb1REREIpaYko9+Uw/iXlJegx975Q+RmPs5C/uGYCK3gpX/k4BGg+LUOKHjkIH5dON1LFxzqcGPm5apxIDXDuF2XFaDH1tIzZo1Q1BQEJo1ayZ0FKOiUhYj40osJFIpbFs2FzqOQWFRT0REJFLFJWUYMesYHqQWNFofq7dF4qvtUY12fGPyZzEvs3YQOAkZkl3H4vHe2suNdvzM7GIEvXEUOXkljdaHvmVlZWHnzp3IyjKsixViYONdXswXZ+cLnMSwsKgnIiISqSVfX8XNWN0+lEaEBuPB8YmICA2u9T7zv4jAvaRcXeMZNXVxIVS5CpTmZEB5/yYSv54J5b2rsPTrAblHG6HjkYFIz1TiX0v/0GmfupwDElMK8Pb/Lugar8lKS0vD8uXLkZaWJnQUgyazMIO5gw3MHW1hF+CFnp+8BscOrZFxJRa591KEjmdQjKKoVygUWLBgAXx9fSGXy9GiRQvMmTMHBQUFePXVVyGRSLBu3TqhYxIREdXa5dsKLPv+hs77uTpZwrO5FVydav9M6sIiFV794HdOw9dBSugHuD7ZGTemuOD2nI7IOLwedr3GwPe9/UJHIwPy5mfnqnxc3ePU5RwAABv3xODYH0k67UPGrcuCiZgU+T0m3foOo059gcBXhuH+wfM4+fIyoaMZHINf/f7atWsICgpCamoqrKys0LZtWyQnJ2PNmjWIi4vDo0fli3907txZ2KCN5DdFOoacO43P2nbEWz4BVbYx+2UHnnVxw76effWcjoiI6mrpt9dQVqa/Ivt0RArCL6fi6W5cWKo2nJ6ZDvve46EpK4Uy4SZS9yxDiSIJElO5tk1eZDjuLgmqtK9GVQKNugxd95bpMzKJTNS97AZbHLO2lmy4hqG9PfXaJ4lX9A/HcP+Xc5CaymAf4IX2M0fBys0RZcV/3cohNZNh5LHliN8bjhur92i3P7VqJuTOdjgRslSI6KJj0CP1CoUCI0eORGpqKubNm4eUlBRcuXIFqampWLZsGQ4ePIiIiAhIJBJ07NhR6LhERES1kpRagP2nEvXe7/odvLe+tszd/GDbeTCadQ2C65gF8H3vFxTejUDiVzO0bWza9UWX7fkVvtqtj4HMxgnuL3wkYHoSg68EeD+evZqGGwa4Gj41jtx7qUgJv4mHJ6/i1vr9+PWlz+DU2Qe9lr2ubaMuUeH32WvRYfYY2LdtCQDwGtYdnkO64exb64WKLjoGXdTPnj0bSUlJmDVrFlasWAEbGxvtawsWLECnTp2gUqng7e0NW1s+VoGIiMRh095oqNX6nwq/+8R9pDfQ47KMjXVgbzj0n4ys37cjP6rqe6DVpcW499kYWLd9Cm7jF+o5IYmJskiFLWGxgvT9tQFc3LO0tETPnj1haanbLQhUPxmXohG36wxajeoD527+2u2ZN+4h8qsw9F3zJizdHNBr+QxcWLgRyjQuZFhbBlvUR0VFYfv27XBycsKnn35aZZuuXbsCADp16qTdFh4ejsGDB8PNzQ3m5ubw9PTE888/j6go8Z/AiIjIMJyKEGaBIZVKg7PXuLBUXbk9vwiQmiD5p/9W+Xri+hlQlxbBe85m/QYj0bkSlYnc/FJB+j59KVWQfhuSl5cX1q5dCy8vL6GjGJ3rK3dBrSpDl/nPV9y+ajfUZWUIPr4cqWdvIX7/WYESipPBFvWhoaFQq9UICQmBtbV1lW0sLCwAVCzqs7Ky0KFDB6xZswbHjh3DsmXLEBkZiV69eiEpSbyLgxSWlUFRXFzlFxERiYdarcGVqEzB+r98WyFY32Ind/OFQ9+JyLvxK/Iiwyu8lv7LGuRcOgCfd/dBas7RQ3o8Id+Hd+KzkV8ozAWFhlJWVob8/HyUlXHdCn3Lu5+K+P1n4f50R7j0DNRu16jKkBERDbljM9zdfkrAhOJksEX9yZMnAQADBgyots2fRfrfi/rg4GCsXLkS48ePR79+/RASEoI9e/YgJycHu3fvbtzQjWhJdCTcj+2v8ouIiMQj7kEu8gqE+0At5AUFQ+A6/j1AKq0wWp934xSStv4HrRfshHlzb+HCkWhcvSPc+1CjAa4J2H9DiI2NxcCBAxEbK8wtDMbuxuryUfm/j9a79AyE7/MDELXpEHoseQUmcjMBE4qPwa5+n5CQAABo2bJlla+rVCqcPVs+rePvRX1VHB0dAQAyWd3+urp164bUVN2mKllIpbjduVed+qvKa16tMda9RZWvBZ3/rd7Hb9OmDZRqdb2PQ0REj1csawHYvlblaxGhwTU+psrVyUL73wfHJ1bbLlVRiO6TwiptP3H6PDw9X9UhsX5JzCzQfJVwH9RtOvRH1/3Vr3dg0SKwwqr2xWn3cW/5BHi+vBw2HfrrIWHN2rTxg6aEayc0ZQrrFwAz/ypfq+k8UNtzAFD9eeC5sS/CovSODokb17hx43Rqn56eDgA4fPgwLl++XKt9Ro8erXMuIZlqpPgAPQTpO/VcJDa7Vf9vkhP7EFs9/yroZZZyPLVqJi4v3YY7W44iaO8SPPHuC4j4YLMe0latjV8blEr0X9u4urri0qVLOu9nsEV9QUEBAECprPqX0vbt26FQKGBjY4NWrVpVer2srAxqtRoJCQl499134erqigkTJtQpS2pqKh4+fKjTPpYmJkDnOnVXJV9rawxybt5wB/yH5ORkFHIKExFR47OyAqpZ2/XP50/XhsxEWuu2f1daqtb5d5o+Sc0t0Xi/7RqWurgQcZ+OQrMewXAZPkvoOFrJyclQFxcKHYMex7sUqGYgs7bngbqeAwDgUVYOkNN0zgN/fu6vrT/rA6VSWet9m/J5rypmEhOI5WTY/cMpyE9Mx53NRwAAv89Zh+ATK5B4+ALSzguzrllySjJKNOKpbQy2qHd1dUVWVhauXLmCXr0qjninpKRg/vz5AICOHTtCIpFU2r9fv37akXxfX1+cPHkSzs7Odc6iKwupuO6McHd350g9EZEelJjYI6Oa11IVNRdirk4WkJlIoSpTI1VR/WhsdccyNZXAxcOjNlEFITGzEDpCrWX9sRvK+OsoehiDrN+3V3q93brbMHPW/0Je7u7uHKlv4jLNZSiq5rWazgO1PQc87lgO9rawsG465wErK90uTvxZyFtYWNR6X48mfN6riqlGCojgo7nHwC5oFdwH+wfN027LS0jD5aXb0GflTIQNnAeVUv9rgLm7uQs2Ul8XEo1Go/9n4ujB7NmzsXbtWrRo0QInTpxAmzZtAAARERGYPHky7t27h9LSUsycORPr1q2rtH90dDSys7MRHx+P5cuXIz09HWfPntXbKpmaoiKoJrxU7+P8pkjHkHOn8VnbjnjLJ6DKNma/7MCzLm7Y17NvnfuR7dgCiVxe5/2JiKh2ktML4DH45zrv/+D4RHg2t0JSWgFaDNH9OOOHtsKOFQPr3H9jU6qAvoeETiFu4c8CFgY77GMYZn3yB778uW4jmPU9BwDAtZ2j0MnfsU77NoaIiAid2t+5cwdTpkzB1q1bERBQ9efjf+revXtdogmmtLAI23xeFDqGaIXE/QhTS/HUNuIaDtbBggUL4OjoiAcPHqBdu3bo0KED/Pz80KNHD7Ru3RoDB5Z/IKnufnp/f3/07NkTEydOxK+//oq8vDx8/vnn+vwjEBERVeLuYgU3Z+FWR+/atul8kCcyVl3bOgnWt7mZCdq2thes/4bg6+uLo0ePwtfXV+goRA3CYIt6T09PhIeHY/jw4ZDL5bh//z4cHBywYcMGHDx4EDExMQBqXiQPAOzs7ODr64u7d+82dmwiIqIadQ0UrrAWspggonJCvg87tXGAqam4SwiZTAZ7e/s6L4JN1NSI+x1Zg8DAQBw4cAB5eXnIy8vDhQsXMH36dBQUFOD+/fuQSqVo3759jcdJT09HdHQ0fHx89JCaiIjo8Ub21/991gBgb2uG3p1EsvISkQFr52MHb3drQfoe0a/qpymJSVJSEubNm6d9vDWR2Bnl5anIyEhoNBq0adMGlpYVpzC++OKL8PX1RefOnWFnZ4fY2FisXLkSMpkMc+fOFShx3fVzckHJyMev2l/T60RE1LS88KwP3v7fRb0/r/6VUW1gyZutiQRnYiLFjAkBeGeV7o++qg+ZTIJpY6t+lJ6Y5OfnIzw8HNOmTRM6ClGDMOiR+urcvHkTQNVT75988kkcOnQIr7zyCoKCgrB8+XL07dsX165d4303RETUJFhbmuKlYD+99ztjfO0WlCKixjd1VBuY6Xka/OiB3nB1Em5NDyKqGov6f5g1axYuXryIrKwsKJVKxMTEYMOGDWjZsqW+YxIREVXrvWmd4NDMXG/9vfF8IPxaNtNbf0T0eM4OFlj4Ws1rQzUUC7kJlr7ZVW/9EVHtsagnIiISIVcnS6x9p5de+vJ2t8ayueJ6nNM/XX5OAlV+tt76uznNG7f+5Q/FsY3abYrjm3Brhh9uvu6DhHXToFGV3z6RFxmO2//uXK+MyaEf4vpkZ9z9OBgAoC4pwt1PRuHWv9rg9pxOiPnvEBSl/LXgb/z/QnD9JVc82PjvOv8ZSXgLX+uMzgEOeunrkze78cKeyL2csgtmtvqbaTHu4nqMDl8NvxcGabf5TRqIMWfXYsy5dei9YgYkMhMAgEvPQAQfX16vjMN2L8bYC1+i09xxAADrFi4YcXQZgo8vx3OnvkD/b+bBrJkVAMCmZXMEH1+OyQmhcGjnXb8/aBNglEX9yZMnodFoMHz4cKGjEBER1dmkZ1tj4rDWOu2TqihEUloBUhWFtWpvKpNi88dPw9rStC4RjY5GrYZGrQYAtJ6/HU5DXwMAFKfFI3nbIvh/Go72X99FaXYaMo5+AwCwadcXbVddq3ffDv1C4Pt+mPZ756HT0W59NNquvg67ns8hYd1r2tdazdsG52Ez6t0nCcvUVIotH/eDlQ5rXeh6DgCAQT3dMTukXV0iNknOzs6YM2cOnJ2dhY5imCSS8i8Av81YidiffgVQXmR3WTARh0ctwp5esyB3agb/F4cAANIvRCFsyPx6dx3xwWZcX7kLAFCY9giHn1uEsCHzsX/AWyhMe4TOb5evJZaXkIawIfOhTMuqd59NAVe7ISIiEimJRILNHz+NR7nFOPbHw1rt031SWM2N/p+JiQTbPuuPft3c6hqxSUr6/m3k3foNmrJSmFjYouXMbyH39Efq3hUoTo5By5nlxbYqPxu3Zvii/VcxkNk4IHXvCmSd3QFNmQqmzVzg9cYGmLu0RHLoh1Am3IRamY8SxQP4LTleqc+ss7vQrEcwTO1dAQDOw2YgZdcncBk+U6fs1WX4J6mZHM26Pav93qrNk0jbt0KnvkgcOrZxwL7VgzHyzeMoKi6rsb0u5wAA6NHeGXtXDYJUKqlrxCbH0dERISEhQscQVLf/ToFrr7aQykxQkq/EH29/jdy4ZLSbEQxbHzecm78BAGBma4kx59ZhT5/ZKMnOR7sZwfAO7gWpzARFihz8sWADCpIU6DxvAuwCvWBqJYeVuyOOPf9RpT5bjngSD45dgjIjGwAQvfUYOs4Zgzubj9Q69+jf1+DMzNXIvB4HAPCd0B8thnXHqanLK7VVl6i0/y+RSiGzMIeqsEiXvybRMMqReiIiIkNhbmaC/asHY/Sghl37RW5ugt1fDML4oa0a9LhNQfMx/0Hg/yLQdtU1OAe9gQcb5wAAnIa8huwL+7RT4DN//R52PZ+DzMYBj377CcUPoxGw7BzarrwCh34hSPz6De0xC+6cg/fcrWj35W2YOXpU6rNEkQizvxXfZi7eKMlI1Cl3TRkeJ/3Aatj1eE6n/kg8Bj/pgSNfPYNmNmYNetwB3d1w/JthsLFq2OMKLTc3FydOnEBubq7QUQRz88t9OBD0DsKGzEf05qPo+dErAIDYn07Aa1gP7RR434kDkXgkAiXZ+Wg1+ik083XHoRHv4ZehCxC3JxxPfvrXEwRcurZB+Jtrsa/fXBSmPqrUp7WHE/KTMrTf5ydlwMrDSafcd3echu+E/trvfScOQGzoyWrbS01lCD6+HBMjv4NtazdcXb5Dp/7EgiP1REREIic3l2H3F4Pw7e5ozFtxEfmF9XvUXZ8uzfH9kr4Ge/9s3rXjSD+4FmXKPECthiq//MOnzNoO9r3HIfPX7+ASPBcZR75C6/nbAQDZF/ahIDYCUfPKFwrTqCuOiNp2fRamds0bNXdNGaqTsvMTFKfcRcuPfm3MeCSwft3ccHP3aExffBZHztbv+evmZib4aOYTeGtKe5iYGN4YYHJyMhYuXIitW7fC1tZW6DiCcH+6IwKnBsHU2gISqQTmdtYAgJLcQiQcOAffSQNxe8MB+E8Zit9mrAQAeA3rAafOPhh5dBkAQPKPn42kX6+gSJHTqLnjdp5G8PHliFi8BZauDrBt7Y6HJ69W215dqkLYkPmQmsrQc+lU+E8eglvr9zdqRiGwqCciIjIAEokE08cF4JneHnhn1SXsOhEPlUqj0zFauFph/ssd8MbzgQb5QR4ASjISkfjNLASuiIC5mw8K799AzMKnta+7jJiNu0uDIfcMhMzWGZatuwAANBoNXMe9C+dnpld5XBML68f2a+bkheLUuL9ypN+HmbOXTtlrylCV1L0rkH1uD/yWnIDUnI8iM3QtXK1xaP1QbAmLxdJvr+Nuom4j0RIJMOJpL3z+VncEtLJrnJAkOCsPJzy59FUcCHoHeQlpsA9siaC9S7SvR206hIGb30FO7EMUZebi0a14AOW/Z26u3YuYH09UedyaprbnP1TA1ttV+721pzMKHip0yl6Y8giK6/fg9Ux32Pm3wL3dZ6ApU9e4n7pUhbs/n0LvFTMMsqg3zN/YRERERqqluw1CPx+AxKMTsWTmE+jgZw8Tk+rvhbWzMcPwp1tg3+rBuHdoAt58oZ3BFvQAUFaQA4mJKUwd3KDRaJBxcF2F1+WeATBv3hoJX06Hy/BZ2u12PUch48jXUOWVj+prVKUovFf96NA/2fcei5yLYSjNSi3v98jXcOg7UafsumZI2/8FssJD4bfkOGTWdjr1ReIlkUjw8nNtEB02Dsc2DMPYwd5wtpc/pj0Q0KoZ3n21E+4dmoCwtUNY0Bs4UxtLqFVlKEwvXyQuYOqwCq/n3E1GfmIaen/+Ou58f1i7PfHIRfhPGQqz/x/Vl8hM4NC+9rdoJRw8jxZDu8HC2Q4A4D9lKOL3ndU5/92fT8Jv0kD4jO+H2J+rn3pv5ekEE4v/v3VEIkHLkb3wKEq3257EgiP1REREBsjN2RKLXu+CRa93QaFShesxmbibmAtlcRlkJlLY2Zihc4ADWnnYQCIxnAWwamLh3QEOfSciclY7yGwcYffkqEptnIZOQ+I3s2Dfe5x2m2P/EJTlZSLm/QEAAE2ZCk6Dp2pH8mti7toabi8sxp13+gAAbNr3h/Mzr+uUXZcMJYokJH03D2aurbXtJTJzBK64oFOfJF5SqQRDenlgSC8PaDQaPEgtwNU7mXiUU4xSlRrmpiZo7WmDzgEOBnfPPD1e9p1ExO8/i1GnV6I4Kx+JRy5WahOz7QR6Ln0N9w+c1267tycc5vY2GLbrQwDlRf3d0JPakfya5Cem4+qKHQgK+xgAkPpHJKJ/qLywaE0Sj0bgyc+mIy8+BTmx1S8Sax/YEk+880J5VqkEmTfjcfH9TTr3JwYs6omIiAycpYUMvTo1R69OjXvPd1PWdf9ftyK0mLYaLaat1n7vNuH9Cm3zbp6CS9AbkMgqPsbPZeRsuIycXenY7pM+rFUG56HT4Dx0Ws0NH6O6DP9k5uRZ4c9Mxk0ikcDLzRpebo+/TcRYmJubw9/fH+bm5kJH0avNbn9dqLy46HtcXPS99vsbq3ZXaOvauz3ubDkKjari2h1Rmw4hatOhSse+9r/aLUAXu+0EYrdVPX2/ttQlKvzc7pUa2yUdv4yk45fr1ZdYGO78OiIiIiIdlGQm49YbASiMuwKXkf+u17Fkts6IX/kiFMc21tg2LzIct//dGTK75pBI6/bRTCq3RnbEL7j7cXCt2sf/LwSPTv8IEwvjXCSMjFurVq3www8/oFUrw3u6R31ZNLfH6PDVcOzQGre/OVCvYxVl5qLvutnwe2FQjW1degYi+PhyKNOzoFHX7YJkcXY+nlgYgk5zx9XY1qZlcwQfXw6JzATqUlWN7Zs6iUaj4WXcJkhTVATVhJeEjlFrsh1bIJFXf78WERGRPihVQN/Kg0iiUZqdjtgPh1babttpCDxfqfwc5sYQ/ixgwbmcJCIRERGN3kf37t0bvY+GVFpYhG0+Lwodo07kjrYY+vOiStuTz9zApY9+0EuGkLgfYWopntqGp2wiIiKiJsLUzgVtV10TOgaRQYuOjsbUqVPx3Xffwd/fX+g49A9FmbkIGzJf6Biiwun3RERERERkNDQaDUpLS8EJy2QoOFLfVJmbQ7Zji9Apas/IFhohIqKmSW5SPn2c6k5uInQCIqovmYU5QuJ+FDqGaMksxFXbsKhvoiQSCcB71ImIiHQikfB+cCIiiUQiqnvCqX44/Z6IiIiIiIhIpHgtm4iIiIiIjIa3tzdCQ0Ph4eEhdBSiBsGinoiIiIiIjIZcLoePj4/QMYgaDKffExERERGR0UhJScHHH3+MlJQUoaMQNQgW9UREREREZDRycnIQFhaGnJwcoaMQNQgW9UREREREREQixaKeiIiIiIiISKRY1BMRERERERGJFIt6IiIiIiIyGg4ODnjppZfg4OAgdBSiBiHRaDQaoUMQERERERERke44Uk9EREREREQkUizqiYiIiIiIiESKRT0RERERERGRSLGoJyIiIiIiIhIpFvVEREREREREIsWinoiIiIiIiEikWNQTERERERERiRSLeiIiIiIiIiKRYlFPREREREREJFIs6omIiIiIiIhEikU9ERERERERkUixqCciIiIiIiISKRb1RERERERERCLFop6IiIiIiIhIpFjUExEREREREYnU/wFD7qDlMUDR6gAAAABJRU5ErkJggg=="
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "reps = 1\n",
    "edges = [(0, 1), (0, 2), (1, 3)] # Hardcoded edges for testing.\n",
    "n_edges = len(edges)\n",
    "\n",
    "data_symbols = []\n",
    "for layer in range(reps):\n",
    "    data = [Parameter(f'layer[{layer}]_v[{qubit}]') for qubit in range(n_qubits)]\n",
    "    data += [[Parameter(f'layer[{layer}]_e[{ew}]') for ew in range(n_edges)]]\n",
    "    data_symbols.append(data)\n",
    "\n",
    "qc = graph_encoding_circuit(edges, n_qubits, reps, data_symbols)\n",
    "qc.draw(output='mpl', fold=50, style=\"iqp\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:31:28.092931800Z",
     "start_time": "2023-12-10T12:31:27.934401900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classical Layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Equivariant Layer\n",
    "\n",
    "Not to be confused with the Equivariant Parametrized Quantum Circuit!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class EquivariantLayer(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_input_params : int,\n",
    "            n_vars : int,\n",
    "            n_edges : int,\n",
    "            circuit_depth : int,\n",
    "            params : list):\n",
    "        super(EquivariantLayer, self).__init__()\n",
    "\n",
    "        # Define weights for the Layer\n",
    "        self.num_input_params = num_input_params * circuit_depth\n",
    "        self.num_params = 2 * circuit_depth\n",
    "        self.circuit_depth = circuit_depth\n",
    "\n",
    "        param_init = torch.ones(1, self.num_params, dtype=torch.float32)\n",
    "        self.params = torch.nn.Parameter(param_init)\n",
    "\n",
    "        self.param_repeats = []\n",
    "        for layer in range(self.circuit_depth):\n",
    "            self.param_repeats.append(n_vars)\n",
    "            self.param_repeats.append(n_edges)\n",
    "\n",
    "        alphabetical_params = sorted(params)\n",
    "        self.indices = [params.index(a) for a in alphabetical_params]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        repeated_params = self.params.repeat_interleave(torch.tensor(self.param_repeats))\n",
    "\n",
    "        repeat_inputs = inputs.repeat(self.circuit_depth, 1)\n",
    "\n",
    "        data_values = repeat_inputs * repeated_params\n",
    "        output = data_values[:, self.indices]\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:31:28.099656900Z",
     "start_time": "2023-12-10T12:31:28.097112500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q-Learning model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "class QModel(Module):\n",
    "    def __init__(self, n_input_params : int, n_vars : int, num_edges_in_graph : int, n_layers : int,\n",
    "                 flattened_data_symbols : list, circuit : QuantumCircuit):\n",
    "        \"\"\"\n",
    "        The neural network that will parameterize the function Q.\n",
    "        \n",
    "        :param n_input_params: Number of input parameters.\n",
    "        :param n_vars: Number of variables in the Equivariant Layer.\n",
    "        :param num_edges_in_graph: Number of edges in the graph.\n",
    "        :param n_layers: Number of repetitions/layers.\n",
    "        :param flattened_data_symbols: Flattened data symbols for the Equivariant Layer.\n",
    "        :param circuit: The Equivariant Quantum Circuit.\n",
    "        \"\"\"\n",
    "        super(QModel, self).__init__()\n",
    "        self.n_input_params = n_input_params\n",
    "        self.n_vars = n_vars\n",
    "        self.num_edges_in_graph = num_edges_in_graph\n",
    "        self.n_layers = n_layers\n",
    "        self.flattened_data_symbols = flattened_data_symbols\n",
    "        \n",
    "        # Classical encoding layer.\n",
    "        self.encoding_layer = EquivariantLayer(num_input_params=self.n_input_params, n_vars=self.n_vars, \n",
    "                                               n_edges=self.num_edges_in_graph, circuit_depth=self.n_layers, \n",
    "                                               params=self.flattened_data_symbols)\n",
    "        \n",
    "        # The Equivariant Quantum Circuit for Torch Connector.\n",
    "        self.circuit = circuit\n",
    "\n",
    "    def forward(self, input_data, observables):\n",
    "        \"\"\"\n",
    "        Forward execution of the neural net.\n",
    "        \n",
    "        :param input_data: Input data.\n",
    "        :param observables: Observables for the Quantum Circuit.\n",
    "        :return: Expectation values for all available nodes.\n",
    "        \"\"\"\n",
    "        encoding_output = self.encoding_layer(input_data)\n",
    "\n",
    "        qnn = EstimatorQNN(\n",
    "            circuit=self.circuit,\n",
    "            input_params=self.circuit.parameters,\n",
    "            observables=observables\n",
    "        )\n",
    "        qnn = TorchConnector(qnn)\n",
    "        expectation_values = qnn(encoding_output)\n",
    "\n",
    "        return expectation_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:31:28.106598100Z",
     "start_time": "2023-12-10T12:31:28.104060600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, hyperparams : dict, path : str = BASE_PATH):\n",
    "        self.path = path\n",
    "\n",
    "        self.n_vars = hyperparams.get('n_vars')\n",
    "        self.episodes = hyperparams.get('episodes')\n",
    "        self.batch_size = hyperparams.get('batch_size')\n",
    "        self.gamma = hyperparams.get('gamma')\n",
    "        self.n_layers = hyperparams.get('n_layers')\n",
    "        self.update_after = hyperparams.get('update_after')\n",
    "        self.update_target_after = hyperparams.get('update_target_after')\n",
    "        self.memory_length = hyperparams.get('memory_length')\n",
    "        self.n_pred_layers = hyperparams.get('n_pred_layers', 1)\n",
    "\n",
    "        self.epsilon = hyperparams.get('epsilon')\n",
    "        self.epsilon_schedule = hyperparams.get('epsilon_schedule')\n",
    "        self.epsilon_min = hyperparams.get('epsilon_min')\n",
    "        self.epsilon_decay = hyperparams.get('epsilon_decay')\n",
    "\n",
    "        self.learning_rate = hyperparams.get('learning_rate', 0.01)\n",
    "        self.learning_rate_out = hyperparams.get('learning_rate_out', 0.01)\n",
    "        self.learning_rate_in = hyperparams.get('learning_rate_in', 0.01)\n",
    "\n",
    "        self.optimizers = []\n",
    "        self.w_idx = []\n",
    "\n",
    "        self.memory = self.initialize_memory()\n",
    "\n",
    "        self.meta = self.generate_meta_data_dict()\n",
    "\n",
    "    def generate_meta_data_dict(self):\n",
    "        meta = {key: str(value) for key, value in self.__dict__.items() if\n",
    "                not key.startswith('__') and not callable(key)}\n",
    "\n",
    "        del meta['memory']\n",
    "\n",
    "        return meta\n",
    "\n",
    "    def initialize_memory(self):\n",
    "        memory = deque(maxlen=self.memory_length)\n",
    "        return memory\n",
    "\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        transition = self.interaction(\n",
    "            state, action, reward, next_state, float(done))\n",
    "        self.memory.append(transition)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:31:28.114414900Z",
     "start_time": "2023-12-10T12:31:28.106598100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, running avg 2.4188230225427496, epsilon 0.99\n",
      "\tFinal tour: [0, 7, 4, 2, 8, 1, 9, 6, 3, 5, 0]\n",
      "Episode 2, running avg 2.0803749693556592, epsilon 0.9801\n",
      "\tFinal tour: [0, 6, 8, 1, 7, 9, 5, 2, 4, 3, 0]\n",
      "Episode 3, running avg 2.1017409681335315, epsilon 0.9702989999999999\n",
      "\tFinal tour: [0, 4, 1, 2, 9, 7, 8, 5, 6, 3, 0]\n",
      "Episode 4, running avg 1.9896176492234434, epsilon 0.96059601\n",
      "\tFinal tour: [0, 7, 9, 6, 5, 3, 1, 8, 4, 2, 0]\n",
      "Episode 5, running avg 1.9847908270218995, epsilon 0.9509900498999999\n",
      "\tFinal tour: [0, 1, 2, 4, 3, 7, 8, 9, 6, 5, 0]\n",
      "Episode 6, running avg 1.9961880693167628, epsilon 0.9414801494009999\n",
      "\tFinal tour: [0, 7, 5, 3, 4, 6, 9, 1, 2, 8, 0]\n",
      "Episode 7, running avg 2.042673922547309, epsilon 0.9320653479069899\n",
      "\tFinal tour: [0, 5, 6, 9, 3, 1, 7, 8, 4, 2, 0]\n",
      "Episode 8, running avg 2.0127835649490837, epsilon 0.92274469442792\n",
      "\tFinal tour: [0, 1, 3, 6, 8, 9, 2, 4, 5, 7, 0]\n",
      "Episode 9, running avg 1.9941330111026225, epsilon 0.9135172474836407\n",
      "\tFinal tour: [0, 6, 9, 4, 3, 7, 8, 5, 2, 1, 0]\n",
      "Episode 10, loss 0.6556211896572275, running avg 1.9831265297128504, epsilon 0.9043820750088043\n",
      "\tFinal tour: [0, 9, 7, 5, 4, 8, 2, 3, 6, 1, 0]\n",
      "Episode 11, running avg 2.0000300364278893, epsilon 0.8953382542587163\n",
      "\tFinal tour: [0, 9, 1, 6, 4, 8, 3, 5, 2, 7, 0]\n",
      "Episode 12, running avg 2.0340957431701217, epsilon 0.8863848717161291\n",
      "\tFinal tour: [0, 1, 9, 7, 5, 6, 4, 8, 3, 2, 0]\n",
      "Episode 13, running avg 2.0226030766391894, epsilon 0.8775210229989678\n",
      "\tFinal tour: [0, 6, 2, 4, 1, 8, 3, 5, 7, 9, 0]\n",
      "Episode 14, running avg 2.036784870509565, epsilon 0.8687458127689781\n",
      "\tFinal tour: [0, 6, 2, 4, 5, 7, 1, 9, 3, 8, 0]\n",
      "Episode 15, running avg 2.061440752102644, epsilon 0.8600583546412883\n",
      "\tFinal tour: [0, 1, 4, 2, 6, 7, 8, 9, 5, 3, 0]\n",
      "Episode 16, running avg 2.0704668154038, epsilon 0.8514577710948754\n",
      "\tFinal tour: [0, 4, 7, 1, 8, 9, 2, 3, 5, 6, 0]\n",
      "Episode 17, running avg 2.0823561253155445, epsilon 0.8429431933839266\n",
      "\tFinal tour: [0, 5, 9, 2, 4, 3, 6, 1, 7, 8, 0]\n",
      "Episode 18, running avg 2.0980106666959104, epsilon 0.8345137614500874\n",
      "\tFinal tour: [0, 7, 9, 5, 1, 3, 8, 2, 6, 4, 0]\n",
      "Episode 19, running avg 2.1062735770776846, epsilon 0.8261686238355865\n",
      "\tFinal tour: [0, 9, 5, 3, 6, 8, 1, 4, 7, 2, 0]\n",
      "Episode 20, loss 0.2127113879909362, running avg 2.093357123156869, epsilon 0.8179069375972307\n",
      "\tFinal tour: [0, 2, 9, 3, 4, 1, 7, 8, 5, 6, 0]\n",
      "Episode 21, running avg 2.0785027703576304, epsilon 0.8097278682212583\n",
      "\tFinal tour: [0, 7, 2, 4, 9, 6, 5, 8, 3, 1, 0]\n",
      "Episode 22, running avg 2.047752315433127, epsilon 0.8016305895390458\n",
      "\tFinal tour: [0, 5, 2, 4, 1, 7, 8, 3, 9, 6, 0]\n",
      "Episode 23, running avg 2.0421206127305243, epsilon 0.7936142836436553\n",
      "\tFinal tour: [0, 6, 7, 1, 9, 8, 3, 4, 2, 5, 0]\n",
      "Episode 24, running avg 2.0193413567114766, epsilon 0.7856781408072188\n",
      "\tFinal tour: [0, 6, 8, 1, 3, 2, 4, 9, 5, 7, 0]\n",
      "Episode 25, running avg 2.003751248985944, epsilon 0.7778213593991465\n",
      "\tFinal tour: [0, 5, 3, 8, 6, 7, 4, 1, 2, 9, 0]\n",
      "Episode 26, running avg 2.006835434895387, epsilon 0.7700431458051551\n",
      "\tFinal tour: [0, 5, 2, 3, 9, 8, 6, 4, 7, 1, 0]\n",
      "Episode 27, running avg 1.989355552194514, epsilon 0.7623427143471035\n",
      "\tFinal tour: [0, 5, 8, 1, 3, 9, 4, 7, 2, 6, 0]\n",
      "Episode 28, running avg 1.9722593538547548, epsilon 0.7547192872036325\n",
      "\tFinal tour: [0, 2, 1, 8, 6, 5, 3, 7, 9, 4, 0]\n",
      "Episode 29, running avg 1.9737951860401537, epsilon 0.7471720943315961\n",
      "\tFinal tour: [0, 3, 4, 2, 8, 6, 9, 5, 1, 7, 0]\n",
      "Episode 30, loss 0.7065270128011637, running avg 1.972814854483483, epsilon 0.7397003733882802\n",
      "\tFinal tour: [0, 7, 3, 1, 9, 6, 8, 5, 4, 2, 0]\n",
      "Episode 31, running avg 1.9799474368531056, epsilon 0.7323033696543974\n",
      "\tFinal tour: [0, 3, 2, 4, 9, 6, 8, 1, 5, 7, 0]\n",
      "Episode 32, running avg 1.9890288399441767, epsilon 0.7249803359578534\n",
      "\tFinal tour: [0, 9, 4, 7, 3, 8, 5, 1, 6, 2, 0]\n",
      "Episode 33, running avg 1.9829096377882265, epsilon 0.7177305325982748\n",
      "\tFinal tour: [0, 7, 6, 1, 5, 4, 9, 2, 8, 3, 0]\n",
      "Episode 34, running avg 1.986828596427772, epsilon 0.7105532272722921\n",
      "\tFinal tour: [0, 9, 5, 4, 7, 2, 6, 1, 8, 3, 0]\n",
      "Episode 35, running avg 2.0015971885004986, epsilon 0.7034476949995692\n",
      "\tFinal tour: [0, 6, 8, 9, 4, 3, 2, 5, 1, 7, 0]\n",
      "Episode 36, running avg 2.0120745150273858, epsilon 0.6964132180495735\n",
      "\tFinal tour: [0, 6, 8, 5, 1, 3, 2, 4, 7, 9, 0]\n",
      "Episode 37, running avg 2.008029836930824, epsilon 0.6894490858690777\n",
      "\tFinal tour: [0, 9, 3, 6, 4, 5, 7, 1, 2, 8, 0]\n",
      "Episode 38, running avg 2.005127860023734, epsilon 0.682554595010387\n",
      "\tFinal tour: [0, 8, 3, 4, 5, 1, 7, 9, 6, 2, 0]\n",
      "Episode 39, running avg 2.002698026024287, epsilon 0.6757290490602831\n",
      "\tFinal tour: [0, 6, 2, 4, 3, 5, 7, 8, 9, 1, 0]\n",
      "Episode 40, loss 0.3902447682292972, running avg 2.0069588918401893, epsilon 0.6689717585696803\n",
      "\tFinal tour: [0, 8, 1, 6, 2, 5, 4, 7, 3, 9, 0]\n",
      "Episode 41, running avg 2.01185458287321, epsilon 0.6622820409839835\n",
      "\tFinal tour: [0, 9, 5, 2, 7, 1, 4, 3, 8, 6, 0]\n",
      "Episode 42, running avg 2.011801206995533, epsilon 0.6556592205741436\n",
      "\tFinal tour: [0, 7, 1, 8, 9, 6, 5, 2, 4, 3, 0]\n",
      "Episode 43, running avg 2.0094510780414785, epsilon 0.6491026283684022\n",
      "\tFinal tour: [0, 5, 7, 2, 9, 1, 8, 3, 6, 4, 0]\n",
      "Episode 44, running avg 2.0087365953178433, epsilon 0.6426116020847181\n",
      "\tFinal tour: [0, 9, 4, 3, 1, 8, 6, 7, 5, 2, 0]\n",
      "Episode 45, running avg 2.0001214083249392, epsilon 0.6361854860638709\n",
      "\tFinal tour: [0, 4, 8, 9, 6, 1, 2, 7, 3, 5, 0]\n",
      "Episode 46, running avg 2.0078493838800218, epsilon 0.6298236312032323\n",
      "\tFinal tour: [0, 8, 7, 4, 3, 2, 6, 9, 5, 1, 0]\n",
      "Episode 47, running avg 2.0150314062523096, epsilon 0.6235253948912\n",
      "\tFinal tour: [0, 8, 9, 2, 7, 6, 3, 1, 4, 5, 0]\n",
      "Episode 48, running avg 2.0118386539440674, epsilon 0.617290140942288\n",
      "\tFinal tour: [0, 5, 2, 8, 4, 1, 9, 7, 6, 3, 0]\n",
      "Episode 49, running avg 2.012401607444767, epsilon 0.6111172395328651\n",
      "\tFinal tour: [0, 1, 2, 3, 7, 4, 9, 8, 6, 5, 0]\n",
      "Episode 50, loss 0.8789574557050488, running avg 2.016382163549458, epsilon 0.6050060671375365\n",
      "\tFinal tour: [0, 3, 8, 6, 5, 1, 4, 7, 9, 2, 0]\n",
      "Episode 51, running avg 2.016561715251759, epsilon 0.5989560064661611\n",
      "\tFinal tour: [0, 9, 6, 7, 2, 4, 3, 5, 8, 1, 0]\n",
      "Episode 52, running avg 2.0131654282184765, epsilon 0.5929664464014994\n",
      "\tFinal tour: [0, 7, 1, 5, 8, 3, 2, 4, 9, 6, 0]\n",
      "Episode 53, running avg 2.0151212816558015, epsilon 0.5870367819374844\n",
      "\tFinal tour: [0, 4, 6, 3, 1, 8, 2, 9, 5, 7, 0]\n",
      "Episode 54, running avg 2.021403091648899, epsilon 0.5811664141181095\n",
      "\tFinal tour: [0, 6, 8, 4, 9, 1, 7, 5, 2, 3, 0]\n",
      "Episode 55, running avg 2.028211939340928, epsilon 0.5753547499769285\n",
      "\tFinal tour: [0, 4, 3, 7, 2, 9, 8, 1, 6, 5, 0]\n",
      "Episode 56, running avg 2.0246245355354957, epsilon 0.5696012024771592\n",
      "\tFinal tour: [0, 2, 3, 1, 5, 8, 9, 4, 7, 6, 0]\n",
      "Episode 57, running avg 2.025790895648049, epsilon 0.5639051904523876\n",
      "\tFinal tour: [0, 8, 3, 4, 5, 6, 2, 1, 7, 9, 0]\n",
      "Episode 58, running avg 2.01994910831095, epsilon 0.5582661385478638\n",
      "\tFinal tour: [0, 9, 4, 8, 1, 2, 5, 3, 7, 6, 0]\n",
      "Episode 59, running avg 2.017750857988009, epsilon 0.5526834771623851\n",
      "\tFinal tour: [0, 2, 3, 8, 5, 9, 6, 7, 1, 4, 0]\n",
      "Episode 60, loss 0.7801710029770301, running avg 2.018886629725253, epsilon 0.5471566423907612\n",
      "\tFinal tour: [0, 9, 4, 2, 3, 7, 6, 8, 1, 5, 0]\n",
      "Episode 61, running avg 2.0231601292152175, epsilon 0.5416850759668536\n",
      "\tFinal tour: [0, 9, 8, 5, 3, 1, 2, 7, 4, 6, 0]\n",
      "Episode 62, running avg 2.0180160504509597, epsilon 0.536268225207185\n",
      "\tFinal tour: [0, 7, 3, 9, 8, 5, 6, 1, 4, 2, 0]\n",
      "Episode 63, running avg 2.0209871208466965, epsilon 0.5309055429551132\n",
      "\tFinal tour: [0, 4, 2, 3, 6, 7, 1, 8, 5, 9, 0]\n",
      "Episode 64, running avg 2.0145137746243216, epsilon 0.525596487525562\n",
      "\tFinal tour: [0, 4, 5, 8, 6, 1, 3, 2, 7, 9, 0]\n",
      "Episode 65, running avg 2.01148434211044, epsilon 0.5203405226503064\n",
      "\tFinal tour: [0, 7, 8, 2, 4, 3, 1, 9, 6, 5, 0]\n",
      "Episode 66, running avg 2.009540417941171, epsilon 0.5151371174238033\n",
      "\tFinal tour: [0, 6, 1, 3, 8, 9, 2, 4, 7, 5, 0]\n",
      "Episode 67, running avg 2.009736137857033, epsilon 0.5099857462495653\n",
      "\tFinal tour: [0, 9, 7, 4, 8, 5, 1, 2, 6, 3, 0]\n",
      "Episode 68, running avg 2.0067596232443456, epsilon 0.5048858887870696\n",
      "\tFinal tour: [0, 8, 6, 4, 2, 7, 9, 1, 3, 5, 0]\n",
      "Episode 69, running avg 2.0050754041025622, epsilon 0.4998370298991989\n",
      "\tFinal tour: [0, 5, 2, 3, 6, 4, 7, 1, 9, 8, 0]\n",
      "Episode 70, loss 0.447616150442784, running avg 1.9991767784689225, epsilon 0.49483865960020695\n",
      "\tFinal tour: [0, 5, 6, 7, 1, 8, 9, 2, 3, 4, 0]\n",
      "Episode 71, running avg 1.9948370037272203, epsilon 0.4898902730042049\n",
      "\tFinal tour: [0, 6, 1, 9, 3, 2, 4, 8, 5, 7, 0]\n",
      "Episode 72, running avg 1.9949621999903782, epsilon 0.48499137027416284\n",
      "\tFinal tour: [0, 5, 4, 3, 7, 1, 9, 8, 6, 2, 0]\n",
      "Episode 73, running avg 1.9930960158031144, epsilon 0.4801414565714212\n",
      "\tFinal tour: [0, 8, 3, 6, 7, 5, 4, 9, 2, 1, 0]\n",
      "Episode 74, running avg 1.9991329459915457, epsilon 0.475340042005707\n",
      "\tFinal tour: [0, 3, 4, 2, 6, 9, 8, 5, 7, 1, 0]\n",
      "Episode 75, running avg 2.0010584554242596, epsilon 0.47058664158564995\n",
      "\tFinal tour: [0, 3, 6, 2, 5, 9, 7, 1, 4, 8, 0]\n",
      "Episode 76, running avg 2.001060073514646, epsilon 0.4658807751697934\n",
      "\tFinal tour: [0, 8, 6, 4, 2, 3, 7, 1, 5, 9, 0]\n",
      "Episode 77, running avg 2.0069340225830192, epsilon 0.4612219674180955\n",
      "\tFinal tour: [0, 6, 7, 2, 4, 5, 1, 3, 9, 8, 0]\n",
      "Episode 78, running avg 2.0027331865966183, epsilon 0.45660974774391455\n",
      "\tFinal tour: [0, 8, 3, 1, 6, 9, 7, 4, 2, 5, 0]\n",
      "Episode 79, running avg 1.9995981655994894, epsilon 0.4520436502664754\n",
      "\tFinal tour: [0, 6, 2, 1, 4, 8, 3, 5, 7, 9, 0]\n",
      "Episode 80, loss 0.4480593392313826, running avg 2.000880055304046, epsilon 0.44752321376381066\n",
      "\tFinal tour: [0, 5, 1, 4, 8, 9, 6, 7, 3, 2, 0]\n",
      "Episode 81, running avg 2.002790673378758, epsilon 0.44304798162617254\n",
      "\tFinal tour: [0, 9, 8, 2, 4, 7, 6, 5, 3, 1, 0]\n",
      "Episode 82, running avg 2.0104963319328713, epsilon 0.4386175018099108\n",
      "\tFinal tour: [0, 9, 8, 1, 3, 7, 6, 5, 4, 2, 0]\n",
      "Episode 83, running avg 2.0063656583459863, epsilon 0.4342313267918117\n",
      "\tFinal tour: [0, 3, 1, 2, 8, 6, 7, 4, 9, 5, 0]\n",
      "Episode 84, running avg 1.9979420365748477, epsilon 0.4298890135238936\n",
      "\tFinal tour: [0, 1, 4, 8, 7, 2, 6, 5, 9, 3, 0]\n",
      "Episode 85, running avg 1.9991067681800176, epsilon 0.42559012338865465\n",
      "\tFinal tour: [0, 3, 8, 5, 4, 2, 9, 7, 1, 6, 0]\n",
      "Episode 86, running avg 2.006923722439593, epsilon 0.4213342221547681\n",
      "\tFinal tour: [0, 7, 2, 6, 4, 9, 5, 3, 1, 8, 0]\n",
      "Episode 87, running avg 2.0117660796810277, epsilon 0.41712087993322045\n",
      "\tFinal tour: [0, 2, 4, 7, 9, 8, 1, 6, 5, 3, 0]\n",
      "Episode 88, running avg 2.0166265592070025, epsilon 0.41294967113388825\n",
      "\tFinal tour: [0, 3, 6, 7, 4, 9, 8, 2, 5, 1, 0]\n",
      "Episode 89, running avg 2.023914419040343, epsilon 0.40882017442254937\n",
      "\tFinal tour: [0, 6, 3, 9, 2, 5, 7, 8, 1, 4, 0]\n",
      "Episode 90, loss 0.302195723263655, running avg 2.02210219939823, epsilon 0.4047319726783239\n",
      "\tFinal tour: [0, 5, 9, 2, 7, 3, 6, 8, 1, 4, 0]\n",
      "Episode 91, running avg 2.0230614881834326, epsilon 0.40068465295154065\n",
      "\tFinal tour: [0, 7, 3, 8, 2, 4, 9, 5, 6, 1, 0]\n",
      "Episode 92, running avg 2.021166339675346, epsilon 0.39667780642202527\n",
      "\tFinal tour: [0, 8, 9, 2, 6, 3, 5, 4, 7, 1, 0]\n",
      "Episode 93, running avg 2.0229897618918735, epsilon 0.392711028357805\n",
      "\tFinal tour: [0, 4, 7, 5, 3, 2, 8, 9, 1, 6, 0]\n",
      "Episode 94, running avg 2.027180511970335, epsilon 0.38878391807422696\n",
      "\tFinal tour: [0, 8, 5, 6, 3, 4, 2, 1, 7, 9, 0]\n",
      "Episode 95, running avg 2.029154081997528, epsilon 0.3848960788934847\n",
      "\tFinal tour: [0, 5, 6, 7, 8, 3, 1, 2, 4, 9, 0]\n",
      "Episode 96, running avg 2.0283732331105484, epsilon 0.38104711810454983\n",
      "\tFinal tour: [0, 3, 7, 2, 9, 5, 8, 6, 1, 4, 0]\n",
      "Episode 97, running avg 2.0306993157334685, epsilon 0.37723664692350434\n",
      "\tFinal tour: [0, 3, 9, 5, 1, 4, 2, 6, 8, 7, 0]\n",
      "Episode 98, running avg 2.0333079429149903, epsilon 0.37346428045426927\n",
      "\tFinal tour: [0, 7, 8, 3, 2, 5, 1, 4, 9, 6, 0]\n",
      "Episode 99, running avg 2.029180706224326, epsilon 0.36972963764972655\n",
      "\tFinal tour: [0, 9, 1, 4, 3, 5, 8, 6, 7, 2, 0]\n",
      "Episode 100, loss 0.22916317283266782, running avg 2.030844897464117, epsilon 0.36603234127322926\n",
      "\tFinal tour: [0, 9, 1, 8, 2, 7, 4, 5, 3, 6, 0]\n",
      "Episode 101, running avg 2.032990444563784, epsilon 0.36237201786049694\n",
      "\tFinal tour: [0, 8, 1, 7, 3, 6, 5, 9, 4, 2, 0]\n",
      "Episode 102, running avg 2.0342793383649114, epsilon 0.358748297681892\n",
      "\tFinal tour: [0, 9, 1, 3, 2, 5, 6, 4, 8, 7, 0]\n",
      "Episode 103, running avg 2.033246660780834, epsilon 0.35516081470507305\n",
      "\tFinal tour: [0, 3, 7, 4, 5, 9, 8, 2, 1, 6, 0]\n",
      "Episode 104, running avg 2.037466914452664, epsilon 0.3516092065580223\n",
      "\tFinal tour: [0, 7, 3, 4, 2, 9, 8, 1, 6, 5, 0]\n",
      "Episode 105, running avg 2.0433305789547904, epsilon 0.34809311449244207\n",
      "\tFinal tour: [0, 6, 7, 8, 9, 1, 5, 2, 4, 3, 0]\n",
      "Episode 106, running avg 2.044088699731069, epsilon 0.34461218334751764\n",
      "\tFinal tour: [0, 9, 5, 3, 2, 8, 1, 4, 6, 7, 0]\n",
      "Episode 107, running avg 2.0452135464179673, epsilon 0.34116606151404244\n",
      "\tFinal tour: [0, 5, 6, 1, 7, 3, 8, 2, 4, 9, 0]\n",
      "Episode 108, running avg 2.0484357960779156, epsilon 0.337754400898902\n",
      "\tFinal tour: [0, 6, 3, 8, 7, 4, 1, 9, 5, 2, 0]\n",
      "Episode 109, running avg 2.0484451459363315, epsilon 0.334376856889913\n",
      "\tFinal tour: [0, 6, 2, 7, 3, 1, 9, 5, 4, 8, 0]\n",
      "Episode 110, loss 0.7358732405710413, running avg 2.04814685922863, epsilon 0.33103308832101386\n",
      "\tFinal tour: [0, 5, 3, 4, 9, 1, 7, 6, 2, 8, 0]\n",
      "Episode 111, running avg 2.0406748360398677, epsilon 0.3277227574378037\n",
      "\tFinal tour: [0, 2, 5, 3, 4, 6, 8, 1, 7, 9, 0]\n",
      "Episode 112, running avg 2.0402246622209548, epsilon 0.3244455298634257\n",
      "\tFinal tour: [0, 4, 6, 5, 1, 9, 8, 2, 3, 7, 0]\n",
      "Episode 113, running avg 2.0460467825857167, epsilon 0.3212010745647914\n",
      "\tFinal tour: [0, 9, 8, 6, 2, 5, 1, 7, 4, 3, 0]\n",
      "Episode 114, running avg 2.049773129344178, epsilon 0.3179890638191435\n",
      "\tFinal tour: [0, 6, 8, 4, 1, 7, 5, 2, 3, 9, 0]\n",
      "Episode 115, running avg 2.043743645841982, epsilon 0.31480917318095203\n",
      "\tFinal tour: [0, 8, 3, 1, 7, 6, 2, 9, 5, 4, 0]\n",
      "Episode 116, running avg 2.0435326851549243, epsilon 0.3116610814491425\n",
      "\tFinal tour: [0, 3, 6, 2, 4, 1, 9, 7, 8, 5, 0]\n",
      "Episode 117, running avg 2.042116043265301, epsilon 0.30854447063465107\n",
      "\tFinal tour: [0, 6, 8, 3, 1, 7, 5, 4, 2, 9, 0]\n",
      "Episode 118, running avg 2.03722610941968, epsilon 0.30545902592830454\n",
      "\tFinal tour: [0, 3, 2, 1, 9, 4, 7, 5, 6, 8, 0]\n",
      "Episode 119, running avg 2.03208861093022, epsilon 0.3024044356690215\n",
      "\tFinal tour: [0, 8, 9, 1, 5, 3, 2, 4, 7, 6, 0]\n",
      "Episode 120, loss 0.2514737783469377, running avg 2.037125908529293, epsilon 0.29938039131233124\n",
      "\tFinal tour: [0, 3, 5, 6, 9, 7, 4, 2, 1, 8, 0]\n",
      "Episode 121, running avg 2.038754014262782, epsilon 0.2963865873992079\n",
      "\tFinal tour: [0, 9, 1, 3, 2, 7, 4, 8, 6, 5, 0]\n",
      "Episode 122, running avg 2.049906146085444, epsilon 0.29342272152521587\n",
      "\tFinal tour: [0, 5, 2, 9, 1, 4, 8, 6, 3, 7, 0]\n",
      "Episode 123, running avg 2.0530681982336354, epsilon 0.2904884943099637\n",
      "\tFinal tour: [0, 3, 8, 9, 1, 5, 7, 4, 6, 2, 0]\n",
      "Episode 124, running avg 2.0604001856095624, epsilon 0.28758360936686406\n",
      "\tFinal tour: [0, 3, 4, 9, 7, 5, 2, 1, 8, 6, 0]\n",
      "Episode 125, running avg 2.064948428996492, epsilon 0.2847077732731954\n",
      "\tFinal tour: [0, 5, 4, 3, 1, 9, 8, 2, 6, 7, 0]\n",
      "Episode 126, running avg 2.0640294542550697, epsilon 0.28186069554046345\n",
      "\tFinal tour: [0, 2, 4, 6, 5, 8, 1, 3, 9, 7, 0]\n",
      "Episode 127, running avg 2.070833626492002, epsilon 0.2790420885850588\n",
      "\tFinal tour: [0, 7, 1, 8, 9, 5, 3, 4, 6, 2, 0]\n",
      "Episode 128, running avg 2.070787730905417, epsilon 0.2762516676992082\n",
      "\tFinal tour: [0, 9, 7, 4, 2, 5, 6, 3, 8, 1, 0]\n",
      "Episode 129, running avg 2.0696008589151447, epsilon 0.27348915102221616\n",
      "\tFinal tour: [0, 9, 8, 2, 5, 7, 4, 3, 6, 1, 0]\n",
      "Episode 130, loss 0.8002463723920072, running avg 2.0692672699373356, epsilon 0.270754259511994\n",
      "\tFinal tour: [0, 7, 4, 6, 5, 2, 9, 8, 1, 3, 0]\n",
      "Episode 131, running avg 2.0656563871764924, epsilon 0.26804671691687404\n",
      "\tFinal tour: [0, 3, 6, 1, 9, 5, 4, 7, 2, 8, 0]\n",
      "Episode 132, running avg 2.062380475485643, epsilon 0.2653662497477053\n",
      "\tFinal tour: [0, 8, 5, 4, 6, 3, 1, 9, 7, 2, 0]\n",
      "Episode 133, running avg 2.064589198506916, epsilon 0.2627125872502282\n",
      "\tFinal tour: [0, 6, 9, 2, 5, 7, 8, 3, 4, 1, 0]\n",
      "Episode 134, running avg 2.06474763459304, epsilon 0.2600854613777259\n",
      "\tFinal tour: [0, 7, 2, 5, 1, 9, 3, 4, 6, 8, 0]\n",
      "Episode 135, running avg 2.059348272626157, epsilon 0.2574846067639487\n",
      "\tFinal tour: [0, 3, 7, 6, 1, 5, 4, 2, 9, 8, 0]\n",
      "Episode 136, running avg 2.0548660795633324, epsilon 0.2549097606963092\n",
      "\tFinal tour: [0, 5, 7, 9, 3, 2, 4, 1, 8, 6, 0]\n",
      "Episode 137, running avg 2.060029176295274, epsilon 0.2523606630893461\n",
      "\tFinal tour: [0, 3, 9, 4, 8, 7, 5, 2, 1, 6, 0]\n",
      "Episode 138, running avg 2.06297057928079, epsilon 0.24983705645845267\n",
      "\tFinal tour: [0, 9, 7, 6, 8, 3, 5, 1, 4, 2, 0]\n",
      "Episode 139, running avg 2.0663083152229005, epsilon 0.24733868589386815\n",
      "\tFinal tour: [0, 7, 1, 8, 5, 4, 2, 9, 3, 6, 0]\n",
      "Episode 140, loss 0.432550015955021, running avg 2.0620597683589024, epsilon 0.24486529903492946\n",
      "\tFinal tour: [0, 3, 2, 4, 1, 9, 7, 5, 8, 6, 0]\n",
      "Episode 141, running avg 2.0649100959490125, epsilon 0.24241664604458016\n",
      "\tFinal tour: [0, 9, 5, 8, 3, 2, 6, 1, 7, 4, 0]\n",
      "Episode 142, running avg 2.0626480652477635, epsilon 0.23999247958413436\n",
      "\tFinal tour: [0, 9, 1, 5, 3, 4, 2, 6, 7, 8, 0]\n",
      "Episode 143, running avg 2.0664407496604364, epsilon 0.23759255478829303\n",
      "\tFinal tour: [0, 7, 1, 6, 2, 9, 4, 3, 5, 8, 0]\n",
      "Episode 144, running avg 2.0657642902683553, epsilon 0.2352166292404101\n",
      "\tFinal tour: [0, 6, 9, 2, 4, 1, 3, 8, 7, 5, 0]\n",
      "Episode 145, running avg 2.0686944603503337, epsilon 0.232864462948006\n",
      "\tFinal tour: [0, 6, 2, 8, 9, 5, 7, 1, 3, 4, 0]\n",
      "Episode 146, running avg 2.06844160150961, epsilon 0.23053581831852593\n",
      "\tFinal tour: [0, 1, 5, 9, 8, 7, 6, 4, 2, 3, 0]\n",
      "Episode 147, running avg 2.0696743070791155, epsilon 0.22823046013534068\n",
      "\tFinal tour: [0, 1, 7, 5, 6, 3, 4, 2, 8, 9, 0]\n",
      "Episode 148, running avg 2.0713688621219335, epsilon 0.22594815553398728\n",
      "\tFinal tour: [0, 6, 3, 8, 7, 1, 2, 9, 5, 4, 0]\n",
      "Episode 149, running avg 2.070369452810773, epsilon 0.22368867397864742\n",
      "\tFinal tour: [0, 9, 4, 8, 5, 6, 7, 3, 1, 2, 0]\n",
      "Episode 150, loss 0.8431653795841736, running avg 2.072482318489617, epsilon 0.22145178723886094\n",
      "\tFinal tour: [0, 6, 8, 2, 4, 5, 9, 1, 3, 7, 0]\n",
      "Episode 151, running avg 2.0754507335828376, epsilon 0.21923726936647234\n",
      "\tFinal tour: [0, 8, 1, 5, 3, 9, 7, 4, 2, 6, 0]\n",
      "Episode 152, running avg 2.077719659594232, epsilon 0.2170448966728076\n",
      "\tFinal tour: [0, 3, 5, 4, 6, 8, 2, 7, 1, 9, 0]\n",
      "Episode 153, running avg 2.0743940868682924, epsilon 0.21487444770607952\n",
      "\tFinal tour: [0, 4, 7, 1, 8, 2, 5, 6, 3, 9, 0]\n",
      "Episode 154, running avg 2.0773957802500385, epsilon 0.21272570322901874\n",
      "\tFinal tour: [0, 7, 1, 3, 4, 6, 9, 8, 5, 2, 0]\n",
      "Episode 155, running avg 2.071016721309645, epsilon 0.21059844619672854\n",
      "\tFinal tour: [0, 9, 8, 2, 6, 4, 3, 5, 1, 7, 0]\n",
      "Episode 156, running avg 2.0693437126183283, epsilon 0.20849246173476127\n",
      "\tFinal tour: [0, 3, 8, 5, 1, 9, 6, 2, 4, 7, 0]\n",
      "Episode 157, running avg 2.071547733981959, epsilon 0.20640753711741366\n",
      "\tFinal tour: [0, 5, 2, 6, 8, 9, 4, 7, 3, 1, 0]\n",
      "Episode 158, running avg 2.076603184160449, epsilon 0.20434346174623952\n",
      "\tFinal tour: [0, 7, 3, 1, 2, 4, 9, 5, 6, 8, 0]\n",
      "Episode 159, running avg 2.0780264857809096, epsilon 0.20230002712877712\n",
      "\tFinal tour: [0, 9, 3, 2, 8, 1, 7, 6, 4, 5, 0]\n",
      "Episode 160, loss 0.4549369840112977, running avg 2.0763952684854816, epsilon 0.20027702685748935\n",
      "\tFinal tour: [0, 3, 8, 9, 7, 4, 6, 2, 1, 5, 0]\n",
      "Episode 161, running avg 2.073103608866461, epsilon 0.19827425658891445\n",
      "\tFinal tour: [0, 6, 7, 8, 2, 3, 1, 5, 9, 4, 0]\n",
      "Episode 162, running avg 2.076957141617919, epsilon 0.1962915140230253\n",
      "\tFinal tour: [0, 6, 3, 2, 5, 1, 7, 4, 8, 9, 0]\n",
      "Episode 163, running avg 2.0761135648156706, epsilon 0.19432859888279505\n",
      "\tFinal tour: [0, 8, 5, 4, 6, 3, 1, 9, 7, 2, 0]\n",
      "Episode 164, running avg 2.0801263098987746, epsilon 0.1923853128939671\n",
      "\tFinal tour: [0, 9, 8, 2, 6, 4, 3, 7, 1, 5, 0]\n",
      "Episode 165, running avg 2.078831113447207, epsilon 0.19046145976502743\n",
      "\tFinal tour: [0, 4, 7, 6, 5, 2, 3, 1, 8, 9, 0]\n",
      "Episode 166, running avg 2.083883839430749, epsilon 0.18855684516737714\n",
      "\tFinal tour: [0, 9, 1, 2, 5, 3, 6, 7, 4, 8, 0]\n",
      "Episode 167, running avg 2.083195043056947, epsilon 0.18667127671570335\n",
      "\tFinal tour: [0, 9, 8, 6, 3, 4, 1, 5, 2, 7, 0]\n",
      "Episode 168, running avg 2.085298584548735, epsilon 0.18480456394854633\n",
      "\tFinal tour: [0, 3, 4, 9, 8, 1, 2, 7, 5, 6, 0]\n",
      "Episode 169, running avg 2.088640745667804, epsilon 0.18295651830906087\n",
      "\tFinal tour: [0, 8, 3, 1, 7, 6, 4, 5, 9, 2, 0]\n",
      "Episode 170, loss 0.16926760408430552, running avg 2.094742532380773, epsilon 0.18112695312597027\n",
      "\tFinal tour: [0, 3, 8, 1, 2, 7, 6, 5, 9, 4, 0]\n",
      "Episode 171, running avg 2.0995018203254623, epsilon 0.17931568359471056\n",
      "\tFinal tour: [0, 3, 6, 7, 9, 5, 1, 8, 2, 4, 0]\n",
      "Episode 172, running avg 2.0978846471380126, epsilon 0.17752252675876345\n",
      "\tFinal tour: [0, 3, 4, 9, 8, 6, 2, 1, 7, 5, 0]\n",
      "Episode 173, running avg 2.10344285717177, epsilon 0.17574730149117582\n",
      "\tFinal tour: [0, 3, 6, 7, 2, 5, 1, 8, 4, 9, 0]\n",
      "Episode 174, running avg 2.096857133313588, epsilon 0.17398982847626407\n",
      "\tFinal tour: [0, 1, 5, 8, 7, 4, 2, 9, 6, 3, 0]\n",
      "Episode 175, running avg 2.097922340451139, epsilon 0.17224993019150142\n",
      "\tFinal tour: [0, 5, 7, 3, 2, 8, 1, 4, 9, 6, 0]\n",
      "Episode 176, running avg 2.099805018412643, epsilon 0.1705274308895864\n",
      "\tFinal tour: [0, 9, 5, 2, 6, 7, 8, 1, 3, 4, 0]\n",
      "Episode 177, running avg 2.0935005592443305, epsilon 0.16882215658069055\n",
      "\tFinal tour: [0, 6, 7, 9, 8, 5, 1, 2, 4, 3, 0]\n",
      "Episode 178, running avg 2.1003186718525937, epsilon 0.16713393501488363\n",
      "\tFinal tour: [0, 3, 8, 1, 9, 4, 6, 5, 7, 2, 0]\n",
      "Episode 179, running avg 2.1033200686848734, epsilon 0.16546259566473479\n",
      "\tFinal tour: [0, 5, 9, 4, 3, 1, 6, 7, 2, 8, 0]\n",
      "Episode 180, loss 0.7446277011848978, running avg 2.0998103159378942, epsilon 0.16380796970808745\n",
      "\tFinal tour: [0, 3, 4, 7, 2, 1, 8, 6, 5, 9, 0]\n",
      "Episode 181, running avg 2.1016936016813506, epsilon 0.16216989001100657\n",
      "\tFinal tour: [0, 8, 2, 3, 6, 1, 4, 7, 9, 5, 0]\n",
      "Episode 182, running avg 2.0935747577871004, epsilon 0.1605481911108965\n",
      "\tFinal tour: [0, 6, 3, 2, 5, 7, 4, 1, 8, 9, 0]\n",
      "Episode 183, running avg 2.0983682989638153, epsilon 0.15894270919978754\n",
      "\tFinal tour: [0, 8, 9, 2, 7, 4, 3, 1, 6, 5, 0]\n",
      "Episode 184, running avg 2.1047448657271266, epsilon 0.15735328210778965\n",
      "\tFinal tour: [0, 3, 8, 9, 1, 5, 7, 4, 6, 2, 0]\n",
      "Episode 185, running avg 2.1060615955556448, epsilon 0.15577974928671176\n",
      "\tFinal tour: [0, 9, 6, 2, 7, 3, 1, 8, 5, 4, 0]\n",
      "Episode 186, running avg 2.097055778236223, epsilon 0.15422195179384465\n",
      "\tFinal tour: [0, 8, 9, 7, 5, 3, 2, 6, 1, 4, 0]\n",
      "Episode 187, running avg 2.0917225786947147, epsilon 0.1526797322759062\n",
      "\tFinal tour: [0, 4, 3, 1, 2, 9, 8, 6, 5, 7, 0]\n",
      "Episode 188, running avg 2.0904432505827186, epsilon 0.15115293495314713\n",
      "\tFinal tour: [0, 3, 5, 4, 6, 8, 2, 7, 1, 9, 0]\n",
      "Episode 189, running avg 2.0816334736170026, epsilon 0.14964140560361566\n",
      "\tFinal tour: [0, 3, 5, 9, 8, 1, 2, 7, 4, 6, 0]\n",
      "Episode 190, loss 0.8278148867234636, running avg 2.084571375972435, epsilon 0.1481449915475795\n",
      "\tFinal tour: [0, 8, 9, 1, 7, 4, 2, 5, 3, 6, 0]\n",
      "Episode 191, running avg 2.0915207628038774, epsilon 0.1466635416321037\n",
      "\tFinal tour: [0, 1, 3, 4, 2, 8, 7, 5, 6, 9, 0]\n",
      "Episode 192, running avg 2.08991294509907, epsilon 0.14519690621578268\n",
      "\tFinal tour: [0, 9, 1, 3, 2, 7, 6, 4, 8, 5, 0]\n",
      "Episode 193, running avg 2.091815858295165, epsilon 0.14374493715362485\n",
      "\tFinal tour: [0, 4, 2, 6, 1, 3, 7, 5, 8, 9, 0]\n",
      "Episode 194, running avg 2.0915271421195194, epsilon 0.1423074877820886\n",
      "\tFinal tour: [0, 9, 1, 5, 3, 4, 2, 6, 7, 8, 0]\n",
      "Episode 195, running avg 2.092280586506368, epsilon 0.1408844129042677\n",
      "\tFinal tour: [0, 9, 8, 5, 6, 7, 3, 2, 1, 4, 0]\n",
      "Episode 196, running avg 2.0946481131078274, epsilon 0.13947556877522502\n",
      "\tFinal tour: [0, 3, 5, 4, 6, 8, 2, 7, 1, 9, 0]\n",
      "Episode 197, running avg 2.0899507645105158, epsilon 0.13808081308747278\n",
      "\tFinal tour: [0, 6, 7, 9, 8, 5, 1, 4, 3, 2, 0]\n",
      "Episode 198, running avg 2.0869516788651366, epsilon 0.13670000495659804\n",
      "\tFinal tour: [0, 1, 7, 5, 6, 3, 4, 2, 8, 9, 0]\n",
      "Episode 199, running avg 2.09101691175713, epsilon 0.13533300490703207\n",
      "\tFinal tour: [0, 4, 8, 5, 1, 2, 3, 9, 6, 7, 0]\n",
      "Episode 200, loss 0.7002925271530247, running avg 2.089004939966416, epsilon 0.13397967485796175\n",
      "\tFinal tour: [0, 7, 1, 6, 8, 3, 4, 9, 5, 2, 0]\n",
      "Episode 201, running avg 2.0829542640680985, epsilon 0.13263987810938213\n",
      "\tFinal tour: [0, 4, 6, 7, 2, 3, 9, 1, 8, 5, 0]\n",
      "Episode 202, running avg 2.088536222841322, epsilon 0.1313134793282883\n",
      "\tFinal tour: [0, 8, 6, 1, 5, 9, 7, 3, 2, 4, 0]\n",
      "Episode 203, running avg 2.0921052804723113, epsilon 0.13000034453500542\n",
      "\tFinal tour: [0, 6, 3, 4, 5, 7, 1, 9, 2, 8, 0]\n",
      "Episode 204, running avg 2.096464747509483, epsilon 0.12870034108965536\n",
      "\tFinal tour: [0, 6, 3, 2, 5, 7, 4, 1, 8, 9, 0]\n",
      "Episode 205, running avg 2.0924162930441286, epsilon 0.12741333767875881\n",
      "\tFinal tour: [0, 8, 3, 9, 7, 1, 4, 6, 5, 2, 0]\n",
      "Episode 206, running avg 2.0938272160888673, epsilon 0.12613920430197123\n",
      "\tFinal tour: [0, 5, 2, 3, 9, 6, 4, 8, 1, 7, 0]\n",
      "Episode 207, running avg 2.0856724602177867, epsilon 0.12487781225895152\n",
      "\tFinal tour: [0, 9, 8, 6, 2, 3, 5, 1, 7, 4, 0]\n",
      "Episode 208, running avg 2.0898923014152797, epsilon 0.123629034136362\n",
      "\tFinal tour: [0, 2, 3, 1, 7, 5, 9, 8, 6, 4, 0]\n",
      "Episode 209, running avg 2.0914487298901245, epsilon 0.12239274379499838\n",
      "\tFinal tour: [0, 4, 6, 3, 2, 7, 1, 8, 5, 9, 0]\n",
      "Episode 210, loss 0.36150206683543434, running avg 2.0898452227959465, epsilon 0.1211688163570484\n",
      "\tFinal tour: [0, 8, 9, 2, 4, 7, 3, 1, 6, 5, 0]\n",
      "Episode 211, running avg 2.0958721825963793, epsilon 0.11995712819347792\n",
      "\tFinal tour: [0, 3, 8, 1, 2, 7, 4, 5, 6, 9, 0]\n",
      "Episode 212, running avg 2.0937456215486865, epsilon 0.11875755691154315\n",
      "\tFinal tour: [0, 7, 4, 3, 5, 9, 8, 2, 6, 1, 0]\n",
      "Episode 213, running avg 2.0927838911234025, epsilon 0.11756998134242772\n",
      "\tFinal tour: [0, 9, 1, 7, 3, 2, 5, 8, 4, 6, 0]\n",
      "Episode 214, running avg 2.087902778070114, epsilon 0.11639428152900344\n",
      "\tFinal tour: [0, 9, 8, 6, 7, 1, 5, 2, 4, 3, 0]\n",
      "Episode 215, running avg 2.0880787915945738, epsilon 0.11523033871371341\n",
      "\tFinal tour: [0, 8, 9, 7, 6, 2, 1, 4, 3, 5, 0]\n",
      "Episode 216, running avg 2.081825952265829, epsilon 0.11407803532657627\n",
      "\tFinal tour: [0, 9, 5, 8, 1, 7, 4, 6, 2, 3, 0]\n",
      "Episode 217, running avg 2.0803685416232574, epsilon 0.11293725497331052\n",
      "\tFinal tour: [0, 4, 3, 1, 2, 9, 8, 7, 5, 6, 0]\n",
      "Episode 218, running avg 2.086021692653659, epsilon 0.1118078824235774\n",
      "\tFinal tour: [0, 3, 4, 1, 6, 2, 8, 9, 7, 5, 0]\n",
      "Episode 219, running avg 2.0957051697310867, epsilon 0.11068980359934164\n",
      "\tFinal tour: [0, 5, 2, 8, 9, 7, 1, 3, 6, 4, 0]\n",
      "Episode 220, loss 0.5092464430297193, running avg 2.0894260676904968, epsilon 0.10958290556334822\n",
      "\tFinal tour: [0, 5, 2, 7, 8, 6, 3, 4, 1, 9, 0]\n",
      "Episode 221, running avg 2.0908971174173527, epsilon 0.10848707650771475\n",
      "\tFinal tour: [0, 8, 9, 2, 4, 7, 3, 1, 6, 5, 0]\n",
      "Episode 222, running avg 2.0859706456219573, epsilon 0.1074022057426376\n",
      "\tFinal tour: [0, 9, 7, 1, 8, 6, 4, 5, 2, 3, 0]\n",
      "Episode 223, running avg 2.086346546324285, epsilon 0.10632818368521123\n",
      "\tFinal tour: [0, 7, 3, 1, 8, 5, 4, 9, 2, 6, 0]\n",
      "Episode 224, running avg 2.083606196292884, epsilon 0.10526490184835911\n",
      "\tFinal tour: [0, 8, 6, 5, 7, 1, 9, 3, 2, 4, 0]\n",
      "Episode 225, running avg 2.0780403764012476, epsilon 0.10421225282987552\n",
      "\tFinal tour: [0, 9, 5, 8, 3, 7, 6, 1, 4, 2, 0]\n",
      "Episode 226, running avg 2.077056829758396, epsilon 0.10317013030157676\n",
      "\tFinal tour: [0, 8, 7, 6, 3, 4, 9, 5, 1, 2, 0]\n",
      "Episode 227, running avg 2.079302763867781, epsilon 0.10213842899856099\n",
      "\tFinal tour: [0, 2, 9, 8, 5, 1, 4, 6, 3, 7, 0]\n",
      "Episode 228, running avg 2.0890470523806157, epsilon 0.10111704470857538\n",
      "\tFinal tour: [0, 9, 5, 1, 6, 7, 3, 8, 2, 4, 0]\n",
      "Episode 229, running avg 2.0904995360644567, epsilon 0.10010587426148963\n",
      "\tFinal tour: [0, 2, 9, 8, 5, 1, 4, 6, 3, 7, 0]\n",
      "Episode 230, loss 0.6025629469362445, running avg 2.0961942855619284, epsilon 0.09910481551887473\n",
      "\tFinal tour: [0, 8, 6, 3, 1, 5, 4, 9, 2, 7, 0]\n",
      "Episode 231, running avg 2.097694818603841, epsilon 0.09811376736368599\n",
      "\tFinal tour: [0, 7, 4, 2, 8, 9, 5, 1, 3, 6, 0]\n",
      "Episode 232, running avg 2.0999370431506, epsilon 0.09713262969004913\n",
      "\tFinal tour: [0, 9, 1, 2, 8, 4, 3, 7, 5, 6, 0]\n",
      "Episode 233, running avg 2.1006042642193146, epsilon 0.09616130339314863\n",
      "\tFinal tour: [0, 8, 6, 3, 2, 4, 1, 7, 9, 5, 0]\n",
      "Episode 234, running avg 2.100066691642205, epsilon 0.09519969035921715\n",
      "\tFinal tour: [0, 4, 2, 6, 1, 8, 3, 7, 5, 9, 0]\n",
      "Episode 235, running avg 2.103167350456838, epsilon 0.09424769345562498\n",
      "\tFinal tour: [0, 7, 4, 2, 8, 9, 5, 1, 3, 6, 0]\n",
      "Episode 236, running avg 2.105533570298562, epsilon 0.09330521652106873\n",
      "\tFinal tour: [0, 3, 4, 1, 8, 9, 2, 7, 5, 6, 0]\n",
      "Episode 237, running avg 2.1034048473164573, epsilon 0.09237216435585804\n",
      "\tFinal tour: [0, 3, 8, 1, 2, 7, 4, 9, 6, 5, 0]\n",
      "Episode 238, running avg 2.101091982856213, epsilon 0.09144844271229946\n",
      "\tFinal tour: [0, 9, 1, 3, 2, 7, 4, 8, 5, 6, 0]\n",
      "Episode 239, running avg 2.1019094227260795, epsilon 0.09053395828517646\n",
      "\tFinal tour: [0, 4, 6, 7, 2, 3, 9, 1, 8, 5, 0]\n",
      "Episode 240, loss 0.5144029817569354, running avg 2.108716764739511, epsilon 0.08962861870232469\n",
      "\tFinal tour: [0, 3, 7, 5, 1, 4, 2, 9, 6, 8, 0]\n",
      "Episode 241, running avg 2.1028630418901044, epsilon 0.08873233251530144\n",
      "\tFinal tour: [0, 7, 5, 2, 8, 9, 6, 4, 1, 3, 0]\n",
      "Episode 242, running avg 2.106277846035162, epsilon 0.08784500919014843\n",
      "\tFinal tour: [0, 5, 9, 8, 4, 6, 1, 7, 2, 3, 0]\n",
      "Episode 243, running avg 2.1009167563963267, epsilon 0.08696655909824694\n",
      "\tFinal tour: [0, 9, 7, 1, 8, 3, 2, 5, 4, 6, 0]\n",
      "Episode 244, running avg 2.1058726304777626, epsilon 0.08609689350726446\n",
      "\tFinal tour: [0, 8, 2, 3, 6, 1, 4, 7, 9, 5, 0]\n",
      "Episode 245, running avg 2.1049596314433225, epsilon 0.08523592457219181\n",
      "\tFinal tour: [0, 9, 1, 2, 5, 3, 6, 7, 4, 8, 0]\n",
      "Episode 246, running avg 2.1011941475946956, epsilon 0.0843835653264699\n",
      "\tFinal tour: [0, 9, 4, 1, 2, 8, 6, 7, 5, 3, 0]\n",
      "Episode 247, running avg 2.0943915637512074, epsilon 0.0835397296732052\n",
      "\tFinal tour: [0, 9, 1, 3, 2, 8, 6, 4, 5, 7, 0]\n",
      "Episode 248, running avg 2.0931918870185586, epsilon 0.08270433237647315\n",
      "\tFinal tour: [0, 9, 2, 1, 7, 4, 8, 3, 5, 6, 0]\n",
      "Episode 249, running avg 2.092750129552324, epsilon 0.08187728905270841\n",
      "\tFinal tour: [0, 3, 2, 4, 1, 9, 5, 8, 6, 7, 0]\n",
      "Episode 250, loss 0.4961162750929867, running avg 2.0935519367374056, epsilon 0.08105851616218133\n",
      "\tFinal tour: [0, 6, 3, 4, 5, 7, 1, 9, 2, 8, 0]\n",
      "Episode 251, running avg 2.0954403262744505, epsilon 0.08024793100055952\n",
      "\tFinal tour: [0, 9, 8, 3, 4, 2, 5, 7, 6, 1, 0]\n",
      "Episode 252, running avg 2.097046823775901, epsilon 0.07944545169055392\n",
      "\tFinal tour: [0, 8, 3, 1, 7, 6, 4, 5, 9, 2, 0]\n",
      "Episode 253, running avg 2.1012276427083205, epsilon 0.07865099717364837\n",
      "\tFinal tour: [0, 3, 5, 4, 6, 8, 2, 7, 1, 9, 0]\n",
      "Episode 254, running avg 2.0925252429917705, epsilon 0.07786448720191189\n",
      "\tFinal tour: [0, 9, 7, 3, 6, 1, 8, 4, 5, 2, 0]\n",
      "Episode 255, running avg 2.0956994933149464, epsilon 0.07708584232989277\n",
      "\tFinal tour: [0, 7, 1, 3, 8, 2, 6, 9, 5, 4, 0]\n",
      "Episode 256, running avg 2.099400209538037, epsilon 0.07631498390659384\n",
      "\tFinal tour: [0, 7, 1, 3, 8, 2, 6, 5, 9, 4, 0]\n",
      "Episode 257, running avg 2.0995073335425416, epsilon 0.07555183406752791\n",
      "\tFinal tour: [0, 9, 5, 3, 2, 8, 1, 6, 4, 7, 0]\n",
      "Episode 258, running avg 2.10142200346386, epsilon 0.07479631572685264\n",
      "\tFinal tour: [0, 8, 6, 3, 1, 5, 4, 9, 2, 7, 0]\n",
      "Episode 259, running avg 2.100925077811312, epsilon 0.07404835256958411\n",
      "\tFinal tour: [0, 9, 3, 1, 4, 5, 8, 2, 7, 6, 0]\n",
      "Episode 260, loss 0.6226987238424554, running avg 2.105893068824653, epsilon 0.07330786904388827\n",
      "\tFinal tour: [0, 2, 4, 3, 6, 9, 7, 8, 5, 1, 0]\n",
      "Episode 261, running avg 2.1102649713699044, epsilon 0.07257479035344938\n",
      "\tFinal tour: [0, 8, 9, 7, 5, 3, 2, 6, 1, 4, 0]\n",
      "Episode 262, running avg 2.1083180546430693, epsilon 0.07184904244991488\n",
      "\tFinal tour: [0, 6, 9, 5, 4, 2, 1, 7, 8, 3, 0]\n",
      "Episode 263, running avg 2.1138994636259145, epsilon 0.07113055202541574\n",
      "\tFinal tour: [0, 3, 4, 5, 9, 2, 1, 6, 7, 8, 0]\n",
      "Episode 264, running avg 2.1160041331925323, epsilon 0.07041924650516158\n",
      "\tFinal tour: [0, 6, 8, 4, 1, 9, 2, 3, 5, 7, 0]\n",
      "Episode 265, running avg 2.1200457509675474, epsilon 0.06971505404010997\n",
      "\tFinal tour: [0, 9, 7, 3, 6, 1, 8, 4, 5, 2, 0]\n",
      "Episode 266, running avg 2.1169152600445056, epsilon 0.06901790349970886\n",
      "\tFinal tour: [0, 1, 3, 4, 5, 7, 2, 8, 6, 9, 0]\n",
      "Episode 267, running avg 2.114698284242167, epsilon 0.06832772446471178\n",
      "\tFinal tour: [0, 8, 3, 9, 7, 2, 6, 4, 1, 5, 0]\n",
      "Episode 268, running avg 2.11705653961211, epsilon 0.06764444722006466\n",
      "\tFinal tour: [0, 4, 7, 1, 8, 2, 5, 6, 3, 9, 0]\n",
      "Episode 269, running avg 2.1213539770630057, epsilon 0.066968002747864\n",
      "\tFinal tour: [0, 4, 3, 1, 2, 9, 8, 7, 6, 5, 0]\n",
      "Episode 270, loss 0.6989677546872105, running avg 2.1271434523589536, epsilon 0.06629832272038537\n",
      "\tFinal tour: [0, 9, 5, 2, 1, 3, 7, 4, 8, 6, 0]\n",
      "Episode 271, running avg 2.121190386258633, epsilon 0.06563533949318151\n",
      "\tFinal tour: [0, 9, 4, 7, 8, 2, 3, 1, 5, 6, 0]\n",
      "Episode 272, running avg 2.1234774720479197, epsilon 0.06497898609824969\n",
      "\tFinal tour: [0, 6, 8, 4, 1, 9, 2, 3, 5, 7, 0]\n",
      "Episode 273, running avg 2.120254382406629, epsilon 0.0643291962372672\n",
      "\tFinal tour: [0, 4, 6, 3, 2, 7, 1, 8, 5, 9, 0]\n",
      "Episode 274, running avg 2.1193807059375107, epsilon 0.06368590427489453\n",
      "\tFinal tour: [0, 7, 2, 8, 9, 3, 6, 4, 1, 5, 0]\n",
      "Episode 275, running avg 2.1191515143510733, epsilon 0.06304904523214558\n",
      "\tFinal tour: [0, 9, 8, 6, 7, 5, 1, 4, 3, 2, 0]\n",
      "Episode 276, running avg 2.1229582926270747, epsilon 0.06241855477982412\n",
      "\tFinal tour: [0, 4, 3, 1, 2, 7, 6, 5, 8, 9, 0]\n",
      "Episode 277, running avg 2.1342552139612354, epsilon 0.06179436923202588\n",
      "\tFinal tour: [0, 4, 5, 8, 1, 3, 2, 6, 9, 7, 0]\n",
      "Episode 278, running avg 2.1392253653713142, epsilon 0.06117642553970562\n",
      "\tFinal tour: [0, 9, 4, 3, 5, 1, 8, 6, 2, 7, 0]\n",
      "Episode 279, running avg 2.1385874641788845, epsilon 0.06056466128430856\n",
      "\tFinal tour: [0, 8, 3, 1, 7, 6, 4, 5, 9, 2, 0]\n",
      "Episode 280, loss 0.6712738584995386, running avg 2.1430992263166697, epsilon 0.05995901467146548\n",
      "\tFinal tour: [0, 5, 7, 8, 2, 4, 1, 9, 3, 6, 0]\n",
      "Episode 281, running avg 2.1391781314251928, epsilon 0.05935942452475082\n",
      "\tFinal tour: [0, 9, 7, 4, 2, 5, 6, 1, 8, 3, 0]\n",
      "Episode 282, running avg 2.1406058045162357, epsilon 0.058765830279503314\n",
      "\tFinal tour: [0, 8, 1, 5, 3, 2, 4, 7, 9, 6, 0]\n",
      "Episode 283, running avg 2.1390900597867324, epsilon 0.05817817197670828\n",
      "\tFinal tour: [0, 9, 1, 3, 2, 7, 4, 8, 5, 6, 0]\n",
      "Episode 284, running avg 2.142984497880147, epsilon 0.057596390256941195\n",
      "\tFinal tour: [0, 9, 8, 2, 5, 3, 7, 6, 4, 1, 0]\n",
      "Episode 285, running avg 2.13980764335774, epsilon 0.05702042635437178\n",
      "\tFinal tour: [0, 3, 2, 6, 4, 1, 9, 7, 5, 8, 0]\n",
      "Episode 286, running avg 2.14076686989228, epsilon 0.05645022209082806\n",
      "\tFinal tour: [0, 5, 1, 4, 7, 2, 3, 6, 8, 9, 0]\n",
      "Episode 287, running avg 2.1427665505307845, epsilon 0.05588571986991978\n",
      "\tFinal tour: [0, 5, 3, 2, 7, 9, 8, 1, 4, 6, 0]\n",
      "Episode 288, running avg 2.136435567565079, epsilon 0.055326862671220584\n",
      "\tFinal tour: [0, 1, 7, 8, 9, 5, 2, 4, 3, 6, 0]\n",
      "Episode 289, running avg 2.135403440559359, epsilon 0.05477359404450838\n",
      "\tFinal tour: [0, 6, 8, 4, 1, 5, 2, 7, 9, 3, 0]\n",
      "Episode 290, loss 0.17781979681084964, running avg 2.1347828860924105, epsilon 0.054225858104063294\n",
      "\tFinal tour: [0, 9, 4, 7, 8, 2, 3, 1, 5, 6, 0]\n",
      "Episode 291, running avg 2.1274479484210347, epsilon 0.05368359952302266\n",
      "\tFinal tour: [0, 5, 3, 4, 1, 9, 8, 6, 7, 2, 0]\n",
      "Episode 292, running avg 2.1295451664294434, epsilon 0.053146763527792434\n",
      "\tFinal tour: [0, 7, 3, 1, 8, 5, 4, 9, 2, 6, 0]\n",
      "Episode 293, running avg 2.1252806292024844, epsilon 0.052615295892514506\n",
      "\tFinal tour: [0, 8, 5, 4, 6, 3, 9, 7, 1, 2, 0]\n",
      "Episode 294, running avg 2.1216458710206942, epsilon 0.052089142933589364\n",
      "\tFinal tour: [0, 8, 6, 1, 5, 9, 3, 2, 4, 7, 0]\n",
      "Episode 295, running avg 2.122009772603417, epsilon 0.05156825150425347\n",
      "\tFinal tour: [0, 3, 6, 7, 9, 5, 4, 1, 8, 2, 0]\n",
      "Episode 296, running avg 2.1205082187876063, epsilon 0.051052568989210935\n",
      "\tFinal tour: [0, 9, 5, 2, 1, 3, 7, 4, 8, 6, 0]\n",
      "Episode 297, running avg 2.118382284472029, epsilon 0.05054204329931883\n",
      "\tFinal tour: [0, 9, 7, 1, 8, 3, 2, 5, 4, 6, 0]\n",
      "Episode 298, running avg 2.1225774753935513, epsilon 0.05003662286632564\n",
      "\tFinal tour: [0, 3, 5, 4, 6, 8, 2, 7, 1, 9, 0]\n",
      "Episode 299, running avg 2.120107811274193, epsilon 0.04953625663766238\n",
      "\tFinal tour: [0, 4, 6, 3, 2, 7, 1, 8, 5, 9, 0]\n",
      "Episode 300, loss 0.3728185802840176, running avg 2.117102672933042, epsilon 0.04904089407128576\n",
      "\tFinal tour: [0, 9, 8, 4, 2, 7, 6, 1, 5, 3, 0]\n",
      "Episode 301, running avg 2.122361180376848, epsilon 0.0485504851305729\n",
      "\tFinal tour: [0, 9, 3, 1, 4, 5, 6, 7, 2, 8, 0]\n",
      "Episode 302, running avg 2.123425916928074, epsilon 0.048064980279267165\n",
      "\tFinal tour: [0, 6, 3, 4, 9, 8, 5, 7, 1, 2, 0]\n",
      "Episode 303, running avg 2.12056349073965, epsilon 0.04758433047647449\n",
      "\tFinal tour: [0, 8, 7, 6, 3, 9, 2, 5, 1, 4, 0]\n",
      "Episode 304, running avg 2.1160460958225644, epsilon 0.04710848717170975\n",
      "\tFinal tour: [0, 3, 6, 7, 9, 5, 1, 8, 2, 4, 0]\n",
      "Episode 305, running avg 2.112997388562931, epsilon 0.04663740229999265\n",
      "\tFinal tour: [0, 8, 6, 1, 5, 9, 3, 2, 4, 7, 0]\n",
      "Episode 306, running avg 2.113560644549111, epsilon 0.04617102827699272\n",
      "\tFinal tour: [0, 1, 3, 4, 2, 8, 7, 5, 6, 9, 0]\n",
      "Episode 307, running avg 2.1142539238635756, epsilon 0.045709317994222794\n",
      "\tFinal tour: [0, 3, 8, 5, 1, 2, 7, 4, 6, 9, 0]\n",
      "Episode 308, running avg 2.110351103565312, epsilon 0.04525222481428057\n",
      "\tFinal tour: [0, 9, 8, 7, 6, 3, 4, 1, 5, 2, 0]\n",
      "Episode 309, running avg 2.113403851904558, epsilon 0.04479970256613776\n",
      "\tFinal tour: [0, 9, 8, 4, 2, 7, 6, 1, 5, 3, 0]\n",
      "Episode 310, loss 0.45774924289516816, running avg 2.1220065726049704, epsilon 0.04435170554047638\n",
      "\tFinal tour: [0, 7, 4, 6, 5, 2, 9, 1, 3, 8, 0]\n",
      "Episode 311, running avg 2.1200800079338644, epsilon 0.043908188485071616\n",
      "\tFinal tour: [0, 6, 3, 4, 5, 7, 1, 9, 2, 8, 0]\n",
      "Episode 312, running avg 2.123680755261057, epsilon 0.0434691066002209\n",
      "\tFinal tour: [0, 6, 1, 4, 3, 7, 5, 8, 2, 9, 0]\n",
      "Episode 313, running avg 2.120607226041477, epsilon 0.04303441553421869\n",
      "\tFinal tour: [0, 8, 9, 7, 5, 3, 2, 6, 1, 4, 0]\n",
      "Episode 314, running avg 2.118499398910995, epsilon 0.0426040713788765\n",
      "\tFinal tour: [0, 8, 2, 3, 6, 4, 1, 9, 5, 7, 0]\n",
      "Episode 315, running avg 2.120846871878353, epsilon 0.04217803066508773\n",
      "\tFinal tour: [0, 9, 1, 5, 3, 4, 2, 6, 7, 8, 0]\n",
      "Episode 316, running avg 2.1281522352773288, epsilon 0.04175625035843686\n",
      "\tFinal tour: [0, 9, 8, 6, 7, 1, 2, 4, 3, 5, 0]\n",
      "Episode 317, running avg 2.1336586931631194, epsilon 0.041338687854852486\n",
      "\tFinal tour: [0, 8, 9, 7, 5, 3, 2, 6, 1, 4, 0]\n",
      "Episode 318, running avg 2.128202985759653, epsilon 0.04092530097630396\n",
      "\tFinal tour: [0, 9, 1, 2, 5, 3, 6, 7, 4, 8, 0]\n",
      "Episode 319, running avg 2.120644687681426, epsilon 0.040516047966540916\n",
      "\tFinal tour: [0, 5, 2, 3, 9, 6, 4, 8, 1, 7, 0]\n",
      "Episode 320, loss 0.4901839595483784, running avg 2.119593028371452, epsilon 0.04011088748687551\n",
      "\tFinal tour: [0, 9, 8, 2, 6, 4, 3, 7, 5, 1, 0]\n",
      "Episode 321, running avg 2.117512761818754, epsilon 0.03970977861200675\n",
      "\tFinal tour: [0, 9, 5, 8, 3, 7, 6, 1, 4, 2, 0]\n",
      "Episode 322, running avg 2.116204053613342, epsilon 0.03931268082588668\n",
      "\tFinal tour: [0, 9, 8, 7, 6, 3, 4, 1, 5, 2, 0]\n",
      "Episode 323, running avg 2.1165516817059067, epsilon 0.03891955401762781\n",
      "\tFinal tour: [0, 9, 7, 1, 8, 3, 2, 5, 4, 6, 0]\n",
      "Episode 324, running avg 2.1210654127500153, epsilon 0.03853035847745153\n",
      "\tFinal tour: [0, 3, 6, 7, 9, 5, 1, 8, 2, 4, 0]\n",
      "Episode 325, running avg 2.124208440778287, epsilon 0.03814505489267701\n",
      "\tFinal tour: [0, 4, 3, 5, 7, 9, 8, 1, 6, 2, 0]\n",
      "Episode 326, running avg 2.126571337915751, epsilon 0.03776360434375024\n",
      "\tFinal tour: [0, 1, 7, 8, 5, 2, 4, 3, 6, 9, 0]\n",
      "Episode 327, running avg 2.1196988557884637, epsilon 0.03738596830031274\n",
      "\tFinal tour: [0, 8, 9, 2, 4, 7, 5, 1, 6, 3, 0]\n",
      "Episode 328, running avg 2.1172962434751774, epsilon 0.03701210861730961\n",
      "\tFinal tour: [0, 9, 5, 3, 2, 7, 4, 6, 8, 1, 0]\n",
      "Episode 329, running avg 2.1195346197835736, epsilon 0.03664198753113651\n",
      "\tFinal tour: [0, 8, 9, 2, 4, 7, 3, 1, 6, 5, 0]\n",
      "Episode 330, loss 0.21703347200516795, running avg 2.1149751945179642, epsilon 0.036275567655825146\n",
      "\tFinal tour: [0, 4, 7, 1, 6, 5, 8, 2, 3, 9, 0]\n",
      "Episode 331, running avg 2.119468477592399, epsilon 0.03591281197926689\n",
      "\tFinal tour: [0, 4, 7, 1, 8, 2, 5, 6, 3, 9, 0]\n",
      "Episode 332, running avg 2.124341724973392, epsilon 0.035553683859474224\n",
      "\tFinal tour: [0, 9, 3, 1, 4, 5, 8, 2, 7, 6, 0]\n",
      "Episode 333, running avg 2.127790574535565, epsilon 0.03519814702087948\n",
      "\tFinal tour: [0, 6, 9, 2, 4, 5, 7, 8, 3, 1, 0]\n",
      "Episode 334, running avg 2.1262185858042066, epsilon 0.03484616555067068\n",
      "\tFinal tour: [0, 2, 3, 1, 7, 6, 8, 9, 5, 4, 0]\n",
      "Episode 335, running avg 2.123492284990835, epsilon 0.034497703895163975\n",
      "\tFinal tour: [0, 9, 4, 1, 3, 2, 6, 5, 8, 7, 0]\n",
      "Episode 336, running avg 2.1185899493148135, epsilon 0.03415272685621234\n",
      "\tFinal tour: [0, 8, 3, 9, 7, 1, 4, 6, 5, 2, 0]\n",
      "Episode 337, running avg 2.1196321479393596, epsilon 0.03381119958765021\n",
      "\tFinal tour: [0, 9, 4, 7, 8, 2, 3, 1, 5, 6, 0]\n",
      "Episode 338, running avg 2.120734486218056, epsilon 0.03347308759177371\n",
      "\tFinal tour: [0, 8, 3, 1, 7, 2, 9, 6, 4, 5, 0]\n",
      "Episode 339, running avg 2.1201837455260035, epsilon 0.03313835671585597\n",
      "\tFinal tour: [0, 9, 8, 6, 3, 4, 1, 5, 2, 7, 0]\n",
      "Episode 340, loss 0.7180229287878765, running avg 2.1160704967236983, epsilon 0.03280697314869741\n",
      "\tFinal tour: [0, 1, 6, 8, 2, 5, 3, 4, 7, 9, 0]\n",
      "Episode 341, running avg 2.114814476363805, epsilon 0.032478903417210436\n",
      "\tFinal tour: [0, 3, 7, 4, 9, 8, 6, 5, 1, 2, 0]\n",
      "Episode 342, running avg 2.1176283697289113, epsilon 0.032154114383038335\n",
      "\tFinal tour: [0, 7, 4, 2, 8, 9, 5, 1, 3, 6, 0]\n",
      "Episode 343, running avg 2.121761154548945, epsilon 0.03183257323920795\n",
      "\tFinal tour: [0, 7, 4, 6, 5, 2, 9, 1, 3, 8, 0]\n",
      "Episode 344, running avg 2.1160206244539226, epsilon 0.03151424750681587\n",
      "\tFinal tour: [0, 6, 4, 3, 2, 1, 5, 9, 7, 8, 0]\n",
      "Episode 345, running avg 2.123039647422288, epsilon 0.03119910503174771\n",
      "\tFinal tour: [0, 9, 4, 3, 1, 5, 2, 8, 6, 7, 0]\n",
      "Episode 346, running avg 2.1214817012979723, epsilon 0.030887113981430233\n",
      "\tFinal tour: [0, 8, 3, 1, 7, 6, 4, 5, 9, 2, 0]\n",
      "Episode 347, running avg 2.125621038028626, epsilon 0.03057824284161593\n",
      "\tFinal tour: [0, 9, 1, 3, 2, 7, 4, 8, 5, 6, 0]\n",
      "Episode 348, running avg 2.1297671859163194, epsilon 0.03027246041319977\n",
      "\tFinal tour: [0, 6, 8, 4, 1, 9, 2, 3, 5, 7, 0]\n",
      "Episode 349, running avg 2.1317365468745986, epsilon 0.029969735809067772\n",
      "\tFinal tour: [0, 4, 7, 1, 6, 5, 8, 2, 3, 9, 0]\n",
      "Episode 350, loss 0.9016218079440936, running avg 2.131029762318801, epsilon 0.029670038450977095\n",
      "\tFinal tour: [0, 8, 9, 2, 4, 7, 5, 1, 6, 3, 0]\n",
      "Episode 351, running avg 2.1283199652846423, epsilon 0.029373338066467324\n",
      "\tFinal tour: [0, 7, 1, 8, 6, 9, 2, 4, 5, 3, 0]\n",
      "Episode 352, running avg 2.126201826653455, epsilon 0.02907960468580265\n",
      "\tFinal tour: [0, 5, 9, 6, 4, 3, 1, 8, 2, 7, 0]\n",
      "Episode 353, running avg 2.1218777496862535, epsilon 0.028788808638944622\n",
      "\tFinal tour: [0, 6, 8, 4, 1, 5, 2, 7, 9, 3, 0]\n",
      "Episode 354, running avg 2.124960560209213, epsilon 0.028500920552555174\n",
      "\tFinal tour: [0, 6, 8, 4, 1, 9, 2, 3, 5, 7, 0]\n",
      "Episode 355, running avg 2.125128899614993, epsilon 0.028215911347029624\n",
      "\tFinal tour: [0, 6, 9, 5, 4, 2, 1, 7, 8, 3, 0]\n",
      "Episode 356, running avg 2.1316177858552727, epsilon 0.027933752233559327\n",
      "\tFinal tour: [0, 5, 1, 9, 3, 2, 8, 6, 4, 7, 0]\n",
      "Episode 357, running avg 2.1330165760139126, epsilon 0.027654414711223735\n",
      "\tFinal tour: [0, 4, 2, 6, 1, 8, 3, 7, 5, 9, 0]\n",
      "Episode 358, running avg 2.1319153736506227, epsilon 0.027377870564111496\n",
      "\tFinal tour: [0, 9, 1, 3, 2, 7, 4, 8, 5, 6, 0]\n",
      "Episode 359, running avg 2.135345293442565, epsilon 0.027104091858470382\n",
      "\tFinal tour: [0, 2, 9, 8, 5, 1, 4, 6, 3, 7, 0]\n",
      "Episode 360, loss 0.5600559536456177, running avg 2.135954561015488, epsilon 0.026833050939885677\n",
      "\tFinal tour: [0, 3, 6, 7, 9, 5, 1, 8, 2, 4, 0]\n",
      "Episode 361, running avg 2.1304999552624224, epsilon 0.02656472043048682\n",
      "\tFinal tour: [0, 3, 2, 4, 1, 9, 7, 6, 8, 5, 0]\n",
      "Episode 362, running avg 2.1362743308434045, epsilon 0.02629907322618195\n",
      "\tFinal tour: [0, 3, 4, 9, 8, 1, 7, 2, 6, 5, 0]\n",
      "Episode 363, running avg 2.1327770845951886, epsilon 0.026036082493920133\n",
      "\tFinal tour: [0, 5, 2, 4, 1, 9, 8, 6, 3, 7, 0]\n",
      "Episode 364, running avg 2.1311577408434625, epsilon 0.02577572166898093\n",
      "\tFinal tour: [0, 7, 4, 3, 6, 2, 9, 5, 1, 8, 0]\n",
      "Episode 365, running avg 2.135023069033408, epsilon 0.025517964452291122\n",
      "\tFinal tour: [0, 8, 3, 1, 7, 6, 4, 5, 9, 2, 0]\n",
      "Episode 366, running avg 2.1362924833139676, epsilon 0.02526278480776821\n",
      "\tFinal tour: [0, 6, 4, 9, 5, 8, 1, 3, 2, 7, 0]\n",
      "Episode 367, running avg 2.1444835860099567, epsilon 0.025010156959690527\n",
      "\tFinal tour: [0, 7, 4, 6, 5, 2, 9, 1, 3, 8, 0]\n",
      "Episode 368, running avg 2.1402674806826285, epsilon 0.02476005539009362\n",
      "\tFinal tour: [0, 9, 1, 2, 8, 4, 3, 7, 5, 6, 0]\n",
      "Episode 369, running avg 2.134469292866017, epsilon 0.024512454836192684\n",
      "\tFinal tour: [0, 9, 3, 6, 8, 5, 1, 7, 2, 4, 0]\n",
      "Episode 370, loss 0.515359219690293, running avg 2.130519326617085, epsilon 0.024267330287830756\n",
      "\tFinal tour: [0, 7, 4, 2, 8, 9, 5, 1, 3, 6, 0]\n",
      "Episode 371, running avg 2.1364744132682194, epsilon 0.024024656984952448\n",
      "\tFinal tour: [0, 8, 9, 1, 5, 2, 4, 3, 7, 6, 0]\n",
      "Episode 372, running avg 2.1374465491484265, epsilon 0.023784410415102923\n",
      "\tFinal tour: [0, 7, 8, 9, 2, 1, 5, 3, 6, 4, 0]\n",
      "Episode 373, running avg 2.1406129387986526, epsilon 0.023546566310951894\n",
      "\tFinal tour: [0, 9, 1, 2, 8, 4, 3, 7, 5, 6, 0]\n",
      "Episode 374, running avg 2.144420946406448, epsilon 0.023311100647842375\n",
      "\tFinal tour: [0, 9, 8, 4, 2, 7, 6, 1, 5, 3, 0]\n",
      "Episode 375, running avg 2.1476910781914653, epsilon 0.02307798964136395\n",
      "\tFinal tour: [0, 3, 4, 5, 9, 2, 1, 6, 7, 8, 0]\n",
      "Episode 376, running avg 2.1441741519268924, epsilon 0.022847209744950314\n",
      "\tFinal tour: [0, 8, 9, 1, 5, 2, 4, 3, 7, 6, 0]\n",
      "Episode 377, running avg 2.136328708072041, epsilon 0.02261873764750081\n",
      "\tFinal tour: [0, 6, 1, 4, 3, 7, 5, 8, 2, 9, 0]\n",
      "Episode 378, running avg 2.128381527399838, epsilon 0.022392550271025803\n",
      "\tFinal tour: [0, 9, 7, 4, 2, 5, 6, 1, 8, 3, 0]\n",
      "Episode 379, running avg 2.128122742426707, epsilon 0.022168624768315544\n",
      "\tFinal tour: [0, 2, 9, 8, 5, 1, 4, 6, 3, 7, 0]\n",
      "Episode 380, loss 0.835316187841098, running avg 2.130904252529323, epsilon 0.02194693852063239\n",
      "\tFinal tour: [0, 9, 4, 7, 8, 2, 3, 1, 5, 6, 0]\n",
      "Episode 381, running avg 2.1320940844323695, epsilon 0.021727469135426065\n",
      "\tFinal tour: [0, 7, 1, 6, 2, 9, 4, 3, 5, 8, 0]\n",
      "Episode 382, running avg 2.1315423874773494, epsilon 0.021510194444071803\n",
      "\tFinal tour: [0, 4, 6, 7, 2, 3, 9, 1, 8, 5, 0]\n",
      "Episode 383, running avg 2.1358782085239607, epsilon 0.021295092499631085\n",
      "\tFinal tour: [0, 9, 8, 7, 3, 4, 1, 2, 5, 6, 0]\n",
      "Episode 384, running avg 2.130622512686559, epsilon 0.021082141574634772\n",
      "\tFinal tour: [0, 3, 7, 4, 9, 8, 6, 5, 1, 2, 0]\n",
      "Episode 385, running avg 2.135575989919327, epsilon 0.020871320158888425\n",
      "\tFinal tour: [0, 9, 5, 8, 1, 7, 4, 6, 3, 2, 0]\n",
      "Episode 386, running avg 2.1361955671143225, epsilon 0.020662606957299542\n",
      "\tFinal tour: [0, 5, 2, 3, 9, 6, 4, 8, 1, 7, 0]\n",
      "Episode 387, running avg 2.1314329792280056, epsilon 0.020455980887726547\n",
      "\tFinal tour: [0, 4, 3, 8, 9, 2, 6, 5, 7, 1, 0]\n",
      "Episode 388, running avg 2.133809112239676, epsilon 0.02025142107884928\n",
      "\tFinal tour: [0, 4, 6, 3, 8, 9, 7, 2, 1, 5, 0]\n",
      "Episode 389, running avg 2.1360942455917393, epsilon 0.020048906868060788\n",
      "\tFinal tour: [0, 3, 2, 6, 1, 4, 8, 7, 5, 9, 0]\n",
      "Episode 390, loss 0.5335693056332429, running avg 2.1347752897361976, epsilon 0.01984841779938018\n",
      "\tFinal tour: [0, 6, 9, 5, 4, 2, 1, 7, 8, 3, 0]\n",
      "Episode 391, running avg 2.1408566328220355, epsilon 0.019649933621386378\n",
      "\tFinal tour: [0, 3, 6, 7, 9, 5, 1, 8, 2, 4, 0]\n",
      "Episode 392, running avg 2.1403014924233354, epsilon 0.019453434285172513\n",
      "\tFinal tour: [0, 6, 1, 4, 3, 7, 5, 8, 2, 9, 0]\n",
      "Episode 393, running avg 2.1413894418986525, epsilon 0.019258899942320787\n",
      "\tFinal tour: [0, 6, 4, 9, 5, 8, 1, 3, 2, 7, 0]\n",
      "Episode 394, running avg 2.1466555806063026, epsilon 0.01906631094289758\n",
      "\tFinal tour: [0, 9, 7, 5, 2, 3, 1, 4, 6, 8, 0]\n",
      "Episode 395, running avg 2.151532280976075, epsilon 0.018875647833468602\n",
      "\tFinal tour: [0, 3, 4, 1, 8, 9, 2, 7, 5, 6, 0]\n",
      "Episode 396, running avg 2.1527829703063337, epsilon 0.018686891355133916\n",
      "\tFinal tour: [0, 5, 2, 3, 9, 6, 4, 8, 1, 7, 0]\n",
      "Episode 397, running avg 2.1532522019790075, epsilon 0.018500022441582577\n",
      "\tFinal tour: [0, 3, 8, 1, 2, 7, 9, 5, 6, 4, 0]\n",
      "Episode 398, running avg 2.1494943413469496, epsilon 0.01831502221716675\n",
      "\tFinal tour: [0, 8, 2, 3, 6, 1, 4, 7, 9, 5, 0]\n",
      "Episode 399, running avg 2.1498793603228328, epsilon 0.018131871994995084\n",
      "\tFinal tour: [0, 6, 9, 5, 4, 2, 1, 7, 8, 3, 0]\n",
      "Episode 400, loss 1.2009613437514268, running avg 2.1597302391870836, epsilon 0.017950553275045134\n",
      "\tFinal tour: [0, 7, 1, 8, 6, 9, 2, 4, 5, 3, 0]\n",
      "Episode 401, running avg 2.1543454630933687, epsilon 0.017771047742294682\n",
      "\tFinal tour: [0, 5, 9, 3, 1, 4, 8, 6, 7, 2, 0]\n",
      "Episode 402, running avg 2.150564709905162, epsilon 0.017593337264871736\n",
      "\tFinal tour: [0, 3, 1, 9, 7, 6, 8, 4, 2, 5, 0]\n",
      "Episode 403, running avg 2.153536405154186, epsilon 0.01741740389222302\n",
      "\tFinal tour: [0, 6, 3, 2, 5, 7, 4, 1, 8, 9, 0]\n",
      "Episode 404, running avg 2.154411647856268, epsilon 0.01724322985330079\n",
      "\tFinal tour: [0, 6, 9, 2, 4, 5, 7, 8, 3, 1, 0]\n",
      "Episode 405, running avg 2.155200726789955, epsilon 0.017070797554767782\n",
      "\tFinal tour: [0, 5, 1, 9, 8, 2, 3, 6, 4, 7, 0]\n",
      "Episode 406, running avg 2.154320654286843, epsilon 0.016900089579220106\n",
      "\tFinal tour: [0, 9, 1, 5, 3, 4, 2, 6, 7, 8, 0]\n",
      "Episode 407, running avg 2.160341534769641, epsilon 0.016731088683427906\n",
      "\tFinal tour: [0, 6, 1, 4, 3, 7, 5, 8, 2, 9, 0]\n",
      "Episode 408, running avg 2.159400525095424, epsilon 0.016563777796593626\n",
      "\tFinal tour: [0, 9, 8, 2, 5, 3, 7, 6, 4, 1, 0]\n",
      "Episode 409, running avg 2.155442030155862, epsilon 0.016398140018627688\n",
      "\tFinal tour: [0, 5, 2, 4, 1, 9, 8, 6, 7, 3, 0]\n",
      "Episode 410, loss 0.27176534110250694, running avg 2.150438790452583, epsilon 0.01623415861844141\n",
      "\tFinal tour: [0, 9, 5, 3, 2, 7, 4, 6, 8, 1, 0]\n",
      "Episode 411, running avg 2.1547917403505155, epsilon 0.016071817032256998\n",
      "\tFinal tour: [0, 9, 7, 3, 6, 1, 8, 4, 5, 2, 0]\n",
      "Episode 412, running avg 2.150433631246469, epsilon 0.01591109886193443\n",
      "\tFinal tour: [0, 8, 2, 3, 6, 1, 4, 7, 9, 5, 0]\n",
      "Episode 413, running avg 2.148027562597802, epsilon 0.015751987873315085\n",
      "\tFinal tour: [0, 8, 6, 1, 5, 9, 3, 2, 4, 7, 0]\n",
      "Episode 414, running avg 2.152342716729974, epsilon 0.015594467994581935\n",
      "\tFinal tour: [0, 5, 6, 4, 9, 2, 1, 7, 3, 8, 0]\n",
      "Episode 415, running avg 2.156216831067054, epsilon 0.015438523314636115\n",
      "\tFinal tour: [0, 9, 8, 1, 5, 6, 7, 3, 2, 4, 0]\n",
      "Episode 416, running avg 2.1577334847732335, epsilon 0.015284138081489753\n",
      "\tFinal tour: [0, 9, 8, 2, 5, 3, 7, 6, 4, 1, 0]\n",
      "Episode 417, running avg 2.1514845461168575, epsilon 0.015131296700674856\n",
      "\tFinal tour: [0, 7, 4, 2, 8, 9, 5, 1, 3, 6, 0]\n",
      "Episode 418, running avg 2.1542074938475064, epsilon 0.014979983733668108\n",
      "\tFinal tour: [0, 8, 9, 7, 5, 3, 2, 6, 1, 4, 0]\n",
      "Episode 419, running avg 2.153618642181205, epsilon 0.014830183896331426\n",
      "\tFinal tour: [0, 8, 7, 6, 3, 9, 2, 5, 1, 4, 0]\n",
      "Episode 420, loss 0.48427448287114494, running avg 2.15802746366293, epsilon 0.014681882057368112\n",
      "\tFinal tour: [0, 8, 6, 3, 1, 5, 4, 9, 2, 7, 0]\n",
      "Episode 421, running avg 2.1590233169720423, epsilon 0.01453506323679443\n",
      "\tFinal tour: [0, 9, 7, 5, 2, 3, 1, 4, 6, 8, 0]\n",
      "Episode 422, running avg 2.168227180514881, epsilon 0.014389712604426485\n",
      "\tFinal tour: [0, 6, 4, 3, 2, 1, 5, 9, 7, 8, 0]\n",
      "Episode 423, running avg 2.1704060938613408, epsilon 0.01424581547838222\n",
      "\tFinal tour: [0, 8, 2, 3, 6, 1, 4, 7, 9, 5, 0]\n",
      "Episode 424, running avg 2.164574243643882, epsilon 0.014103357323598397\n",
      "\tFinal tour: [0, 5, 1, 9, 3, 2, 8, 6, 4, 7, 0]\n",
      "Episode 425, running avg 2.170773911630871, epsilon 0.013962323750362413\n",
      "\tFinal tour: [0, 3, 8, 1, 2, 7, 4, 5, 9, 6, 0]\n",
      "Episode 426, running avg 2.1719700679627123, epsilon 0.013822700512858789\n",
      "\tFinal tour: [0, 6, 4, 3, 2, 1, 5, 9, 7, 8, 0]\n",
      "Episode 427, running avg 2.17969038354624, epsilon 0.0136844735077302\n",
      "\tFinal tour: [0, 6, 2, 4, 8, 5, 7, 3, 9, 1, 0]\n",
      "Episode 428, running avg 2.1805539885142666, epsilon 0.013547628772652899\n",
      "\tFinal tour: [0, 8, 9, 2, 4, 7, 5, 1, 6, 3, 0]\n",
      "Episode 429, running avg 2.1802844162397625, epsilon 0.01341215248492637\n",
      "\tFinal tour: [0, 3, 4, 9, 8, 1, 7, 2, 6, 5, 0]\n",
      "Episode 430, loss 0.7312817916426072, running avg 2.1833313493785145, epsilon 0.013278030960077106\n",
      "\tFinal tour: [0, 8, 9, 2, 4, 7, 3, 1, 6, 5, 0]\n",
      "Episode 431, running avg 2.179254754591046, epsilon 0.013145250650476335\n",
      "\tFinal tour: [0, 4, 6, 7, 2, 3, 9, 1, 8, 5, 0]\n",
      "Episode 432, running avg 2.1769997927325053, epsilon 0.01301379814397157\n",
      "\tFinal tour: [0, 7, 1, 6, 8, 4, 5, 9, 2, 3, 0]\n",
      "Episode 433, running avg 2.173929838125915, epsilon 0.012883660162531854\n",
      "\tFinal tour: [0, 9, 1, 3, 2, 7, 4, 8, 5, 6, 0]\n",
      "Episode 434, running avg 2.1779782401853613, epsilon 0.012754823560906535\n",
      "\tFinal tour: [0, 3, 5, 4, 6, 8, 2, 7, 1, 9, 0]\n",
      "Episode 435, running avg 2.1758086348392767, epsilon 0.01262727532529747\n",
      "\tFinal tour: [0, 9, 3, 1, 4, 5, 8, 2, 7, 6, 0]\n",
      "Episode 436, running avg 2.183234879641854, epsilon 0.012501002572044496\n",
      "\tFinal tour: [0, 5, 2, 3, 9, 6, 4, 8, 1, 7, 0]\n",
      "Episode 437, running avg 2.1767200742480486, epsilon 0.01237599254632405\n",
      "\tFinal tour: [0, 9, 1, 2, 8, 4, 3, 7, 5, 6, 0]\n",
      "Episode 438, running avg 2.1767585460774312, epsilon 0.01225223262086081\n",
      "\tFinal tour: [0, 3, 2, 4, 1, 9, 7, 6, 8, 5, 0]\n",
      "Episode 439, running avg 2.178773731680972, epsilon 0.012129710294652202\n",
      "\tFinal tour: [0, 3, 4, 5, 9, 8, 2, 6, 7, 1, 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 376\u001B[0m\n\u001B[0;32m    370\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(hyperparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrepetitions\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m)):\n\u001B[0;32m    371\u001B[0m     tsp_multi \u001B[38;5;241m=\u001B[39m QLearningTsp(\n\u001B[0;32m    372\u001B[0m         hyperparams\u001B[38;5;241m=\u001B[39mhyperparams,\n\u001B[0;32m    373\u001B[0m         path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/save_path/\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    374\u001B[0m     )\n\u001B[1;32m--> 376\u001B[0m     \u001B[43mtsp_multi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperform_episodes\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    377\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_instances\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhyperparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnum_instances\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    378\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[48], line 336\u001B[0m, in \u001B[0;36mQLearningTsp.perform_episodes\u001B[1;34m(self, num_instances)\u001B[0m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size:\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m episode \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_after \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 336\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    337\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    338\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpisode \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepisode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, running avg \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrunning_avg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, epsilon \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepsilon\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    339\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mFinal tour: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtour\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[48], line 201\u001B[0m, in \u001B[0;36mQLearningTsp.train_step\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# IMPLICIT TAPE-BASED AD:\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# The expectation values estimated by Q for the given actions.\u001B[39;00m\n\u001B[0;32m    200\u001B[0m exp_val_masks \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(get_masks_for_actions(edge_weights, partial_tours))\n\u001B[1;32m--> 201\u001B[0m exp_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreadout_op\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    202\u001B[0m q_values_masked \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(exp_values \u001B[38;5;241m*\u001B[39m exp_val_masks, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    204\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fun(target_q_values, q_values_masked)\n",
      "File \u001B[1;32m~\\Documents\\repositories\\QML-QGNN\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\repositories\\QML-QGNN\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[46], line 45\u001B[0m, in \u001B[0;36mQModel.forward\u001B[1;34m(self, input_data, observables)\u001B[0m\n\u001B[0;32m     39\u001B[0m qnn \u001B[38;5;241m=\u001B[39m EstimatorQNN(\n\u001B[0;32m     40\u001B[0m     circuit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcircuit,\n\u001B[0;32m     41\u001B[0m     input_params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcircuit\u001B[38;5;241m.\u001B[39mparameters,\n\u001B[0;32m     42\u001B[0m     observables\u001B[38;5;241m=\u001B[39mobservables\n\u001B[0;32m     43\u001B[0m )\n\u001B[0;32m     44\u001B[0m qnn \u001B[38;5;241m=\u001B[39m TorchConnector(qnn)\n\u001B[1;32m---> 45\u001B[0m expectation_values \u001B[38;5;241m=\u001B[39m \u001B[43mqnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoding_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m expectation_values\n",
      "File \u001B[1;32m~\\Documents\\repositories\\QML-QGNN\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\repositories\\QML-QGNN\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\repositories\\QML-QGNN\\venv\\Lib\\site-packages\\qiskit_machine_learning\\connectors\\torch_connector.py:333\u001B[0m, in \u001B[0;36mTorchConnector.forward\u001B[1;34m(self, input_data)\u001B[0m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Forward pass.\u001B[39;00m\n\u001B[0;32m    325\u001B[0m \n\u001B[0;32m    326\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;124;03m    Result of forward pass of this model.\u001B[39;00m\n\u001B[0;32m    331\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    332\u001B[0m input_ \u001B[38;5;241m=\u001B[39m input_data \u001B[38;5;28;01mif\u001B[39;00m input_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m--> 333\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mTorchConnector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_TorchNNFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_neural_network\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sparse\u001B[49m\n\u001B[0;32m    335\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\repositories\\QML-QGNN\\venv\\Lib\\site-packages\\torch\\autograd\\function.py:539\u001B[0m, in \u001B[0;36mFunction.apply\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[0;32m    537\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[0;32m    538\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[1;32m--> 539\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39msetup_context \u001B[38;5;241m==\u001B[39m _SingleLevelFunction\u001B[38;5;241m.\u001B[39msetup_context:\n\u001B[0;32m    542\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    543\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    544\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    545\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    546\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    547\u001B[0m     )\n",
      "File \u001B[1;32m~\\Documents\\repositories\\QML-QGNN\\venv\\Lib\\site-packages\\qiskit_machine_learning\\connectors\\torch_connector.py:100\u001B[0m, in \u001B[0;36mTorchConnector._TorchNNFunction.forward\u001B[1;34m(ctx, input_data, weights, neural_network, sparse)\u001B[0m\n\u001B[0;32m     95\u001B[0m ctx\u001B[38;5;241m.\u001B[39msave_for_backward(input_data, weights)\n\u001B[0;32m     97\u001B[0m \u001B[38;5;66;03m# Detach the tensors and move it to CPU as we need numpy array to compute gradients\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;66;03m# of the quantum neural network. If the tensors are on CPU already this does nothing.\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;66;03m# Some other tensors down below are also moved to CPU for computations.\u001B[39;00m\n\u001B[1;32m--> 100\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mneural_network\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    101\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    102\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ctx\u001B[38;5;241m.\u001B[39msparse:\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m neural_network\u001B[38;5;241m.\u001B[39msparse:\n",
      "File \u001B[1;32m~\\Documents\\repositories\\QML-QGNN\\venv\\Lib\\site-packages\\qiskit_machine_learning\\neural_networks\\neural_network.py:226\u001B[0m, in \u001B[0;36mNeuralNetwork.forward\u001B[1;34m(self, input_data, weights)\u001B[0m\n\u001B[0;32m    224\u001B[0m input_, shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_input(input_data)\n\u001B[0;32m    225\u001B[0m weights_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_weights(weights)\n\u001B[1;32m--> 226\u001B[0m output_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_forward_output(output_data, shape)\n",
      "File \u001B[1;32m~\\Documents\\repositories\\QML-QGNN\\venv\\Lib\\site-packages\\qiskit_machine_learning\\neural_networks\\estimator_qnn.py:222\u001B[0m, in \u001B[0;36mEstimatorQNN._forward\u001B[1;34m(self, input_data, weights)\u001B[0m\n\u001B[0;32m    216\u001B[0m job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimator\u001B[38;5;241m.\u001B[39mrun(\n\u001B[0;32m    217\u001B[0m     [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_circuit] \u001B[38;5;241m*\u001B[39m num_samples \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_shape[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    218\u001B[0m     [op \u001B[38;5;28;01mfor\u001B[39;00m op \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_observables \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_samples)],\n\u001B[0;32m    219\u001B[0m     np\u001B[38;5;241m.\u001B[39mtile(parameter_values_, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_shape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m1\u001B[39m)),\n\u001B[0;32m    220\u001B[0m )\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 222\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    224\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m QiskitMachineLearningError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEstimator job failed.\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\repositories\\QML-QGNN\\venv\\Lib\\site-packages\\qiskit\\primitives\\primitive_job.py:55\u001B[0m, in \u001B[0;36mPrimitiveJob.result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return the results of the job.\"\"\"\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_submitted()\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_future\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:451\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    448\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m    449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[1;32m--> 451\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_condition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001B[0;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:320\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 320\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    321\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    322\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "class QLearningTsp(QLearning):\n",
    "    def __init__(self,\n",
    "                 hyperparams : dict,\n",
    "                 path :str =BASE_PATH):\n",
    "\n",
    "        super(QLearningTsp, self).__init__(hyperparams, path)\n",
    "\n",
    "        self.fully_connected_qubits = list(combinations(list(range(self.n_vars)), 2))\n",
    "        self.is_multi_instance = hyperparams.get('is_multi_instance')\n",
    "        self.interaction = namedtuple(\n",
    "            typename='interaction', \n",
    "            field_names=('state', 'action', 'reward', 'next_state', 'done', 'partial_tour', 'edge_weights')\n",
    "        )\n",
    "        \n",
    "        self.model = self.generate_eqc_model()\n",
    "\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=self.learning_rate_in)\n",
    "        self.loss_fun = MSELoss()\n",
    "\n",
    "        self.data_path = hyperparams.get('data_path')\n",
    "\n",
    "    def generate_eqc_model(self):\n",
    "        num_edges_in_graph = len(self.fully_connected_qubits)\n",
    "        n_input_params = self.n_vars + num_edges_in_graph\n",
    "\n",
    "        data_symbols = []\n",
    "        for layer in range(self.n_layers):\n",
    "            data = [Parameter(f'layer[{layer}]_v[{qubit}]') for qubit in range(self.n_vars)]\n",
    "            data += [[Parameter(f'layer[{layer}]_e[{ew}]') for ew in range(num_edges_in_graph)]]\n",
    "            data_symbols.append(data)\n",
    "\n",
    "        circuit = graph_encoding_circuit(self.fully_connected_qubits, self.n_vars, self.n_layers, data_symbols)\n",
    "\n",
    "        flattened_data_symbols = []\n",
    "        for layer in data_symbols:\n",
    "            for item in layer:\n",
    "                if type(item) == list:\n",
    "                    for symbol in item:\n",
    "                        flattened_data_symbols.append(str(symbol))\n",
    "                else:\n",
    "                    flattened_data_symbols.append(str(item))\n",
    "\n",
    "        model = QModel(\n",
    "            n_input_params=n_input_params, n_vars=self.n_vars, \n",
    "            num_edges_in_graph=num_edges_in_graph, n_layers=self.n_layers, \n",
    "            flattened_data_symbols=flattened_data_symbols, circuit=circuit\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_observables(self, edge_weights: dict | list[dict], partial_tours:  list[int] | list[list[int]]) -> \\\n",
    "            (tuple[list[SparsePauliOp], list[int]] | list[tuple[list[SparsePauliOp], list[int]]]):\n",
    "        if not isinstance(edge_weights, list):\n",
    "            edge_weights = [edge_weights]\n",
    "\n",
    "        if not isinstance(partial_tours[0], list):\n",
    "            partial_tours = [partial_tours]\n",
    "\n",
    "        observables_batch = []\n",
    "        available_nodes_batch = []\n",
    "\n",
    "        for i, partial_tour in enumerate(partial_tours):\n",
    "            observables = []\n",
    "            available_nodes = []\n",
    "            last_edge = () if len(partial_tour) == 0 else partial_tour[-1]\n",
    "            last_node = 0 if len(last_edge) == 0 else last_edge[1]\n",
    "\n",
    "            edge_weight_batch = edge_weights[i] if isinstance(edge_weights[i], list) else [edge_weights[i]]\n",
    "\n",
    "            for edge_weight in edge_weight_batch:\n",
    "                for edge, weight in edge_weight.items():\n",
    "                    if edge[0] == last_node or edge[1] == last_node:\n",
    "                        if edge[0] == last_node and len([tup for tup in partial_tour if tup[0] == edge[1]]) >= 1:\n",
    "                            continue\n",
    "                        elif edge[1] == last_node and len([tup for tup in partial_tour if tup[0] == edge[0]]) >= 1:\n",
    "                            continue\n",
    "\n",
    "                        if edge[0] == last_node:\n",
    "                            available_nodes.append(edge[1])\n",
    "                        else:\n",
    "                            available_nodes.append(edge[0])\n",
    "\n",
    "                        observable = SparsePauliOp.from_sparse_list(\n",
    "                            [(\"ZZ\", [edge[0], edge[1]], weight)],\n",
    "                            num_qubits=self.n_vars\n",
    "                        )\n",
    "                        observables.append(observable)\n",
    "\n",
    "            if observables == []:\n",
    "                observable = SparsePauliOp.from_sparse_list(\n",
    "                    [(\"I\", [0], 0)],\n",
    "                    num_qubits=self.n_vars\n",
    "                )\n",
    "                observables.append(observable)\n",
    "                available_nodes.append(0)\n",
    "\n",
    "            observables_batch.append(observables)\n",
    "            available_nodes_batch.append(available_nodes)\n",
    "\n",
    "        if len(partial_tours) == 1:\n",
    "            return observables_batch[0], available_nodes_batch[0]\n",
    "        else:\n",
    "            return observables_batch, available_nodes_batch\n",
    "\n",
    "    def get_action(self, state_list : list[float], available_nodes : list,\n",
    "                   partial_tour : list[int], edge_weights : dict) -> int:\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = random.choice(available_nodes)\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state_list).unsqueeze(0)\n",
    "\n",
    "            observables, available_nodes = self.get_observables(\n",
    "                edge_weights=edge_weights,\n",
    "                partial_tours=[partial_tour]\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                expectations = self.model(state_tensor, observables)\n",
    "                \n",
    "            action = available_nodes[torch.argmax(expectations).item()]\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def q_vals_from_expectations(self, partial_tours, edge_weights, expectations):\n",
    "        expectations = expectations.detach().numpy()\n",
    "        indexed_expectations = []\n",
    "        for exps in expectations:\n",
    "            batch_ix_exp = {}\n",
    "            for edge, exp_val in zip(self.fully_connected_qubits, exps):\n",
    "                batch_ix_exp[edge] = exp_val\n",
    "            indexed_expectations.append(batch_ix_exp)\n",
    "\n",
    "        batch_q_vals = []\n",
    "        for tour_ix, partial_tour in enumerate(partial_tours):\n",
    "            q_vals = []\n",
    "            for i in range(self.n_vars):\n",
    "                node_in_tour = False\n",
    "                for edge in partial_tour:\n",
    "                    if i in edge:\n",
    "                        node_in_tour = True\n",
    "\n",
    "                if not node_in_tour:\n",
    "                    next_edge = None\n",
    "                    if partial_tour:\n",
    "                        next_edge = (partial_tour[-1][1], i)\n",
    "                    else:\n",
    "                        if i > 0:\n",
    "                            next_edge = (0, i)\n",
    "\n",
    "                    if next_edge is not None:\n",
    "                        try:\n",
    "                            q_val = edge_weights[tour_ix][next_edge] * indexed_expectations[tour_ix][next_edge]\n",
    "                        except KeyError:\n",
    "                            q_val = edge_weights[tour_ix][\n",
    "                                (next_edge[1], next_edge[0])] * indexed_expectations[tour_ix][\n",
    "                                        (next_edge[1], next_edge[0])]\n",
    "                    else:\n",
    "                        q_val = -10000\n",
    "                else:\n",
    "                    q_val = -10000\n",
    "                q_vals.append(q_val)\n",
    "\n",
    "            batch_q_vals.append(q_vals)\n",
    "\n",
    "        return np.asarray(batch_q_vals)\n",
    "    \n",
    "    # TODO: Check this training step method.\n",
    "    def train_step(self):\n",
    "        training_batch = random.choices(self.memory, k=self.batch_size)\n",
    "        training_batch = self.interaction(*zip(*training_batch))\n",
    "\n",
    "        states = [x for x in training_batch.state]\n",
    "        rewards = np.asarray([x for x in training_batch.reward], dtype=np.float32)\n",
    "        next_states = [x for x in training_batch.next_state]\n",
    "        done = np.asarray([x for x in training_batch.done])\n",
    "        partial_tours = [x for x in training_batch.partial_tour]\n",
    "        edge_weights = [x for x in training_batch.edge_weights]\n",
    "\n",
    "        states = torch.tensor(states)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states)\n",
    "        done = torch.tensor(done, dtype=torch.float64)\n",
    "        \n",
    "        readout_op = self.get_readout_op()\n",
    "        exp_values_future = self.model(next_states, readout_op)\n",
    "        future_rewards = torch.tensor(self.q_vals_from_expectations(\n",
    "            partial_tours, edge_weights, exp_values_future), dtype=torch.float32)\n",
    "\n",
    "        target_q_values = rewards + (\n",
    "                self.gamma * torch.max(future_rewards) * (1.0 - done)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # IMPLICIT TAPE-BASED AD:\n",
    "        \n",
    "        # The expectation values estimated by Q for the given actions.\n",
    "        exp_val_masks = torch.tensor(get_masks_for_actions(edge_weights, partial_tours))\n",
    "        exp_values = self.model(states, readout_op)\n",
    "        q_values_masked = torch.sum(exp_values * exp_val_masks, dim=1)\n",
    "        \n",
    "        loss = self.loss_fun(target_q_values, q_values_masked)\n",
    "        loss_val = loss.item()\n",
    "\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update training variables / parameters\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def get_readout_op(self):\n",
    "        observables = []\n",
    "        for edge in self.fully_connected_qubits:\n",
    "            observable = SparsePauliOp.from_sparse_list(\n",
    "                            [(\"ZZ\", [edge[0], edge[1]], 1)],\n",
    "                            num_qubits=self.n_vars\n",
    "                        )\n",
    "            observables.append(observable)\n",
    "        return observables\n",
    "\n",
    "    def perform_episodes(self, num_instances : int) -> None:\n",
    "        self.meta['num_instances'] = num_instances\n",
    "        self.meta['best_tour_length'] = 100000\n",
    "        self.meta['best_tour'] = []\n",
    "        self.meta['best_tour_ix'] = 0\n",
    "        self.meta['env_solved'] = False\n",
    "\n",
    "        with open(self.data_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "\n",
    "        if self.is_multi_instance:\n",
    "            x_train = data['x_train'][:num_instances]\n",
    "            y_train = data['y_train'][:num_instances]\n",
    "        else:\n",
    "            x_train = data['x_train']\n",
    "            y_train = data['y_train']\n",
    "\n",
    "        tour_length_history = []\n",
    "        optimal_tour_length_history = []\n",
    "        ratio_history = []\n",
    "        running_avgs = []\n",
    "        running_avg = 0\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            instance_number = random.randint(0, num_instances - 1)\n",
    "            tsp_graph_nodes = x_train[instance_number]\n",
    "            optimal_tour_length = compute_tour_length(\n",
    "                tsp_graph_nodes, [int(x - 1) for x in y_train[instance_number][:-1]])\n",
    "            node_to_qubit_map = {}\n",
    "            for i, node in enumerate(tsp_graph_nodes):\n",
    "                node_to_qubit_map[node] = i\n",
    "\n",
    "            fully_connected_edges = []\n",
    "            edge_weights = {}\n",
    "            edge_weights_ix = {}\n",
    "            for edge in self.fully_connected_qubits:\n",
    "                fully_connected_edges.append((tsp_graph_nodes[edge[0]], tsp_graph_nodes[edge[1]]))\n",
    "                edge_distance = np.linalg.norm(\n",
    "                    np.asarray(tsp_graph_nodes[edge[0]]) - np.asarray(tsp_graph_nodes[edge[1]]))\n",
    "                edge_weights[(tsp_graph_nodes[edge[0]], tsp_graph_nodes[edge[1]])] = edge_distance\n",
    "                edge_weights_ix[edge] = edge_distance\n",
    "\n",
    "            tour = [0]  # Without loss of generality we always start at city 0\n",
    "            tour_edges = []\n",
    "            step_rewards = []\n",
    "            available_nodes = list(range(1, self.n_vars))\n",
    "\n",
    "            for i in range(self.n_vars):\n",
    "                prev_tour = copy.deepcopy(tour)\n",
    "                state_list = graph_to_list(\n",
    "                    tsp_graph_nodes, fully_connected_edges, edge_weights,\n",
    "                    available_nodes, node_to_qubit_map)\n",
    "                \n",
    "                next_node = self.get_action(state_list, available_nodes, tour_edges, edge_weights_ix)\n",
    "                tour_edges.append((tour[-1], next_node))\n",
    "                new_tour_edges = copy.deepcopy(tour_edges)\n",
    "                tour.append(next_node)\n",
    "\n",
    "                remove_node_ix = available_nodes.index(next_node)\n",
    "                del available_nodes[remove_node_ix]\n",
    "\n",
    "                if len(tour) > 1:\n",
    "                    reward = compute_reward(tsp_graph_nodes, prev_tour, tour)\n",
    "                    step_rewards.append(reward)\n",
    "\n",
    "                    done = 0 if len(available_nodes) > 1 else 1\n",
    "                    transition = (\n",
    "                        state_list, next_node, reward, \n",
    "                        graph_to_list(\n",
    "                            tsp_graph_nodes, fully_connected_edges, edge_weights,\n",
    "                            available_nodes, node_to_qubit_map\n",
    "                        ), done, new_tour_edges, edge_weights_ix\n",
    "                    )\n",
    "                    self.memory.append(transition)\n",
    "\n",
    "                if len(available_nodes) == 1:\n",
    "                    prev_tour = copy.deepcopy(tour)\n",
    "\n",
    "                    tour_edges.append((tour[-1], available_nodes[0]))\n",
    "                    tour_edges.append((available_nodes[0], tour[0]))\n",
    "                    new_tour_edges = copy.deepcopy(tour_edges)\n",
    "\n",
    "                    tour.append(available_nodes[0])\n",
    "                    tour.append(tour[0])\n",
    "\n",
    "                    reward = compute_reward(tsp_graph_nodes, prev_tour, tour)\n",
    "                    step_rewards.append(reward)\n",
    "\n",
    "                    transition = (\n",
    "                        state_list, next_node, reward, \n",
    "                        graph_to_list(\n",
    "                            tsp_graph_nodes, fully_connected_edges, edge_weights,\n",
    "                            available_nodes, node_to_qubit_map\n",
    "                        ), 1, new_tour_edges, edge_weights_ix\n",
    "                    )\n",
    "                    \n",
    "                    self.memory.append(transition)\n",
    "                    break\n",
    "\n",
    "            tour_length = compute_tour_length(tsp_graph_nodes, tour)\n",
    "            tour_length_history.append(tour_length)\n",
    "            optimal_tour_length_history.append(optimal_tour_length)\n",
    "\n",
    "            if tour_length < self.meta.get('best_tour_length'):\n",
    "                self.meta['best_tour_length'] = tour_length\n",
    "                self.meta['best_tour'] = tour\n",
    "                self.meta['best_tour_ix'] = instance_number\n",
    "\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                if episode % self.update_after == 0:\n",
    "                    loss = self.train_step()\n",
    "                    print(\n",
    "                        f\"Episode {episode}, loss {loss}, running avg {running_avg}, epsilon {self.epsilon}\")\n",
    "                    print(f\"\\tFinal tour: {tour}\")\n",
    "                else:\n",
    "                    print(\n",
    "                            f\"Episode {episode}, running avg {running_avg}, epsilon {self.epsilon}\")\n",
    "                    print(f\"\\tFinal tour: {tour}\")\n",
    "\n",
    "            if self.epsilon_schedule == 'fast':\n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon_decay * self.epsilon)\n",
    "\n",
    "            ratio_history.append(tour_length_history[-1] / optimal_tour_length)\n",
    "\n",
    "            if len(ratio_history) >= 100:\n",
    "                running_avg = np.mean(ratio_history[-100:])\n",
    "            else:\n",
    "                running_avg = np.mean(ratio_history)\n",
    "\n",
    "            running_avgs.append(running_avg)\n",
    "\n",
    "            if len(ratio_history) >= 100 and running_avg <= 1.05:\n",
    "                print(f\"Environment solved in {episode+1} episodes!\")\n",
    "                self.meta['env_solved'] = True\n",
    "                break\n",
    "\n",
    "        # This is just for visualization purposes.\n",
    "        plt.plot(running_avgs)\n",
    "        plt.ylabel(\"Ratio to optimal tour length\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.title(\"Running average over past 100 episodes\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "for i in range(hyperparams.get('repetitions', 1)):\n",
    "    tsp_multi = QLearningTsp(\n",
    "        hyperparams=hyperparams,\n",
    "        path='/save_path/',\n",
    "    )\n",
    "\n",
    "    tsp_multi.perform_episodes(\n",
    "        num_instances=hyperparams.get('num_instances')\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T12:41:09.671577100Z",
     "start_time": "2023-12-10T12:31:28.114414900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
